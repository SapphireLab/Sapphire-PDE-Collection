# From PINNs to PIKANs: Recent Advances in Physics-Informed Machine Learning

<details>
<summary>基本信息</summary>

- 标题: "From PINNs to PIKANs: Recent Advances in Physics-Informed Machine Learning"
- 作者:
  - 01 Juan Diego Toscano,
  - 02 Vivek Oommen,
  - 03 Alan John Varghese,
  - 04 Zongren Zou,
  - 05 Nazanin Ahmadi Daryakenari,
  - 06 Chenxi Wu,
  - 07 George Em Karniadakis
- 链接:
  - [ArXiv](https://arxiv.org/abs/2410.13228)
  - [Publication]
  - [Github]
  - [Demo]
- 文件:
  - [ArXiv](2410.13228v2.pdf)
  - [Publication] #TODO

</details>

## Abstract: 摘要

**Physics-Informed Neural Networks (PINNs)** have emerged as a key tool in Scientific Machine Learning since their introduction in 2017, enabling the efficient solution of ordinary and partial differential equations using sparse measurements.
Over the past few years, significant advancements have been made in the training and optimization of **PINNs**, covering aspects such as network architectures, adaptive refinement, domain decomposition, and the use of adaptive weights and activation functions.
A notable recent development is the **Physics-Informed Kolmogorov–Arnold Networks (PIKANs)**, which leverage a representation model originally proposed by Kolmogorov in 1957, offering a promising alternative to traditional **PINNs**.
In this review, we provide a comprehensive overview of the latest advancements in **PINNs**, focusing on improvements in network design, feature expansion, optimization techniques, uncertainty quantification, and theoretical insights.
We also survey key applications across a range of fields, including biomedicine, fluid and solid mechanics, geophysics, dynamical systems, heat transfer, chemical engineering, and beyond.
Finally, we review computational frameworks and software tools developed by both academia and industry to support PINN research and applications.

## 1.Introduction: 引言

The finite element method (FEM) has been the cornerstone of Computational Science and Engineering (CSE) in the last few decades but it was viewed with skepticism when the first published works appeared in the early 1960s.
Despite their success in academic research and industrial applications, FEM cannot easily assimilate measured data unless elaborate data assimilation methods are employed that render large-scale computations prohibitively expensive.
FEM and other conventional numerical methods are effective in solving well-posed problem with full knowledge of the boundary and initial conditions as well as all material parameters.
Unfortunately, in practical applications, there are always gaps in such a setting and arbitrary assumptions have to be made, e.g.
to assume the thermal boundary conditions at the walls in power electronics cooling applications.
This may lead to erroneous results as in such a problem of interest is the highest temperature or the highest heat flux that is typically located at the surface where erroneous assumptions are employed.
What may be available instead are a few
sparse thermocouple measurements either on the surface or inside the domain of interest.
Unfortunately, current numerical methods like FEM cannot utilize such measurements effectively and hence important experimental information for the system is lost.
On the other hand, neural networks are trained based on data of any fidelity or any modality so data assimilation is a natural process in such settings.

Physics-Informed Neural Networks (PINNs) were developed to address precisely this need, considering different simulation scenarios where there is some knowledge of the governing physical laws but not complete knowledge, and there exist some sparse measurements for some of the state variables but not for all.
Hence, PINNs provide a framework to encode physical laws in neural networks~\citep{raissi2019physics} and resolve the disconnect between traditional physically grounded mathematical models and modern purely data-driven methods.
Specifically, PINNs incorporate the governing laws by having an additional `residual' loss term in the objective function that enforces the underlying PDE as a soft constraint.
They are effective in solving both forward and inverse problems across all scientific domains.
PINNs can incorporate sparse and noisy data, making them effective in scenarios where acquiring accurate measurements can be difficult or expensive.
A key innovation in PINNs is the use of automatic differentiation based on computational graphs that leads to accurate treatment of the differential operators employed in conservation laws but most importantly removes the tyranny of elaborate mesh generation that is time consuming and limits solution accuracy.

Since the original two papers appeared on the arXiv in 2017~\citep{raissi2017physicsI, raissi2017physicsII} and the subsequent publication of a combined paper in 2019~\citep{raissi2019physics}, there has been  great excitement in the CSE community and very important advances on many  aspects of the method have been proposed by research groups from around the world and across all scientific domains.
At the time of this writing, there have already been  over 11000 citations of~\citep{raissi2019physics} , with many studies investigating the applicability of PINNs  across different scientific domains while other studies proposing algorithmic improvements aimed at addressing the limitations of the original formulation.
In the current review paper, we provide a compilation of most of the major algorithmic developments and present a non-exhaustive list of applications of PINNs across different disciplines.
A comprehensive timeline of some of the important papers about PINNs is presented in the Appendix from PINNs~\citep{raissi2017physicsI} to PIKANs~\citep{liu2024kan}.

While existing reviews, such as those by~\citep{cuomo2022scientific, farea2024understanding, ganga2024exploringphysicsinformedneuralnetworks, raissi2024physics} summarize key aspects of PINNs, our paper differentiates itself by providing a more extensive overview of the latest algorithmic developments and by covering a broader range of applications of PINNs across scientific disciplines.
Reviews by~\citep{cuomo2022scientific} and~\citep{farea2024understanding} focus primarily on the methodology and applications of PINNs in various domains, with less emphasis on recent algorithmic improvements.
The review by~\citep{raissi2024physics} provides a concise overview of PINNs and their extensions, with an example on data-driven discovery of equations, but does not dive deep into applications of PINNs.
The review in~\citep{ganga2024exploringphysicsinformedneuralnetworks} includes a discussion of algorithmic developments, but limits the scope of their discussion on applications to thermal management and computational fluid dynamics.
Additionally, several reviews focus on specific domains of application.
For example,~\citep{chi2024comprehensive} and~\citep{cai2021physics} review the use of PINNs in fluid dynamics, while~\citep{huang2022applications} focuses on applications within power systems.
In contrast,~\citep{lawal2022physics} conducted a bibliometric analysis of 120 research articles, highlighting key publication trends, highly cited authors and leading countries in PINN research.

The structure of the paper is shown schematically in Fig.\ref{fig:schematic}.

In Section \ref{framework} we outline the general framework of Physics-Informed Machine Learning.
Section \ref{algorithmic_developments} provides a comprehensive summary of the major techniques aimed at improving PINNs.
In Section \ref{applications} we provide an overview of the diverse applications of PINNs.
Section \ref{uncertainty_quantification} focuses on uncertainty quantification methods in PINNs.
In Section \ref{theoretical_advances}, we summarize the developments in the theory behind PINNs.
Section \ref{computational_frameworks} reviews the various computational frameworks and software.
Finally, in Section \ref{discussion}, we provide a discussion and future outlook.

## 2.Physics-Informed Machine Learning (PIML)

Physics-Informed Machine Learning (PIML) has emerged as a powerful alternative to traditional numerical methods for solving partial differential equations (PDEs) in both forward and inverse problems.
PIML was first introduced in a series of papers by Raissi, Perdikaris, and Karniadakis~\citep{raissi2017machine} based on Gaussian processes regression (GPR); see also the patent by the same authors~\citep{raissi2021physics}.
In this paper, however, we will review the subsequent development of PIML using neural networks and automatic differentiation, starting with the two papers from 2017 on the arXiv~\citep{raissi2017physicsI, raissi2017physicsII}, which
were combined into a single paper later in~\citep{raissi2019physics}.
It is worth noting that earlier papers by~\citep{dissanayake1994neural,lagaris1998artificial} attempted to solve PDEs (forward problems) but without any data fusion or automatic differentiation.
The PIML we present in this paper employs a representation model, namely a multilayer perceptron (MLP) or a Kolmogorov-Arnold Network (KAN)~\citep{liu2024kan}, to approximate the solution of ordinary or partial differential equations (ODEs/PDEs) and match any given data and constraints by minimizing a loss function comprised of multiple terms.
In particular, this loss function is designed to fit observable data or other physical or mathematical constraints while enforcing the underlying physics, e.g., conservation laws~\citep{raissi2019physics,shukla2024comprehensive}.

Unlike traditional numerical methods, most PIML models do not rely on predefined grids or meshes, allowing them to handle complex geometries and high-dimensional problems efficiently.
By leveraging automatic differentiation, PIML models compute derivatives accurately without discretization, seamlessly integrating governing physical laws with data.
This flexibility allows PIML models to approximate solutions from partial information, making them optimal for uncovering hidden parameters~\citep{raissi2019physics}, as well as reconstructing~\citep{cai2021flow} or inferring hidden fields~\citep{raissi2020hidden} from real-world data.
Moreover, PIML models are well-suited for handling high-dimensional PDEs~\citep{hu2024tackling}, coupled systems~\citep{jin2021nsfnets, shukla2024neurosem}, stochastic differential equations~\citep{yang2019adversarial}, and fractional PDEs~\citep{pang2019fpinns}, all while maintaining scalability through parallelization on modern hardware such as GPUs~\citep{karniadakis2021physics}.
This enables PIML models to efficiently tackle multi-physics problems and large-scale simulations with reduced computational overhead compared to traditional methods.

PIML is agnostic to specific governing laws, so here we consider the following nonlinear ODE/PDE:

$$
\begin{aligned}
    \mathcal{F}_\tau[\hat{u}](x)  &= f(x), \quad x \in \Omega, \\
    \mathcal{B}_\tau[\hat{u}](x)  &= b(x), \quad x \in \Omega_B,
\end{aligned}
$$

where \( x \) represents the spatial-temporal coordinate, \( \hat{u} \) is the solution to the ODE/PDE, \( \tau \) are the parameters of the equation, \( f \) is the source term, \( b \) is the boundary term, and \( \mathcal{F} \) and \( \mathcal{B} \) are general nonlinear differential and boundary operators, respectively.
The PIML approach aims to approximate the solution to the problem defined by Eq.
\eqref{eq:problem} using a representation model, denoted as:

$$
\begin{aligned}
  \hat{u}(x) \approx u(\theta, x), x\in\Omega\cup\Omega_B,
\end{aligned}
$$

where \( u \) is the representation model, and \( \theta \) are its learnable parameters.
Since \( u \) is continuous and differentiable, it allows for the computation of the source and boundary terms \( f \) and \( b \) through automatic differentiation~\citep{baydin2018automatic}, expressed as \( \mathcal{F}_\tau[u] \) and \( \mathcal{B}_\tau[u] \)~\citep{raissi2019physics}.

The goal of PIML training is to find the optimal learnable parameters that minimize the cumulative error between the approximated solution and the known components of the true solution, such as the governing equation, boundary conditions, or data residuals.
This framework can also be easily extended to ODE/PDE systems by stacking constraints for each approximated solution~\citep{raissi2020hidden}.

In general, when the equation parameters \( \tau \) are known and the boundary conditions are prescribed, the problem is referred to as a \textit{forward problem}, where no observational data within the domain are required~\citep{raissi2019physics, meng2020ppinn}.
Conversely, when partial information, such as \( \tau \), boundary conditions, or hidden fields in the ODE/PDE system, is unknown, then the problem is referred to as an \textit{inverse problem}, where the objective is to infer simultaneously the unknown information and the solution from available data or observations~\citep{raissi2020hidden}.
A schematic of the overall PIML framework is shown in Fig.\ref{fig:PIML_compz}.
