# From PINNs to PIKANs: Recent Advances in Physics-Informed Machine Learning

<details>
<summary>基本信息</summary>

- 标题: "From PINNs to PIKANs: Recent Advances in Physics-Informed Machine Learning"
- 作者:
  - 01 Juan Diego Toscano,
  - 02 Vivek Oommen,
  - 03 Alan John Varghese,
  - 04 Zongren Zou,
  - 05 Nazanin Ahmadi Daryakenari,
  - 06 Chenxi Wu,
  - 07 George Em Karniadakis
- 链接:
  - [ArXiv](https://arxiv.org/abs/2410.13228)
  - [Publication]
  - [Github]
  - [Demo]
- 文件:
  - [ArXiv](2410.13228v2.pdf)
  - [Publication] #TODO

</details>

## Abstract: 摘要

**Physics-Informed Neural Networks (PINNs)** have emerged as a key tool in Scientific Machine Learning since their introduction in 2017, enabling the efficient solution of ordinary and partial differential equations using sparse measurements.
Over the past few years, significant advancements have been made in the training and optimization of **PINNs**, covering aspects such as network architectures, adaptive refinement, domain decomposition, and the use of adaptive weights and activation functions.
A notable recent development is the **Physics-Informed Kolmogorov–Arnold Networks (PIKANs)**, which leverage a representation model originally proposed by Kolmogorov in 1957, offering a promising alternative to traditional **PINNs**.
In this review, we provide a comprehensive overview of the latest advancements in **PINNs**, focusing on improvements in network design, feature expansion, optimization techniques, uncertainty quantification, and theoretical insights.
We also survey key applications across a range of fields, including biomedicine, fluid and solid mechanics, geophysics, dynamical systems, heat transfer, chemical engineering, and beyond.
Finally, we review computational frameworks and software tools developed by both academia and industry to support PINN research and applications.

## 1.Introduction: 引言

The finite element method (FEM) has been the cornerstone of Computational Science and Engineering (CSE) in the last few decades but it was viewed with skepticism when the first published works appeared in the early 1960s.
Despite their success in academic research and industrial applications, FEM cannot easily assimilate measured data unless elaborate data assimilation methods are employed that render large-scale computations prohibitively expensive.
FEM and other conventional numerical methods are effective in solving well-posed problem with full knowledge of the boundary and initial conditions as well as all material parameters.
Unfortunately, in practical applications, there are always gaps in such a setting and arbitrary assumptions have to be made, e.g.
to assume the thermal boundary conditions at the walls in power electronics cooling applications.
This may lead to erroneous results as in such a problem of interest is the highest temperature or the highest heat flux that is typically located at the surface where erroneous assumptions are employed.
What may be available instead are a few
sparse thermocouple measurements either on the surface or inside the domain of interest.
Unfortunately, current numerical methods like FEM cannot utilize such measurements effectively and hence important experimental information for the system is lost.
On the other hand, neural networks are trained based on data of any fidelity or any modality so data assimilation is a natural process in such settings.

Physics-Informed Neural Networks (PINNs) were developed to address precisely this need, considering different simulation scenarios where there is some knowledge of the governing physical laws but not complete knowledge, and there exist some sparse measurements for some of the state variables but not for all.
Hence, PINNs provide a framework to encode physical laws in neural networks~\citep{raissi2019physics} and resolve the disconnect between traditional physically grounded mathematical models and modern purely data-driven methods.
Specifically, PINNs incorporate the governing laws by having an additional `residual' loss term in the objective function that enforces the underlying PDE as a soft constraint.
They are effective in solving both forward and inverse problems across all scientific domains.
PINNs can incorporate sparse and noisy data, making them effective in scenarios where acquiring accurate measurements can be difficult or expensive.
A key innovation in PINNs is the use of automatic differentiation based on computational graphs that leads to accurate treatment of the differential operators employed in conservation laws but most importantly removes the tyranny of elaborate mesh generation that is time consuming and limits solution accuracy.

Since the original two papers appeared on the arXiv in 2017~\citep{raissi2017physicsI, raissi2017physicsII} and the subsequent publication of a combined paper in 2019~\citep{raissi2019physics}, there has been  great excitement in the CSE community and very important advances on many  aspects of the method have been proposed by research groups from around the world and across all scientific domains.
At the time of this writing, there have already been  over 11000 citations of~\citep{raissi2019physics} , with many studies investigating the applicability of PINNs  across different scientific domains while other studies proposing algorithmic improvements aimed at addressing the limitations of the original formulation.
In the current review paper, we provide a compilation of most of the major algorithmic developments and present a non-exhaustive list of applications of PINNs across different disciplines.
A comprehensive timeline of some of the important papers about PINNs is presented in the Appendix from PINNs~\citep{raissi2017physicsI} to PIKANs~\citep{liu2024kan}.

While existing reviews, such as those by~\citep{cuomo2022scientific, farea2024understanding, ganga2024exploringphysicsinformedneuralnetworks, raissi2024physics} summarize key aspects of PINNs, our paper differentiates itself by providing a more extensive overview of the latest algorithmic developments and by covering a broader range of applications of PINNs across scientific disciplines.
Reviews by~\citep{cuomo2022scientific} and~\citep{farea2024understanding} focus primarily on the methodology and applications of PINNs in various domains, with less emphasis on recent algorithmic improvements.
The review by~\citep{raissi2024physics} provides a concise overview of PINNs and their extensions, with an example on data-driven discovery of equations, but does not dive deep into applications of PINNs.
The review in~\citep{ganga2024exploringphysicsinformedneuralnetworks} includes a discussion of algorithmic developments, but limits the scope of their discussion on applications to thermal management and computational fluid dynamics.
Additionally, several reviews focus on specific domains of application.
For example,~\citep{chi2024comprehensive} and~\citep{cai2021physics} review the use of PINNs in fluid dynamics, while~\citep{huang2022applications} focuses on applications within power systems.
In contrast,~\citep{lawal2022physics} conducted a bibliometric analysis of 120 research articles, highlighting key publication trends, highly cited authors and leading countries in PINN research.

The structure of the paper is shown schematically in Fig.\ref{fig:schematic}.

In Section \ref{framework} we outline the general framework of Physics-Informed Machine Learning.
Section \ref{algorithmic_developments} provides a comprehensive summary of the major techniques aimed at improving PINNs.
In Section \ref{applications} we provide an overview of the diverse applications of PINNs.
Section \ref{uncertainty_quantification} focuses on uncertainty quantification methods in PINNs.
In Section \ref{theoretical_advances}, we summarize the developments in the theory behind PINNs.
Section \ref{computational_frameworks} reviews the various computational frameworks and software.
Finally, in Section \ref{discussion}, we provide a discussion and future outlook.

## 2.Physics-Informed Machine Learning (PIML)

Physics-Informed Machine Learning (PIML) has emerged as a powerful alternative to traditional numerical methods for solving partial differential equations (PDEs) in both forward and inverse problems.
PIML was first introduced in a series of papers by Raissi, Perdikaris, and Karniadakis~\citep{raissi2017machine} based on Gaussian processes regression (GPR); see also the patent by the same authors~\citep{raissi2021physics}.
In this paper, however, we will review the subsequent development of PIML using neural networks and automatic differentiation, starting with the two papers from 2017 on the arXiv~\citep{raissi2017physicsI, raissi2017physicsII}, which
were combined into a single paper later in~\citep{raissi2019physics}.
It is worth noting that earlier papers by~\citep{dissanayake1994neural,lagaris1998artificial} attempted to solve PDEs (forward problems) but without any data fusion or automatic differentiation.
The PIML we present in this paper employs a representation model, namely a multilayer perceptron (MLP) or a Kolmogorov-Arnold Network (KAN)~\citep{liu2024kan}, to approximate the solution of ordinary or partial differential equations (ODEs/PDEs) and match any given data and constraints by minimizing a loss function comprised of multiple terms.
In particular, this loss function is designed to fit observable data or other physical or mathematical constraints while enforcing the underlying physics, e.g., conservation laws~\citep{raissi2019physics,shukla2024comprehensive}.

Unlike traditional numerical methods, most PIML models do not rely on predefined grids or meshes, allowing them to handle complex geometries and high-dimensional problems efficiently.
By leveraging automatic differentiation, PIML models compute derivatives accurately without discretization, seamlessly integrating governing physical laws with data.
This flexibility allows PIML models to approximate solutions from partial information, making them optimal for uncovering hidden parameters~\citep{raissi2019physics}, as well as reconstructing~\citep{cai2021flow} or inferring hidden fields~\citep{raissi2020hidden} from real-world data.
Moreover, PIML models are well-suited for handling high-dimensional PDEs~\citep{hu2024tackling}, coupled systems~\citep{jin2021nsfnets, shukla2024neurosem}, stochastic differential equations~\citep{yang2019adversarial}, and fractional PDEs~\citep{pang2019fpinns}, all while maintaining scalability through parallelization on modern hardware such as GPUs~\citep{karniadakis2021physics}.
This enables PIML models to efficiently tackle multi-physics problems and large-scale simulations with reduced computational overhead compared to traditional methods.

PIML is agnostic to specific governing laws, so here we consider the following nonlinear ODE/PDE:

$$
\begin{aligned}
    \mathcal{F}_\tau[\hat{u}](x)  &= f(x), \quad x \in \Omega, \\
    \mathcal{B}_\tau[\hat{u}](x)  &= b(x), \quad x \in \Omega_B,
\end{aligned}
$$

where \( x \) represents the spatial-temporal coordinate, \( \hat{u} \) is the solution to the ODE/PDE, \( \tau \) are the parameters of the equation, \( f \) is the source term, \( b \) is the boundary term, and \( \mathcal{F} \) and \( \mathcal{B} \) are general nonlinear differential and boundary operators, respectively.
The PIML approach aims to approximate the solution to the problem defined by Eq.
\eqref{eq:problem} using a representation model, denoted as:

$$
\begin{aligned}
  \hat{u}(x) \approx u(\theta, x), x\in\Omega\cup\Omega_B,
\end{aligned}
$$

where \( u \) is the representation model, and \( \theta \) are its learnable parameters.
Since \( u \) is continuous and differentiable, it allows for the computation of the source and boundary terms \( f \) and \( b \) through automatic differentiation~\citep{baydin2018automatic}, expressed as \( \mathcal{F}_\tau[u] \) and \( \mathcal{B}_\tau[u] \)~\citep{raissi2019physics}.

The goal of PIML training is to find the optimal learnable parameters that minimize the cumulative error between the approximated solution and the known components of the true solution, such as the governing equation, boundary conditions, or data residuals.
This framework can also be easily extended to ODE/PDE systems by stacking constraints for each approximated solution~\citep{raissi2020hidden}.

In general, when the equation parameters \( \tau \) are known and the boundary conditions are prescribed, the problem is referred to as a \textit{forward problem}, where no observational data within the domain are required~\citep{raissi2019physics, meng2020ppinn}.
Conversely, when partial information, such as \( \tau \), boundary conditions, or hidden fields in the ODE/PDE system, is unknown, then the problem is referred to as an \textit{inverse problem}, where the objective is to infer simultaneously the unknown information and the solution from available data or observations~\citep{raissi2020hidden}.
A schematic of the overall PIML framework is shown in Fig.\ref{fig:PIML_compz}.

## 3.Algorithmic Developments of PIML

From the PIML framework outlined in Section~\ref{framework}, we can identify three key components: (1) a representation model to approximate the solution, (2) a governing equation (such as an ODE or PDE), and (3) an optimization process that minimizes a multi-objective loss function to find the optimal learnable parameters,
see Fig.\ref{fig:PIML_compz}. Ongoing research has greatly enhanced PIML's baseline performance through various methods targeting these three areas, namely, modifications to the representation model, advancements in the treatment of the governing equation, and optimization process improvements.

### 3.1.Representation Model Modifications

When the representation model is defined using an MLP, the PIML formulation is referred to as physics-informed neural networks (PINNs) ~\citep{raissi2019physics}. PINNs utilize MLPs to approximate the solutions of an ODE/PDE system ($\bm{\hat{u}} = \{\hat{u}_1, \ldots, \hat{u}_p\}$)  by leveraging the network's ability to model complex nonlinear functions. The approximated solution ($\bm{{u}} = \{{u}_1, \ldots, {u}_p\}$)  using MLPs can be mathematically expressed as:

$$
\begin{aligned}
  \bm{u}(\theta, \bm{x})  &= \text{MLP}(\theta, \bm{x})  \\
  &= \sigma\left(W^{(L) } \sigma\left(W^{(L-1) } \ldots \sigma\left(W^{(1) } \bm{x} + b^{(1) }\right)  \ldots + b^{(L-1) }\right)  + b^{(L) }\right)
\end{aligned}
$$

where ($\bm{x}= \{x_1, \ldots, x_n\}$)  are inputs and $\theta = \{W^{(l) }, b^{(l) }\}$ are the trainable parameters of the network; $W^{(l) }$ and $b^{(l) }$ denote the weights and biases of the $l$-th layer, respectively. The network consists of $L$ layers, and $\sigma$ denotes a suitable activation function. The output of each layer serves as the input for the subsequent layer, culminating in the final output \( u \).

The ability of MLPs to approximate virtually any continuous function on compact subsets of \( \mathbb{R}^n \)  is supported by the Universal Approximation Theorem~\citep{hornik1989multilayer}. This theorem underpins the effectiveness of neural networks in modeling complex nonlinear relationships.

Building on the foundational work in~\citep{raissi2019physics}, many studies have explored enhancing the expressiveness of representation models in PINNs through various strategies. These include input and output normalization, feature expansions, hard constraint encoding, model decompositions, and architectural modifications. Each of these strategies aims to improve the network's ability to capture the underlying physics of the problem more accurately and efficiently.

#### 3.1.1.Input/Output Transformations

One of the most straightforward ways to improve the stability and accuracy of a representation model $\mathcal{M}$ is by transforming the model inputs $\bm{x} \in \mathbb{R}^n$ or outputs $\bm{u} \in \mathbb{R}^p$ using suitable mappings, $I(\cdot) $and $\Gamma(\cdot) $ (see Fig.~\ref{RM_enh}). Under this reformulation, the solutions of an ODE/PDE ($\bm{\hat{u}}$)  can be approximated as:

$$
\begin{aligned}
    \hat{\bm{u}}(\bm{x})  \approx \bm{u}(\theta, \bm{x})  = \Gamma(\mathcal{M}(\theta, I(\bm{x}) ) ),
\end{aligned}
$$

where $\bm{u} = \{u_1, \ldots, u_p\}) $ are the approximated solutions from the representation model $\mathcal{M}$, and  $I: \mathbb{R}^n \rightarrow \mathbb{R}^m$ is a mapping that transforms the multivariate inputs and enhances the model’s expressivity. The choice of $I$ often depends on the activation function $\sigma$, as different activation functions are more effective over specific input domains. For example, Cai et al.~\citep{cai2021artificial} proposed normalizing inputs to the range $[-1, 1]$ when using $\sin(\cdot)$ or $\tanh(\cdot)$ activation functions. Similarly, Raissi et al.~\citep{raissi2020hidden} recommended normalizing inputs based on their mean and standard deviation when using the $swish(\cdot)$ activation function.

On the other hand, the function $\Gamma: \mathbb{R}^p \rightarrow \mathbb{R}^q$ is selected based on the specific characteristics of the problem. For instance, Anagnostopoulos et al.~\citep{anagnostopoulos2024residual} used scalars $a_1, \ldots, a_p \in \mathbb{R}$ to create different linear maps to handle various scales in the model outputs, represented as $\bm{u} = \{a_1u_1, \ldots, a_pu_p\}$, which aids in improving convergence. Similarly, $\Gamma$ can be used to constrain the range of predicted outputs. For example, absolute value $\Gamma(u) =|u|$ or exponential $\Gamma(u) =e^{u}$ functions can enforce the non-negativity required for the density field in high-speed flows~\citep {mao2020physics}. Likewise, Zapf et al.~\citep{zapf2022investigating} used the sigmoid function to constrain the range of the predicted diffusivity.

##### Feature Expansions

These modifications aim to address some fundamental weaknesses in the conventional PIML formulation, particularly in learning high-frequency functions, known as spectral bias~\citep{rahaman2019spectral,cao2019towards}, and capturing other complex relations~\citep{wang2021eigenvector}. To mitigate these issues, researchers have modified the baseline model formulation by transforming its input $\bm{x} \in \mathbb{R}^n$ into an expanded input $I(\bm{x})  \in \mathbb{R}^m$, which is then fed into the representation model as shown in Fig.~\ref{RM_enh}. Several expansion maps have been explored, with their selection typically based on the specific problem. For example, Wang et al.~\citep{wang2021eigenvector} proposed using random Fourier features and demonstrated that this modification helps to mitigate spectral bias. Other types of expansions, such as polynomials, exponentials, Chebyshev polynomials, and even gated recurrent units (GRU), have also been explored or proposed in previous studies~\citep{cai2019multi,liu2020multi,wang2020multi,liu2022linearized,ahmadi2024ai,zhang4957859pkan}.

##### Hard Constraints

Training in PIML involves minimizing an objective function that often combines multiple constraints, significantly complicating the optimization process~\citep{toscano2024inferring}. Research has shown that improper enforcement of boundary conditions can degrade both the performance and stability of neural network training~\citep{dong2021method,wang2021understanding,chen2020comparison}. Thus, accurately imposing boundary conditions is crucial for improving model reliability and efficiency. In particular, Zeinhofer et al.~\citep{zeinhofer2023unified} theoretically demonstrated that hard constraints can lead to lower error estimates in linear problems. One practical way to address these challenges is by embedding boundary conditions directly into the model's structure, either through input/output transformations or specialized architectures, thereby simplifying the optimization and enhancing the overall performance.


###### Dirichlet Boundary Conditions.

Several methods have been developed to enforce Dirichlet boundary conditions exactly in PIML problems. Berrone et al.~\citep{berrone2023enforcing} introduced the Nitsche's method, which applies a variational approach to enforce these conditions. Another systematic technique is the Theory of Functional Connections, which imposes constraints through functional connections, as detailed by Leake et al.~\citep{leake2020deep}. On the other hand, hPINNs utilize penalty methods and the augmented Lagrangian approach to impose hard constraints, providing a flexible framework for handling various boundary conditions~\citep{lu2021physics}. Notably, Sukumar et al.~\citep{sukumar2022exact} introduced Approximate Distance Functions (ADF), which impose boundary conditions through output transformations. Under the ADF framework, the constrained expression for Dirichlet boundary conditions is represented as:

$$
\begin{aligned}
    \hat{\bm{u}}(\bm{x})  \approx \bm{u}(\theta, \bm{x})  = \Gamma(\mathcal{M}(\theta, I(\bm{x}) ) ) =\bm{g}(\bm{x})  + \bm{\phi}(\bm{x}) \mathcal{M}(\theta, I(\bm{x}) ),
\end{aligned}
$$

where $\Gamma(\cdot) $ transform the network output in terms of a function that satisfies the solution $\hat{\bm{u}}$ along the boundaries $\bm{g}(\cdot) $  and a composite distance function $\bm{\phi}(\cdot) $ that equals zero when evaluated on the boundaries. If the boundary is composed of $M$ partitions, denoted as $[S_1, \ldots, S_M]$, the composite distance function for Dirichlet boundary conditions can be expressed as $\bm{\phi}(\phi_1, \phi_2, \ldots, \phi_M)  = \prod_{i=1}^M \phi_i$, where $[\phi_1, \ldots, \phi_M]$ are the individual distance functions. Notice that if $\bm{x} \in S_i$, then $\phi(\bm{x})  = 0$, ensuring that the neural network approximation exactly satisfies the boundary conditions, i.e., $\bm{u}(\bm{x})  = \bm{g}(\mathbf{x}) $.

###### Periodic Boundary Conditions.

On the other hand, this type of boundary conditions can be strictly enforced as hard constraints by selecting a suitable input transformation \(I(\bm{x}) \)  (see Fig.~\ref{RM_enh}). For instance, the periodic nature of a smooth univariate function \(u(x) \)  can be encoded into a model using a one-dimensional Fourier feature embedding, $I(x)  = [1, \cos(\omega_x x), \sin(\omega_x x), \ldots, \cos(m\omega_x x), \sin(m\omega_x x) ]$. Dong et al.~\citep{dong2021method} demonstrated that any representation model, such as \(u(\theta, I(x) ) \), is periodic along the \(x\)  coordinate when using this Fourier feature embedding. Similar expansion maps have been explored for higher dimensions by Wang et al.~\citep{wang2022respecting}.

Other methods to impose hard constraints for periodic boundary conditions include using hPINNs, which employ the penalty and augmented Lagrangian methods to enforce such constraints~\cite {lu2021physics}. Additionally, hybrid approaches that combine various techniques for the exact imposition of periodic boundary conditions have been investigated~\cite {barschkis2023exact}.

###### Complex Constraints.

Complex constraints can be addressed through specialized architectures that incorporate domain-specific knowledge into the learning process. One such approach is the use of Divergence-Free Networks, which apply an appropriate output transformation to ensure that the learned vector fields satisfy divergence-free conditions (see Fig.\ref{RM_enh}), as required in certain fluid dynamics applications\citep{toscano2024inferring,wang2021understanding,anagnostopoulos2024learning}. Zeinhofer et al.\citep{zeinhofer2023unified} theoretically demonstrated that enforcing divergence-free constraints leads to improved error estimates in linear problems. Another example is SympNets, a specialized architecture for identifying Hamiltonian systems from data, which preserves the symplectic structure through different Jacobian matrix factorization techniques\citep{jin2020sympnets}. Finally \citep{zhou2023flow} proposed a method that uses the theory of functional connections to exactly enforce the data constraints in inverse problems.

#### 3.1.2.Architectures

##### Perceptron Modifications

The original PIML formulation uses an MLP as the representation model~\citep{raissi2019physics}. The building blocks of an MLP are perceptrons, which define a layer $l$ and can be defined as follows:

$$
\begin{aligned}
    \bm{z}^{(l) }&=\sigma^{(l) }(W^{(l) }\bm{z}^{(l-1) } + b^{(l) }),
\end{aligned}
$$

where $\bm{z}^{(l) }=\{z_{0}^{(l) },\cdots,z_{H}^{(l) }\}$ are the outputs of layer $l$, $\sigma^{(l) }$ is a non-linear activation function (e.g., $\text{sigmoid}(\cdot) $, $\tanh(\cdot) $, $\sin(\cdot) $, etc.) , and $W^{(l) }$ and $b^{(l) }$ are trainable parameters that linearly transform the inputs $\bm{z}^{(l-1) }=\{z_{0}^{(l-1) },\ldots,z_{H}^{(l-1) }\}$. Several approaches have been proposed to improve the perceptron’s capabilities. For instance, Jagtap et al.~\citep{jagtap2020adaptive,jagtap2020locally,jagtap2022deep} proposed modifying the activation function in as
$\sigma^{(l) }(\cdot)  = \sum_i a_{i}^{(l) } \sigma(f_{i}^{(l) } \cdot) $, where $a_{i}^{(l) }$ and $f_{i}^{(l) }$ are trainable parameters. The authors showed both theoretically and empirically that these modifications significantly improve model performance.

Other approaches proposed modifying the weight matrix $W^{(l) }$; for instance,~\citep{salimans2016weight,raissi2020hidden} proposed decomposing $W^{(l) }$ into its magnitude and its direction via weight normalization described as $W^{(l) }=\frac{g^{(l) }}{\lVert\mathbf{v}^{(l) }\rVert_{2}}\mathbf{v}^{(l) }$. This re-parameterization speeds up the model convergence with minimal computational overhead~\citep{salimans2016weight}. Similarly, ~\citep{wang2022random}  proposed  weight factorization $W^{(l) }=diag (\bm{s} ^{(l) })\cdot V^{(l) }$, where $\bm{s}^{(l) }$ are trainable parameters. The authors experimentally and theoretically showed that this reparameterization significantly improves the model performance~\citep{wang2022random}.

##### Other Representation Models

One natural extension to other representation models, given their similarity to MLPs, is Kolmogorov-Arnold Networks (KANs)~\citep{liu2024kan}, which were introduced as PIKANs in~\citep{shukla2024comprehensive}. Each PIKAN layer can be described as follows:

$$
    \bm{z}^{(l)} = \sum_{i=1}^{H}\Phi_i\left(\sum_{j=1}^{H}\phi_{i,j}(z^{(l-1)}_{j})\right),
$$

where $\bm{z}^{(l-1) }=\{z_{0}^{(l-1) },\cdots,z_{H}^{(l-1) }\}$ is the multivariate input, $H$ denotes the number of neurons and $\Phi_{i,j}$ are the outer and $\phi_{i,j}$ are the inner univariate functions. The specific form of $\phi(\cdot)$ and $\Phi(\cdot)$ defines the types of KAN architectures. Among these variations,~\citep{shukla2024comprehensive} introduced cPIKANs, which use Chebyshev polynomials as inner and outer univariate functions defined as $ \phi(\zeta,\theta) =w_n\sum_n^d c_nT_n(\tanh(\zeta)) $, where $\zeta$ are the inputs, $\theta=(w_n,c_n) $ are trainable parameters, $d$ is the degree and $T_n$ is the $n$-th order Chebyshev polynomial, defined recursively as $T_n(\zeta) =2\zeta T_{n-1}(\zeta)  + T_{n-2}(\zeta) $~\citep{karniadakis2005spectral}. The authors found that this stable representation is more robust to noise~\citep{shukla2024comprehensive} and can lead to improved performance with fewer parameters~\citep{toscano2024inferring,shukla2024comprehensive} than MLPs. Subsequent studies extended the KAN framework and developed new architectures for PIML problems~\citep{howard2024finite,rigas2024adaptive,shuai2024physics,zhang4957859pkan, wang2024kolmogorovarnoldinformedneural,guilhoto2024deeplearningalternativeskolmogorov,koenig2024kan,patra2024physics,toscano2024inferring,wang2024expressiveness,liu2024kan2}.

Other representation models have also been explored, including convolutional neural networks (CNNs)~\citep{gao2021super}, Hermite spline CNNs~\citep{wandel2022spline}, generative adversarial neural networks (GANs)~\citep{yang2020physics,bullwinkel2022deqgan}, spiking neural networks~\citep{zhang2023artificial}, transformers~\citep{zhao2023pinnsformer}, long short-term memory (LSTM)  networks~\citep{cho2022lstm,nathasarma2023physics}, information-botleneck inspired architectures~\citep{guo2024ib} and reinforcement learning models~\citep{banerjee2023survey,ramesh2023physics,radaideh2021physics}.

##### Residual Connections

Another approach to improve the PIML architecture performance is by adding residual connections (see Fig.~\ref{RM_enh}), which enable accurate calculation of high-order derivatives. The general formulation for a single additive skip connection is defined as follows:

$$
\begin{aligned}
    \bm{z}^{(l) }(\bm{z}^{(l-1) },\theta) &=\mathcal{M}^{(l) }(\bm{z}^{(l-1) },\theta) +\bm{z}^{(l-1) },
\end{aligned}
$$

where $\bm{z}^{(l) }=\{z_{0}^{(l) },\ldots,z_{H}^{(l) }\}$ are the outputs of layer $l$, $\mathcal{M}^{(l) }$ is a layer of the representation model (e.g., MLP layer, KAN layer), and $\bm{z}^{(l-1) }=\{z_{0}^{(l-1) },\ldots,z_{H}^{(l-1) }\}$ are the inputs to layer $l$. Several studies have explored the advantages of incorporating skip connections via addition or multiplication. Wang et al.~\citep{wang2021understanding} pioneered this approach by introducing a method referred to as modified MLP, where two single-layer MLPs project the model inputs ($\bm{x}$)  into a high-dimensional feature space, which is then used to update the remaining hidden layers via element-wise multiplication and addition. Other approaches introduce multiplicative connections between every layer~\citep{jiang2024densely}. Finally, the modified MLP was further improved by incorporating adaptive residual connections that incorporate a new learnable parameter that controls the contribution of the deeper layers with respect to the input~\citep {wang2024piratenets}. This improved architecture enables the use of deeper networks without compromising the accuracy of PIML problems.

##### Model Decomposition

Further improvements can be achieved by splitting the network into several components. Cho et al.~\citep{cho2024separable} proposed separable PINNs (sPINNs), which utilize separate sub-networks to approximate the desired solution.Under this formulation, the solution of a PDE is approximated as $\hat{u}(x_1,\cdots,x_d)  = \sum_{j=1}^p \prod_{i=1}^d \mathcal{M}_{j}(x_i,\theta_i)$
, where $\{\mathcal{M}_j(x_i,\theta_i) \}_{j=1}^p$ are the outputs univariate representation models that encodes each dimension $x_i$ into a $p$-dimensional space. This decomposition addressed the
curse-of-dimensionality and was introduced to the deep learning community as tensor neural networks in~\citep{wang2022tensor_integral}. While~\citep{wang2022tensor_integral} focused on improving accuracy through enhanced numerical integration methods,~\citep{cho2024separable} prioritized computational efficiency, achieving up to 60-times speedups for high-dimensional problems, such as the 3D Helmholtz Equation and the 4D Navier-Stokes Equation. This formulation has been further explored and improved in subsequent studies~\citep{wang2024tensor,vemuri2024functional,hu2024tackling}.

Other types of decompositions have also been explored for inverse problems~\citep{toscano2024invivo,zhou2023flow}. For instance,~\citep{toscano2024invivo} extended the negative log-likelihood (NLL)  framework~\citep{lakshminarayanan2017simple} for linear PDEs in PIML, enabling the quantification of aleatoric uncertainty and improving model performance. The authors assumed that the observed data was corrupted by noise, so they decomposed the desired solution into mean fields and fluctuations as $u=\bar{u}+u'$. Then they used independent models to learn the data-driven mean fields \(\bar{u}\) and their corresponding fluctuations, standard deviations \(u'\)  by using the NLL criterion~\citep{quinonero2006machine,lakshminarayanan2017simple,cawley2005estimating}. On the other hand,~\citep {zhou2023flow} proposed a method to simultaneously reconstruct flow states and determine particle properties from Lagrangian particle tracking (LPT) using a neural network as a flow model and a data-constrained polynomial as a particle model.

### 3.2.Governing Equations

PIML aims to obtain a representation model \( u \) that adheres to the governing equations. In the original study by~\citep{raissi2019physics}, the ODE/PDE and boundary conditions (BCs) are enforced by iteratively minimizing the strong-form residuals from the governing equations. The ODE/PDE residuals \( r_E(u, \theta) \) and boundary conditions (or initial conditions) \( r_B(u, \theta) \) are defined as:

$$
\begin{aligned}
    r_E(x,\theta) &= \mathcal{F}_{\tau}[u](x,\theta) - f(x), \quad x \in \Omega_E, \\
    r_B(x,\theta) &= \mathcal{B}_{\tau}[u](x,\theta) - b(x), \quad x \in \Omega_B.
\end{aligned}
$$

These residuals quantify the extent to which the approximation \( u \) satisfies the ODE/PDE and boundary constraints specified in Eq.~\ref{eq:problem}. If \( r_E = 0 \) and \( r_B = 0 \), the approximated solution  satisfies the PDE and BCs exactly.

For inverse problems, it is necessary to incorporate additional observations within the domain. The disagreement between the observations and predictions can be quantified using a data residual \( r_D(x,\theta) \), defined as:

$$
\begin{aligned}
\label{data_res}
    r_D(x,\theta) &= u(x,\theta) - \hat{u}(x), \quad x \in \Omega_D,
\end{aligned}
$$

where $\Omega_D\subseteq\Omega$ is the data domain. The objective of PIML is to find a solution that adheres to both the physical laws and the observational data.

#### 3.2.1.Derivative Calculation

To enforce governing laws, it is necessary to compute the spatial and temporal derivatives of the approximated solution in order to construct and penalize PDE residuals. In the original formulation by~\citep{raissi2019physics}, these derivatives were computed exactly using automatic differentiation (AD). AD leverages the fact that all numerical computations are ultimately compositions of a finite set of elementary operations, for which derivatives are known~\citep{baydin2018automatic,raissi2024forward}. However, AD significantly increases computational cost due to the need for calculating and multiplying gradients at each layer, which can become inaccurate for higher-order derivatives~\citep{wang2024piratenets} and infeasible for fractional operators or high-dimensional problems~\citep{hu2024tackling}. To address these challenges, several studies have explored alternatives to or enhancements of backpropagation.

##### Alternative Differentiation Methods

~\citep{lim2022physics} proposed approximating derivatives using finite differences, which speeds up computation; however, this method relies on a predefined grid, limiting its broader applicability. Other approaches~\citep{gladstone2022fo,buzaev2024hybrid} involve predicting derivatives as additional network outputs and learning the relationship through an auxiliary loss function. To handle fractional derivatives, several studies have employed Monte Carlo methods~\citep{pang2019fpinns,mehta2019discovering,ren2023class,wang2024gmc,sivalingam2024physics,hu2024tackling_fractional}.

###### High-Dimensions

One of the main advantages of PIML methods is their ability to handle high-dimensional problems~\citep{raissi2024forward,wang20222}. However, computing derivatives with AD becomes particularly challenging in such cases since the requirements for derivative calculation increase with the number of dimensions. Alternative approaches to AD have been proposed, particularly for high-dimensional problems. For instance,~\citep{he2023learning} introduced a Gaussian-smoothed model with Stein’s identity to parameterize PINNs, bypassing backpropagation and accelerating convergence. Additionally,~\citep{hu2024tackling} proposed Stochastic Dimension Gradient Descent (SDGD), a method that decomposes the gradient of PDEs and PINN residuals into components corresponding to different dimensions. During each training iteration, a subset of these dimensional components is randomly sampled, resulting in a highly efficient approach that enables solving PDEs with up to 100,000 dimensions.

#### 3.2.2.ODE/PDE Reformulations

To enhance the performance of PIML models, several studies have proposed reformulations of the ODE/PDEs.

##### Non-dimensionalization

As discussed in~\citep{wang2023expert}, one of the simplest and most effective ways to improve model performance is through non-dimensionalizing the governing equations. In this approach, the inputs and predicted outputs are scaled using characteristic units, helping to identify important non-dimensional numbers (e.g., Reynolds, Peclet, Prandtl, Rayleigh) that characterize the solution's behavior. Additionally, choosing appropriate characteristic units can control the magnitude of the inputs and outputs, which is crucial for stabilizing the training process. Several studies have successfully approximated solutions to PDEs in their non-dimensional form~\citep{raissi2020hidden,jin2021nsfnets,cai2021artificial,cai2021flow,toscano2024inferring,toscano2024invivo,wang2021understanding,wang2023solution}.

##### Equivalent and Auxiliary Formulations

Another approach to improving model performance involves transforming the governing equations into an equivalent form that simplifies the optimization problem. For example, Wang et al.~\citep{wang2021understanding} and subsequent studies~\citep{anagnostopoulos2024learning} solved the Navier-Stokes equations using the streamfunction formulation, which inherently satisfies the conservation of mass, thereby reducing the number of constraints to optimize. Similarly, Jin et al.~\citep{jin2021nsfnets} reformulated the Navier-Stokes equations into their vorticity formulation, achieving better performance. Basir et al.~\citep{basir2022investigating} introduced an auxiliary vorticity variable, which lowered the order of the Stokes equations, further simplifying the problem. In some cases, these reformulations are necessary to achieve an acceptable solution. For instance, Toscano et al.~\citep{toscano2024inferring} reformulated the Rayleigh-Bénard equations using the vorticity formulation, eliminating the pressure dependence and enabling the inference of temperature from sparse turbulent velocity data. Similarly, Wang et al.~\citep{wang2023solution} reformulated the Navier-Stokes equations with an entropy-viscosity method, allowing for the approximation of solutions at high Reynolds numbers.

#### 3.2.3.Differential Operator Variations

The PIML approach is flexible enough to solve several types of problems even with multiple solutions; for instance, Huang et al.~\citep{huang2022hompinns} proposed homotopy physics-informed neural networks (HomPINNs) for solving multiple solutions of nonlinear elliptic differential equations. This flexibility allows handling residuals in their strong or weak form and obtaining approximate solutions from different types of operators, leading to various PIML extensions.

##### Variational Methods

Several studies have proposed weakly enforcing the PDE and boundary constraints by solving the problem in its variational form. This approach, known as variational PINNs (vPINNs), was introduced by~\citep{kharazmi2019variational}. Similar to the Deep Ritz Method~\citep{yu2018deep}, vPINNs compute weighted integrals of the residuals by projecting them onto a suitable space of test functions \( V \)~\citep{kharazmi2021hp}. In the variational form, the residuals are represented as:

$$
\begin{aligned}
    \mathcal{R}_{e,j}(u) &= \int_{\Omega} r_e(x,\theta) v_j \, dx, \\
    \mathcal{R}_{b,j}(u) &= \int_{\Omega_B} r_b(x,\theta) v_j \, dx,
\end{aligned}
$$

where \( v_j \) represents a chosen test function. Ideally, the exact solution is obtained when all residuals are identically zero~\citep{kharazmi2021hp}. Various types of test functions have been proposed, including Dirac-delta functions~\citep{raissi2019deep}, global~\citep{kharazmi2019variational}, piece-wise~\citep{khodayi2020varnet} polynomials, and non-overlapping functions~\citep{kharazmi2021hp}. The vPINN formulation has also been explored in the context of mesh-free methods~\citep{berrone2024meshfree}, variable coefficients~\citep{miao2023vc}, volume-weighted methods~\citep{song2024vw}, and has been optimized for computational efficiency~\citep{ghose2024fastvpinns,anandh2024efficient}.

##### Fractional Differential Equations

Another extension involves fractional operators, giving rise to the fractional PINNs (fPINNs) framework, first introduced by~\citep{pang2019fpinns}. Several studies have explored and expanded upon the fPINN formulation~\citep{mehta2019discovering, ren2023class, guo2022monte, wang2024gmc, sivalingam2024physics, hu2024tackling_fractional}. For example,~\citep{wang2024gmc} proposed a Monte Carlo-based method to solve fractional partial differential equations on irregular domains. Furthermore,~\citep{sivalingam2024physics} provided a theoretical analysis to estimate the training and generalization errors for the \(\psi\)-Caputo type fractional PDE. Lastly,~\citep{hu2024tackling_fractional} extended the fPINN framework to overcome the curse of dimensionality in fractional and tempered fractional PDEs.

##### Stochastic Differential Equations

Stochastic Differential Equations (SDEs) have also been explored within the PIML framework. For instance,~\citep{yang2020physics} utilized GANs as a representation model and applied automatic differentiation to encode the governing laws for solving SDEs. Similarly,~\citep{zhang2020learning} combined spectral dynamically orthogonal (DO) and dynamically biorthogonal (BO) methods with PIML to develop two novel PINN approaches for solving time-dependent SDEs. Furthermore,~\citep{raissi2024forward} extended the PIML formulation to solve high-dimensional SDEs; see also~\citep{hu2024score} for more recent PINN algorithms for high-dimensional Fokker-Planck and Hamilton-Jacobi-Bellman equations.

### 3.3.Optimization Process

Training a PIML model involves solving an optimization problem that enables a representation model to approximate the solution of a governing equation. The model parameters are learned by optimizing a loss function (i.e., objective function), which minimizes the residuals (i.e., Eqns. \ref{PDE_res},~\ref{bcs_res}, and~\ref{data_res}) of the governing equations, boundary conditions, and data observations. This loss function is minimized using an optimization algorithm, often referred to as an ``optimizer." Based on this description, the optimization process in PIML can be broken down into three main subcomponents: the optimization problem, the loss (i.e., objective) function, and the optimizer.

#### 3.3.1.Optimization Problem

The optimization problem in PIML can be summarized as minimizing a multi-objective loss function that encourages the model to satisfy constraints related to boundary conditions, governing equations, and potentially observational data. To simplify this problem, several studies have sought to reduce the number of constraints. For instance, in systems of PDEs,~\citep{shukla2024neurosem} proposed using numerical solvers to provide easily accessible high-fidelity data, using the PIML model primarily to uncover hidden fields that are difficult to recover with traditional methods. Other approaches aim to simplify the problem through domain decomposition, learning from low-fidelity data, and sequential training strategies.

##### Domain Decomposition

The baseline approach for domain decomposition was introduced in the extended PINNs (XPINNs) framework~\citep{jagtap2020extended} and can be summarized as follows. First, the training domain \( \Omega \) is divided into \( N \) smaller subdomains \( \Omega_i \subset \Omega \). Then, \( N \) PIML submodels are trained to approximate the solution on each subdomain \(\Omega_i \). Various studies have explored methods for optimally partitioning \( \Omega_i \) and ensuring communication between the submodels. For example,~\citep{jagtap2020extended,hu2021extended,de2024error} proposed incorporating an additional loss term for interface conditions to ensure that submodel predictions and residuals align at the subdomain boundaries. Conservative PINNs (cPINNs)~\citep{jagtap2020conservative} follow a similar approach, ensuring the continuity of fluxes across subdomains, akin to traditional numerical methods. Additionally,~\citep{hu2023augmented} introduced a soft domain decomposition method using a shared subnetwork to route inputs into different submodels. Similar methods have also been published in~\citep{meng2020ppinn, shukla2021parallel, moseley2023finite, dolean2024multilevel, liu2023cv, kharazmi2021hp, nguyen2022efficient, kopanivcakova2024enhancing}, achieving accelerated training, easy parallelization, and improved accuracy.

##### Sequential Training

Sequential training can be viewed as a form of ``problem decomposition," where the model learns or satisfies objectives sequentially. Ahmadi et al.~\citep{CMINNs} proposed a novel method for addressing specific challenges in inverse problems, where the values of constant parameters change at specific times and the system experiences abrupt spikes due to sudden input changes. In this case, it is essential to train the model on sequential intervals, which are strategically chosen to capture the spikes observed in the system. For example, in the CMINNs method, the tumor growth model following drug administration exhibits spikes that cannot be effectively represented without decomposition. Furthermore, domain decomposition is necessary for the inference of piecewise-constant parameter values over continuous intervals, enhancing the ability to capture variations in drug efficacy and providing insights into tolerance phenomena in multi-dose administration.

###### Time Decomposition

For time-dependent problems, several studies have proposed training the model over a short time interval before gradually expanding the training window until the entire time domain is covered~\citep{wight2020solving,krishnapriyan2021characterizing,mattey2022novel,haitsiukevich2023improved,wang2022respecting, chen2024leveraging2}.
A unified approach of the various proposed methods for wave propagation problems was presented in~\citep{penwarden2023unified}.

###### Transfer Learning and Curriculum Training

These strategies involve initializing the model parameters by pretraining on a simpler problem and then retraining on the target problem. Once the model is pre-trained, transfer learning approaches~\citep{desai2021one,liu2023adaptive,xu2023transfer,chen2024enhancing,chen2024leveraging} demonstrate that fine-tuning only the final layers can significantly improve model performance. Curriculum training, by contrast, progressively increases the problem complexity and has proven useful for systems of PDEs~\citep{krishnapriyan2021characterizing,zhang2024meshless,wang2023expert,wang2024piratenets} as well as inverse problems~\citep{ahmadi2024ai,CMINNs,toscano2024invivo,toscano2024inferring}. For instance,~\citep{wang2023expert} solved the 2D Navier-Stokes equations at high Reynolds numbers (Re) by gradually increasing
the Reynolds number during training. Notably,~\citep{wang2024piratenets} proposed initializing the last layer (i.e., linear) using least squares and then employing an adaptive residual connection that progressively includes deeper layers as necessary. Other curriculum training methods include pre-training on simulated data or theoretical models, alternating between learned fields~\citep{zhang2024meshless}, or adjusting the loss function to focus on refining the solution~\citep{toscano2024invivo,toscano2024inferring}. For inverse problems that require inferring hidden fields,~\citep{toscano2024inferring} suggested first fitting the data and boundary conditions, then learning a theoretical representation of the hidden field, and finally incorporating the full physics. Using this approach, the authors successfully inferred hidden temperature fields from sparse experimental turbulent velocity data.

###### Multi Fidelity and stacked training

Multifidelity and stacked training methods have been employed to improve the prediction accuracy of PINNs.
Mutlifidelity PINNs proposed in~\citep{meng2020composite} provide a framework for integrating low- and high-fidelity data.
This model involves four neural networks: the first approximates low-fidelity data, the second and third learn linear and nonlinear correlations between the low- and high-fidelity data, and the final network encodes the underlying PDEs.
The multifidelity approach has been extended to Bayesian networks, quantifying uncertainties in the prediction~\citep{meng2021multi}.
Another approach to learning from multi-fidelity data has been proposed in~\citep{regazzoni2021physics}, involving a NN to approximate the low-fidelity data and a subsequent NN to learn the correction term using the high-fidelity data and a physics-informed loss.
Multifidelity approaches have been successfully applied to solve the inverse-water wave problem governed by Serre-Green-Naghdi equations~\citep{jagtap2022deep}. Stacked training techniques, presented in~\citep{howard2023stacked,wang2024multi}, is another approach to improve the prediction accuracy in PINN.
By stacking networks sequentially, the output from each step serves as low-fidelity input for subsequent stages, allowing the model to refine predictions progressively.
This approach has been further improved in~\citep{heinlein2024multifidelity} by combining multi-fidelity stacking with domain decomposition methods, making it practical for multiscale time-dependent problems. A multi-stage neural network was introduced in~\citep{wang2024multi} to tackle spectral bias by dividing the training into stages, where each stage involves training a new NN to fit the residue from the previous stage. Also,~\citep{penwarden2022multifidelity} presented an approach to combine different neural networks of varying fidelities by exploiting their low-rank structure.

#### 3.3.2.Loss Function Modifications

The loss function (\( \mathcal{L} \)) quantifies the disagreement between the approximation provided by the representation model and known information from the PDE solution, such as boundary conditions, ODE/PDE residuals, and data. Training in PIML involves iteratively minimizing the loss subcomponents (\( \mathcal{L}_\alpha \)) over their respective domains \( \Omega_\alpha \). In general, \( \mathcal{L} \) can be roughly represented as:

$$
\begin{aligned}
\mathcal{L}(\theta) &= \sum_{\alpha \in C} m_\alpha \int_{\Omega_{\alpha}} f_\alpha(r_\alpha(x, \theta)) dx,
\end{aligned}
$$

where \( C = \{D, B, E, \dots \} \) is an index specifying the loss groups, e.g., data (\( \mathcal{L}_D \)), boundary (\( \mathcal{L}_B \)), and equation (\( \mathcal{L}_E \)). The function \( f_\alpha: \mathbb{R} \rightarrow \mathbb{R}^{+} \) is a positive, preferable convex, function applied to the residuals \( r_\alpha \) of each subcomponent (i.e., Eqns.~\ref{bcs_res},~\ref{PDE_res}, and~\ref{data_res}), and \( m_\alpha \) are scalar weights that balance the contributions of each term, often referred to as global weights.

To enable computation, this expression is typically discretized and computed iteratively over finite subdomains \( X_{\beta,\alpha} \subset \Omega_\alpha \) as follows:

$$
\begin{aligned}
\mathcal{L}(\theta, X_{\alpha,\beta}) &= \sum_{\alpha \in C} m_\alpha \sum_{i=1}^{N_{\beta,\alpha}} \lambda_{\alpha,i} f(r_\alpha(x_i, \theta)), \quad \text{where } x_i \in X_{\alpha,\beta},
\end{aligned}
$$

where \( N_{\beta,\alpha} \) is the batch size, representing the number of points \( x_i \) in the subset \( X_{\beta,\alpha} \), sampled from a probability density function \( p_\alpha \) over the domain \( \Omega_\alpha \). Similar to weighted Monte Carlo methods~\citep{liu2001monte}, the discrete residuals \( f(r_\alpha(x_i, \theta)) \) are scaled using pointwise multipliers, referred to as local weights \( \lambda_{\alpha, i} \).

Defining a suitable \( \mathcal{L} \) is crucial to ensure that model predictions align with the PDE solution, and ongoing research has focused on refining four key components: the global weights (\( m_\alpha \)), the local weights (\( \lambda_{\alpha,i} \)), the choice  of function (\( f_\alpha \)), and the sampling strategy, which is indirectly based on the probability density function (\( p_\alpha \)) and the number of points (\( N_\alpha \)).

##### Global Weights

The global weights (\( m_\alpha \)) balance the contribution of each loss subterm and ensure that all constraints are satisfied. These weights can be either fixed, as in the original PIML framework~\citep{raissi2019physics,raissi2020hidden}, or dynamic, adjusting their magnitude during training. In particular, Wang et al.~\citep{wang2021understanding} proposed a learning rate annealing algorithm that dynamically adjusts the global weights based on back-propagated gradients. This approach improved performance and was successfully applied in diverse applications~\citep {jin2021nsfnets,boster2023artificial,cai2021artificial}. Similarly, self-adaptive loss balancing methods, such as those proposed by~\citep{xiang2022self}, also dynamically adjust global weights during training. Liu et al.~\citep{liu2021dual} developed a dual-dimer method for training PINNs with a minimax architecture, optimizing the global weights to manage complex multi-objective problems. Additionally, Basir et al.~\citep{basir2022investigating} explored failure modes in PINNs and refined global weight strategies to enhance model robustness. Finally, specific forms of sequential training can be seen as iteration-based weight adjustments, where the model’s learning process evolves. For example, Wang et al.~\citep{wang2022respecting} introduced a causal parameter for time-dependent problems, which forces the model to learn sequentially, adjusting the weights based on time steps to capture causality. Overall, global weights—whether static, dynamic, or iteration-based—play a critical role in ensuring the effectiveness of PIML models, and ongoing research continues to refine these weights to improve performance. In particular~\citep{wang2022and} extended the Neural Tangent Kernel(NTK) to PIML and proposed a novel adaptive global weighting training strategy that significantly enhance the trainablity and predictive accuracy of PIML models.

##### Local Weights

One of the main challenges in PIML is that residuals at key points can be underrepresented in the overall summation of the objective function (Eq.~\ref{loss_dic}). As a result, despite a decrease in total loss during training, certain spatial or temporal characteristics might not be fully captured. This issue becomes particularly pronounced in multiscale problems, where regions of interest may lack detail, and important information from the initial and boundary conditions may not propagate effectively through the domain, thereby hindering convergence~\citep{anagnostopoulos2024residual}. To address this issue, researchers have proposed assigning local weights \( \lambda_{\alpha,i} \) to balance the contribution of each residual point, thus increasing focus on the challenging regions in both space and time dimensions.

McClenny et al.~\citep{mcclenny2023self} introduced a self-adaptive (SA) approach, where individual loss weights are adjusted through adversarial training. Building on this idea, Zhang et al.~\citep{zhang2023dasa} proposed a differentiable adversarial self-adaptive (DASA) weighting scheme, which uses a subnetwork to optimize the local multipliers. Basir et al.~\citep{basir2022physics} developed the physics and quality-constrained artificial neural network (PECANN), which calculates local weights based on the residuals of constraints, such as initial and boundary conditions, using the augmented Lagrangian method. PECANN has since been expanded with adaptive versions, such as PECANN-AL, which incorporate global Lagrange multipliers~\citep{basir2022investigating,basir2023adaptive}. Similarly, Son et al.~\citep{son2023enhanced} introduced an augmented Lagrangian relaxation method for PINNs (AL-PINNs), where the initial and boundary conditions act as constraints to optimize the PDE residual.

Anagnostopoulos et al.~\citep{anagnostopoulos2023residual} introduced residual-based attention (RBA) weights, where \( \lambda_{\alpha,i} \) is computed using the exponentially weighted moving average of the residuals. Since residuals contain information about regions with high error, this method proved to be highly effective, outperforming previous approaches with minimal computational cost. These techniques have been further developed and refined in subsequent studies~\citep{song2024loss,shukla2024comprehensive,ramirez2024residual,chen2024self}. Notably, Chen et al.~\citep{chen2024self} extended both SA and RBA methods using the Neural Tangent Kernel~\citep{wang2022and} and analogies to traditional numerical methods, resulting in a robust and improved algorithm for calculating \( \lambda_{\alpha,i} \).

##### Sampling

As shown in Eq.~\ref{loss_dic}, PIML models are iteratively optimized on \( N_\alpha \) points from a discrete subdomain \( X_{\beta,\alpha} \), which is sampled from \( \Omega_{\alpha} \) using a suitable probability density function \( p_\alpha \). In this context, the sampling method refers to the specific choice of \( N_\alpha \) and \( p_\alpha \), which define the number and distribution of the training points.

Sampling methods in PIML can be categorized based on uniformity, adaptability, and selection criteria. In terms of uniformity, these methods are divided into uniform and non-uniform sampling techniques. Early approaches employed simple methods like equispaced grids and uniformly random sampling~\citep{lu2021deepxde, wu2023comprehensive}. Later, more sophisticated non-adaptive uniform sampling techniques were introduced, such as Latin Hypercube Sampling (LHS), Halton, Hammersley, and Sobol sequences.

However, recent studies have demonstrated that uniform sampling methods are often insufficient, particularly for solving PDEs with sharp gradients~\citep{wu2023comprehensive}. This has led to the development of adaptive sampling methods, which dynamically adjust the sampling of residual points based on certain criteria. Broadly, adaptive sampling can be classified into two main strategies: (1) \textbf{Resampling} (adaptive \( p_\alpha \)), where all residual points are resampled after a fixed number of iterations according to the specified criteria, and (2) \textbf{Incremental sampling} (adaptive \( N_\alpha \)), where an initial set of residual points is sampled, and additional points are incrementally added during training, guided by either the same or different criteria.

Adaptive sampling methods employ various selection criteria. While uniform sampling methods may serve as a baseline, non-uniform criteria are more commonly used. The most prevalent approach defines \( p_\alpha \) to be proportional to the PDE residual, concentrating the sampling in regions where the residual is large~\citep{wu2023comprehensive, daw2022rethinking, gao2023active, tang2021deep, peng2022rang, zeng2022adaptive, hanna2022residual, subramanian2023adaptive, nabian2021efficient, zapf2022investigating, toscano2024inferring,daw2022mitigating}. This strategy enhances the distribution of residual points by focusing on areas that contribute most significantly to the PDE loss. Alternatively, other methods prioritize high-error regions without directly relying on the PDE residual. For example,~\citep{toscano2024inferring} proposed using pre-computed local multipliers \( \lambda_{\alpha,i} \), which encode historical information about high-error regions, to update \( p_\alpha \) at every training iteration with negligible computational cost. Another approach, failure-informed adaptive sampling~\citep{gao2023failure, gao2023failure2}, defines a failure probability function and adds new residual points (i.e., increases \( N_\alpha \)) in regions where this probability exceeds a predefined threshold.

##### Function Selection

As shown in Eq.~\ref{loss_cont}, \( f_\alpha: \mathbb{R} \rightarrow \mathbb{R}^{+} \) is a positive, preferable convex function applied to the residuals \( r_\alpha \) (i.e., Eqns.~\ref{bcs_res},~\ref{PDE_res}, and~\ref{data_res}). This function aims to transform the residuals and give $\mathcal{L}$ the required characteristics to be optimizable. The most prevalent choice of this function is  $f_\alpha(z,p)=|z|^p$, which transforms $\mathcal{L}$ into the the sum of $L^p$ norm of the residuals for each loss $\mathcal{C}$ subcomponent,($\mathcal{L}=\sum_{\alpha\in\mathcal{C}}\|r_{\alpha}(x)\|^p_p$). Notice that, as described in~\citep{wang20222}, by setting $p=2$ and discretizing with $\lambda_{\alpha,i}=(1/N)$, we recover the mean-squared error as introduced in the first PINN study~\citep{raissi2019physics} and broadly adopted in the PIML community. However, the $L^2$ norm tends to be sensitive to outliers and diminish small values, so it struggles capturing small details; thus, several studies propose training by a combination of $L^1$ and $L^2$ using a sequential approach~\citep{toscano2024inferring} or adaptively via the Huber Loss~\citep{chen2024enhancing,bullwinkel2022deqgan}. On the other hand, Wang et al.,~\citep{wang20222} investigated the relationship between the loss function and the approximation quality of the learned solution and proved that for general $L^p$ loss, several types of equations are stable only if $p$ is sufficiently large and developed a noble algorithm to minimize the $L^{\infty}$ loss.  Other types of functions have also been explored. For instance, ~\citep{urban2024unveiling} experimentally showed that using $f_\alpha(z)=log(z)$ or $f_\alpha(z)=\sqrt{|z|}$ can significantly improve the baseline model performance. Other studies~\citep{toscano2024invivo} proposed using the negative-log-likelihood (NLL), introduced in~\citep{quinonero2006machine,lakshminarayanan2017simple,cawley2005estimating}, for inverse problems in PIML enabling obtaining the aleatoric uncertainty and improving the model performance. Similarly,~\citep{zhou2023stochastic} proposed the stochastic particle advection velocimetry method, which introduces a statistical data loss that improves the accuracy of inverse problems in fluid dynamics. This method is based on an explicit particle advection model that predicts particle positions over time as a function of the estimated velocity field.

#### 3.3.3.Optimizer

The goal of the optimizer is to find the optimal parameters \( \theta \) so that the approximated solution matches as close as possible to the true solution. This process is performed iteratively by gradually minimizing \( \mathcal{L} \) until a desired accuracy is reached. As described in~\citep{urban2024unveiling}, the most common optimizers in PIML fall under the general family of Line Search Methods~\citep{nocedal1999numerical}, where \( \theta \) are updated as follows:

$$
\begin{aligned}
\theta^{k+1}&=\theta^{k}+\alpha^{k}p^{k},\\
p^{k}&=-H_k\nabla_{\theta}\mathcal{L}(\theta^k).
\end{aligned}
$$

Here, \( \alpha^k \) is the step size at iteration \( k \), and \( p^k \) is the step direction, which depends on the loss gradient \( \nabla_{\theta}\mathcal{L}(\theta^k) \) and a symmetric matrix \( H^k \). Under this formulation, linear methods such as gradient descent or ADAM~\citep{kingma2014adam} are recovered by setting \( H_k = I \). On the other hand, quasi-Newton methods, such as L-BFGS~\citep{liu1989limited}, can be recovered by setting \( H^k \) to an approximation of the Hessian matrix of \( \mathcal{L} \), which requires only first-order derivatives and helps achieve superlinear convergence~\citep{urban2024unveiling}.

Due to their computational efficiency and mini-batch flexibility, linear methods are widely used in the PIML community, and several studies have focused on improving their performance~\citep{lu2023nsga,liu2024config,zhou2023generic,yao2023multiadam,fang2023ensemble}. For instance,~\citep{cyr2020robust} proposed adopting an adaptive basis viewpoint of neural networks, which led to novel initialization and a hybrid least squares/gradient descent optimizer. Similarly, \citep{ainsworth2021plateau} proposed an iterative training method, the Active Neuron Least Squares, characterized by explicitly adjusting the activation pattern at each step, designed to enable a quick exit from a plateau. Also, \citep{ainsworth2022active} proposed augmenting the standard gradient descent direction by including search vectors, which are chosen to explicitly adjust the activation patterns of the neurons, which improved the performance of two-layer rectified neural networks. On the other hand,~\citep{lu2023nsga} proposed using Non-dominated Sorting Genetic Algorithms (NSGA) to help traditional stochastic gradient optimization algorithms escape local minima. Similarly~\citep {akhter2024common} analyzed the Pareto front, highlighted the most common pitfalls for multi-objective problems, and compared the standard methods with NSGA-II.

Other studies have focused on improving gradients in multi-task learning~\citep{liu2024config, zhou2023generic, yao2023multiadam}. For example,~\citep{liu2024config} introduced a conflict-free update algorithm to handle multi-objective optimization, ensuring a positive dot product between the final update and each loss-specific gradient. Conversely,~\citep{davi2022pso} proposed replacing gradient descent with particle swarm optimization, which not only improves performance but also allows for the computation of related uncertainties. Finally,~\citep{jnini2024gauss} propose Gauss-Newton's method in function space for the solution of the Navier-Stokes equations. Upon discretization, this yields a natural gradient method that mimics the function space dynamics and allows the authors to achieve close to single-precision accuracy in the relative $L^2$ norm~\citep{jnini2024gauss}.

In some applications, quasi-Newton methods such as BFGS or L-BFGS~\citep{liu1989limited} can be used to achieve better performance with fewer iterations, though they are more prone to getting trapped at saddle points~\citep{urban2024unveiling}. To mitigate this, some studies recommend using Adam during the initial stages of training, followed by L-BFGS for fine-tuning~\citep{raissi2019physics, jin2021nsfnets, urban2024unveiling}. Given the effectiveness of quasi-Newton methods, recent research has aimed at further enhancing their performance.  For instance,~\citep{urban2024unveiling} proposed a modified BFGS algorithm, demonstrating that by selecting an improved optimization method and incorporating modifications to the loss function, the accuracy of solutions comparable to finite-difference schemes can be achieved in specific examples. On the other hand,~\citep{rathore2024challenges} theoretically analyzed PIML ill-conditioning and introduced a novel second-order optimizer that significantly improves PIML performance. Similarly~\citep{muller2023achieving} proposed energy natural gradient descent with respect to a Hessian-induced Riemannian metric as an optimization
algorithm for PINNs yielding highly accurate solutions for shallow networks, outperforming ADAM and LBFGs. This study was extended to deeper networks and further improved in\citep{dangel2024kronecker}. Finally,~\citep{lee2024two} introduced a two-level overlapping additive Schartz preconditioner strategy that can be combined with any optimizer to accelerate the training of PIML problems.

## 4.Applications of PIML

Numerous studies have demonstrated the success of PIML across a wide range of fields. Here, we provide a selective yet comprehensive review of PIML applications in biomedicine, mechanics, geophysics, dynamical systems, control and autonomy, heat transfer, physics, chemical engineering, and other miscellaneous areas.

### 4.1.Applications in Biomedicine

The potential of PINNs have been demonstrated across a wide range of biomedical applications, including systems biology, systems pharmacology, biomechanics, and epidemiology. By integrating biophysical laws with data-driven learning, PINNs offer a powerful framework for solving complex problems in these fields.

In systems biology, for example, PINNs have been employed to model dynamic biological processes. Notably, the work by~\citep{yazdani2020systems} and~\citep{daneker2022systems} successfully applied PINNs to model complex biochemical reactions and conduct parameter identification for system-level biological processes. Building on this, the AI-Aristotle framework~\citep{ahmadi2024ai} has further demonstrated the adaptability of PINNs in both systems biology and systems pharmacology gray-box discovery, showing how these models can handle the intricate dynamics of biological systems~\citep{ahmadi2024ai}. Beyond these applications, PINNs have also been leveraged to explore cellular signaling pathways. For instance,~\citep{jo2024density} introduced Density-PINNs to infer transduction-time distributions in cellular signaling, providing insights into how response times contribute to cell-to-cell heterogeneity. This understanding has the potential to inform more effective disease treatment strategies.

Transitioning to cardiac health, PINNs have made significant strides in diagnosing and treating atrial fibrillation. They have been used to create detailed activation maps for diagnosis and estimate cardiac fiber orientation from electroanatomical data, both of which are essential for personalized treatment and procedural planning~\citep{sahli2020physics, ruiz2022physics}. Additionally, in the area of blood coagulation—a critical process in hemostasis—PINNs address challenges in parameter estimation due to the difficulty of measuring reaction rates. The introduction of Coagulo-Net~\citep{QIAN2024106732} demonstrated how PINNs can infer unknown parameters and dynamics from sparse and noisy data, offering a robust solution to modeling blood clotting. Similarly, TGM-Net has been proposed in~\citep{chen2023tgm} to facilitate tumor growth forecasting. Finally, in electrophysiology, the precise simulation of action potentials and the estimation of electrophysiological tissue properties are essential for diagnosing and treating arrhythmias such as atrial fibrillation.~\citep{herrero2022ep} presented EP-PINNs, which offer highly accurate simulations and parameter estimation from sparse datasets, paving the way for better treatment outcomes.

In the field of systems pharmacology, PINNs solve the compartment model to predict the assimilation of drugs in the human body, enhancing the understanding of pharmacokinetics for better therapeutic outcomes~\citep{goswami2021study, ahmadi2024ai}. In this study~\citep{podina2024learning}, PINNs are employed to uncover unknown components in differential equations modeling chemotherapy pharmacodynamics. Additionally, a pharmacodynamic study~\citep{math12081195} explores the effectiveness of the Verhulst and Montroll models in simulating tumor cell growth through the application of PINNs. In a recent work~\citep{CMINNs}, Compartment Model Informed Neural Networks (CMINNs) was proposed as a means of transforming compartmental modeling, with the objective of enhancing pharmacokinetic (PK) and integrated pharmacokinetic-pharmacodynamic (PK-PD) modeling. This approach incorporates fractional calculus and  time-varying parameters, combined with constant or
piecewise-constant parameters, providing insights into drug dynamics in cancer and revealing the dynamics of cancer cell death in multi-dose administrations, which may exhibit resistance, persistence, and tolerance. Recent research demonstrates that using PINNs enables comprehensive mathematical modeling in systems toxicology, providing a deeper understanding of the effects of pharmaceutical substances on cardiac health~\citep{soukarieh2024hypersbinn}. The characterization of drug effects on cardiac electrophysiology by~\citep{chiu2024characterisation} using PINNs marks a pivotal development in predictive healthcare, enabling better management of arrhythmic disorders.

In the field of biomechanics, PINNs are revolutionizing diagnostics and treatment planning by seamlessly integrating physical laws with clinical data. PINNs have proven particularly effective in inferring blood flow dynamics from non-invasive imaging techniques, such as 4D flow MRI, enabling detailed analyses of cardiovascular conditions like intracranial aneurysms and stenosis~\citep{raissi2020hidden, kissas2020machine, sun2020surrogate, arzani2021uncovering, daneker2024transfer}. These networks are adept at handling noisy, sparse data, facilitating the calibration of conventional models, and predicting flow fields without extensive simulation data, thus enhancing diagnostic accuracy and efficiency~\citep{kissas2020machine, sun2020surrogate, arzani2021uncovering}. Moreover, PINNs contribute to the non-invasive inference of thrombus material properties, facilitating the estimation of permeability and viscoelastic moduli, which are vital for assessing treatment options~\citep{yin2021non}.
This technology also enhances the quality of medical imaging through super-resolution and denoising of 4D flow MRI, making it more reliable for clinical applications~\citep{fathi2020super, gao2021super}.
~\citep{liu2020generic} extended PINNs to model the mechanical behavior of soft biological tissues, providing crucial insights into tissue mechanics, which are essential for medical simulations and prosthetic design. Similarly,~\citep{lagergren2020biologically} utilized these networks to derive mechanistic insights from sparse experimental data, enhancing the modeling of biological phenomena. A novel method proposed in another study~\citep{SAINZDEMENA2024104092} employed PINNs to fit DCE-MRI data, incorporating contrast agent diffusion while ensuring compliance with mass conservation equations from the pharmacokinetic model, resulting in enhanced predictive accuracy.
Recent research has demonstrated the versatility of PINNs in modeling various aspects of tissue behavior, including thermal properties, biomechanical responses, and elasticity~\citep{Awojoyogbe2024,caforio2024physics, wu2024identifying}. Notably,~\citep{Ragoza2023} applied PINNs to tackle the inverse problem of tissue elasticity reconstruction in Magnetic Resonance Elastography, showcasing the potential of these methods for understanding and engineering biological tissues. Additionally,~\citep{movahhedi2023predicting} introduced a hybrid PINN that reconstructs 3D tissue dynamics from sparse 2D images by integrating fluid dynamics with soft tissue modeling for clinical diagnostics.

Applications in cardiovascular engineering are also profound, as evidenced by studies on intraventricular flow mapping~\citep{ling2024physics}, cuffless blood pressure estimation~\citep{sel2023physics}, and modeling aortic blood flow~\citep{du2023evaluation} using PINNs.~\citep{jagtap2023coolpinns} demonstrated the utilization of PINNs in modeling active cooling within vascular systems, indicating the broad utility of PINNs in thermoregulation studies within biological contexts.
Overall, PINNs are setting new standards in medical diagnostics and personalized medicine by providing a robust framework that integrates computational models with real-world medical data~\citep{raissi2020hidden, kissas2020machine, sun2020surrogate, arzani2021uncovering, goswami2021study, yin2021non, ruiz2022physics, cai2021artificial, fathi2020super, gao2021super}.

PINNs have been adapted to address a range of epidemiological models, including the susceptibles-infected-recovered frameworks and their extensions. For instance, bi-objective optimization has been employed to train PINNs on the Susceptible-Vaccinated-Infected-Hospitalized-Recovered (SVIHR)  model, which includes compartments for leaky-vaccinated and hospitalized populations, demonstrating a sophisticated approach to managing the trade-off between data fit and model fidelity~\citep{heldmann2022pinn}.
In a practical application, PINNs were used to model COVID-19 infection and hospitalization scenarios based on data from Germany, proving their effectiveness in comparison to traditional finite difference methods~\citep{treibert2022physics}. A recent work~\citep{epiii} developed a PINNs model for estimating temporal changes in the SIR model to analyze transmission dynamics during outbreaks. Furthermore, the concept of Disease-Informed Neural Networks (DINNs)  extends PINNs to predict the spread of various infectious diseases, showcasing the flexibility of PINNs in handling epidemiological data~\citep{shaier2022data}.
The Susceptible-Exposed-Infected-Recovered (SEIR)  model has been augmented to incorporate environmental pathogen concentrations. PINNs have been employed to address both forward and inverse problems, thereby improving the model’s effectiveness in analyzing COVID-19 dynamics that involve intricate interactions between humans and pathogens~\citep{nguyen2022modeling}. Lastly, PINNs have been integrated with the Extreme Theory of Functional Connections (X-TFC)  to dynamically estimate parameters across several compartmental models, illustrating their potential in refining epidemiological predictions and parameter discovery~\citep{schiassi2021physics}. This study~\citep{kharazmi2021identifiability} explores multiple epidemiological models using PINNs to identify time-varying parameters and fractional differential operators. Various extensions of the classic SIR model, including fractional-order and time-dependent parameters, are examined. By applying these methods to COVID-19 data from New York, Rhode Island, Michigan, and Italy, the work simultaneously infers both unknown parameters and unobserved dynamics. The research highlights the identifiability of model parameters and uncertainties related to neural networks and control measures, offering insights into pandemic forecasting.
These applications demonstrate the capability of PINNs to provide a deeper understanding of disease dynamics, making them invaluable in the ongoing efforts to manage public health crises.

### 4.2.Applications in Mechanics

#### 4.2.1.Fluid Mechanics

PINNs  have made significant strides across a wide array of applications in fluid mechanics, showcasing their versatility and robust capability to solve complex problems by integrating physical laws with machine learning.

In imaging and flow visualization, PINNs have been pivotal in inferring complex fluid dynamics such as the 3D velocity and pressure fields from temperature data in tomographic background oriented Schlieren imaging, exemplified by studies like the flow over an espresso cup~\citep{cai2021flow}.~\citep{cai2024physics} reconstructs the velocity field in a turbulent jet from sparse observations available from Particle Tracking Velocimetry (PTV). This technique also extends to biological flows, where PINNs accurately reconstruct pressure fields around swimming fish from Particle Image Velocimetry (PIV) data, providing insights that surpass traditional methods~\citep{calicchia2022reconstructing}.

For turbulent and high-speed flows, PINNs address both forward and inverse problems. They handle high-speed flows by approximating the Euler equations, solving problems involving moving discontinuities and oblique waves, and inferring flow properties from sparse data in supersonic environments~\citep{mao2020physics, jagtap2022physics}. In two-dimensional turbulence, PINNs aid in predicting flow quantities and improving the understanding of small-scale turbulence~\citep{kag2022physics}.

PINNs also explore complex fluid behavior in non-Newtonian fluids, learning the viscosity from velocity measurements, which helps in modeling the flow between parallel plates—a challenging scenario for traditional models~\citep{reyes2021learning}. Similarly, in the context of vortex dynamics and scalar mixing, they provide new approaches to inferring lift and drag forces on bluff bodies and enhancing the understanding of scalar mixing in turbulent flows~\citep{raissi2019vortex, raissi2019deep}.

Further, PINNs have been instrumental in solving inverse problems such as reconstructing Rayleigh-Bernard flows from temperature data and modeling rarefied-gas dynamics under the Bhatnagar-Gross-Krook approximation, showing potential in regimes where data is sparse or incomplete~\citep{di2023reconstructing, de2021physics}. Their application extends to modeling viscoelastic materials, where the ViscoelasticNet framework aids in understanding the complex stress and pressure fields in fluids~\citep{thakur2022viscoelasticnet}.

In structural applications, PINNs facilitate the design of offshore structures by solving the Serre-Green-Naghdi equations for water waves, predicting future states of water surfaces and velocities crucial for engineering applications~\citep{jagtap2022deep}. They also enhance large-eddy simulations of fluid flows by incorporating physical priors into the modeling process, improving predictions across varying Reynolds numbers~\citep{yang2019predictive}.

Moreover, in acoustics, PINNs solve frequency-domain equations for isotropic media, avoiding the high computational costs associated with traditional methods and eliminating numerical dispersion~\citep{song2021solving}. Their utility in predicting fluid flow behaviors in complex geometries is also demonstrated in the modeling of thermal creep flows and vortex-induced vibrations~\citep{lou2021physics, erichson2019physics}.

#### 4.2.2.Solid Mechanics and Material Science

PINNs address a diverse range of complex problems across different material behaviors and structural forms.
In the domain of nondestructive testing and evaluation, PINNs are effectively utilized to identify and characterize surface-breaking cracks in metal plates. By estimating the speed of sound, which varies due to the presence of cracks, PINNs facilitate precise crack detection from ultrasonic data, highlighting their potential in structural health monitoring~\citep{shukla2020physics}. Similarly, PINNs are deployed to quantify microstructural properties in polycrystalline nickel, solving inverse problems to infer material properties like compressibility and stiffness from ultrasonic data, demonstrating their capability in material characterization~\citep{shukla2021physics}.

For complex fluid behavior in materials, PINNs extend their application to model non-Newtonian fluids, accurately simulating fluids with time and deformation-dependent properties. This includes generalized Newtonian, viscoelastic, and thixotropic fluids, where PINNs recover velocity and stress fields effectively from sparse measurements~\citep{mahmoudabadbozchelou2022nn}. In the realm of solid mechanics, PINNs are applied to infer internal structures and defects in materials, accurately predicting the size, shape, and mechanical properties of voids or inclusions from stress-displacement data, demonstrating versatility across different material types~\citep{zhang2022analyses}.

The modeling of elastic plates using PINNs showcases a comparison between data-driven, PDE-based, and energy-based approaches, emphasizing the efficacy of PINNs in capturing finite deformations governed by complex equations like the Föppl–von Kármán equations~\citep{li2021physics}. Additionally, in the context of shell structures, PINNs solve for the small-strain response of curved shells, showing high accuracy when PDE loss is enforced in the weak form~\citep{bastek2022physics}.

PINNs also enhance the phase-field modeling of fracture, where they predict crack paths in materials by minimizing the variational energy of the system. Transfer learning is employed to improve efficiency, avoiding the need to retrain the network from scratch for each load step~\citep{goswami2020transfer}. Moreover, in the digital realm, PINNs predict the deformation of digital materials under load, applying energy-based formulations and innovative loss functions to prevent erroneous learning of deformation gradients~\citep{zhang2021physics}.

For complex nonlinear problems in computational mechanics, the Integrated Finite Element Neural Network (I-FENN)  combines PINNs with finite element methods to accelerate nonlinear solutions, showcasing the integration of machine learning with traditional numerical methods for enhanced computational performance~\citep{pantidis2022integrated}. Additionally, in elasticity imaging, PINNs are used for inverse identification of nonhomogeneous mechanical properties from displacement measurements under loading, demonstrating their utility in biomechanical applications~\citep{zhang2020physics}.

Lastly, Physics-informed multi-LSTM networks are introduced for the metamodeling of nonlinear structures. These networks incorporate physical laws into LSTM models to learn dynamics from limited data, significantly enhancing their learning efficiency and extrapolation capabilities~\citep{zhang2020physicsLSTM}.

Overall, these applications underscore the broad and impactful role of PINNs in advancing mechanics and material sciences, offering robust solutions to traditionally challenging problems across various material behaviors and structural analyses.
