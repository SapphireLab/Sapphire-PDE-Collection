# From PINNs to PIKANs: Recent Advances in Physics-Informed Machine Learning

<details>
<summary>基本信息</summary>

- 标题: "From PINNs to PIKANs: Recent Advances in Physics-Informed Machine Learning"
- 作者:
  - 01 Juan Diego Toscano,
  - 02 Vivek Oommen,
  - 03 Alan John Varghese,
  - 04 Zongren Zou,
  - 05 Nazanin Ahmadi Daryakenari,
  - 06 Chenxi Wu,
  - 07 George Em Karniadakis
- 链接:
  - [ArXiv](https://arxiv.org/abs/2410.13228)
  - [Publication]
  - [Github]
  - [Demo]
- 文件:
  - [ArXiv](2410.13228v2.pdf)
  - [Publication] #TODO

</details>

## Abstract: 摘要

**Physics-Informed Neural Networks (PINNs)** have emerged as a key tool in Scientific Machine Learning since their introduction in 2017, enabling the efficient solution of ordinary and partial differential equations using sparse measurements.
Over the past few years, significant advancements have been made in the training and optimization of **PINNs**, covering aspects such as network architectures, adaptive refinement, domain decomposition, and the use of adaptive weights and activation functions.
A notable recent development is the **Physics-Informed Kolmogorov–Arnold Networks (PIKANs)**, which leverage a representation model originally proposed by Kolmogorov in 1957, offering a promising alternative to traditional **PINNs**.
In this review, we provide a comprehensive overview of the latest advancements in **PINNs**, focusing on improvements in network design, feature expansion, optimization techniques, uncertainty quantification, and theoretical insights.
We also survey key applications across a range of fields, including biomedicine, fluid and solid mechanics, geophysics, dynamical systems, heat transfer, chemical engineering, and beyond.
Finally, we review computational frameworks and software tools developed by both academia and industry to support PINN research and applications.

## 1.Introduction: 引言

The finite element method (FEM) has been the cornerstone of Computational Science and Engineering (CSE) in the last few decades but it was viewed with skepticism when the first published works appeared in the early 1960s.
Despite their success in academic research and industrial applications, FEM cannot easily assimilate measured data unless elaborate data assimilation methods are employed that render large-scale computations prohibitively expensive.
FEM and other conventional numerical methods are effective in solving well-posed problem with full knowledge of the boundary and initial conditions as well as all material parameters.
Unfortunately, in practical applications, there are always gaps in such a setting and arbitrary assumptions have to be made, e.g.
to assume the thermal boundary conditions at the walls in power electronics cooling applications.
This may lead to erroneous results as in such a problem of interest is the highest temperature or the highest heat flux that is typically located at the surface where erroneous assumptions are employed.
What may be available instead are a few
sparse thermocouple measurements either on the surface or inside the domain of interest.
Unfortunately, current numerical methods like FEM cannot utilize such measurements effectively and hence important experimental information for the system is lost.
On the other hand, neural networks are trained based on data of any fidelity or any modality so data assimilation is a natural process in such settings.

Physics-Informed Neural Networks (PINNs) were developed to address precisely this need, considering different simulation scenarios where there is some knowledge of the governing physical laws but not complete knowledge, and there exist some sparse measurements for some of the state variables but not for all.
Hence, PINNs provide a framework to encode physical laws in neural networks~\citep{raissi2019physics} and resolve the disconnect between traditional physically grounded mathematical models and modern purely data-driven methods.
Specifically, PINNs incorporate the governing laws by having an additional `residual' loss term in the objective function that enforces the underlying PDE as a soft constraint.
They are effective in solving both forward and inverse problems across all scientific domains.
PINNs can incorporate sparse and noisy data, making them effective in scenarios where acquiring accurate measurements can be difficult or expensive.
A key innovation in PINNs is the use of automatic differentiation based on computational graphs that leads to accurate treatment of the differential operators employed in conservation laws but most importantly removes the tyranny of elaborate mesh generation that is time consuming and limits solution accuracy.

Since the original two papers appeared on the arXiv in 2017~\citep{raissi2017physicsI, raissi2017physicsII} and the subsequent publication of a combined paper in 2019~\citep{raissi2019physics}, there has been  great excitement in the CSE community and very important advances on many  aspects of the method have been proposed by research groups from around the world and across all scientific domains.
At the time of this writing, there have already been  over 11000 citations of~\citep{raissi2019physics} , with many studies investigating the applicability of PINNs  across different scientific domains while other studies proposing algorithmic improvements aimed at addressing the limitations of the original formulation.
In the current review paper, we provide a compilation of most of the major algorithmic developments and present a non-exhaustive list of applications of PINNs across different disciplines.
A comprehensive timeline of some of the important papers about PINNs is presented in the Appendix from PINNs~\citep{raissi2017physicsI} to PIKANs~\citep{liu2024kan}.

While existing reviews, such as those by~\citep{cuomo2022scientific, farea2024understanding, ganga2024exploringphysicsinformedneuralnetworks, raissi2024physics} summarize key aspects of PINNs, our paper differentiates itself by providing a more extensive overview of the latest algorithmic developments and by covering a broader range of applications of PINNs across scientific disciplines.
Reviews by~\citep{cuomo2022scientific} and~\citep{farea2024understanding} focus primarily on the methodology and applications of PINNs in various domains, with less emphasis on recent algorithmic improvements.
The review by~\citep{raissi2024physics} provides a concise overview of PINNs and their extensions, with an example on data-driven discovery of equations, but does not dive deep into applications of PINNs.
The review in~\citep{ganga2024exploringphysicsinformedneuralnetworks} includes a discussion of algorithmic developments, but limits the scope of their discussion on applications to thermal management and computational fluid dynamics.
Additionally, several reviews focus on specific domains of application.
For example,~\citep{chi2024comprehensive} and~\citep{cai2021physics} review the use of PINNs in fluid dynamics, while~\citep{huang2022applications} focuses on applications within power systems.
In contrast,~\citep{lawal2022physics} conducted a bibliometric analysis of 120 research articles, highlighting key publication trends, highly cited authors and leading countries in PINN research.

The structure of the paper is shown schematically in Fig.\ref{fig:schematic}.

In Section \ref{framework} we outline the general framework of Physics-Informed Machine Learning.
Section \ref{algorithmic_developments} provides a comprehensive summary of the major techniques aimed at improving PINNs.
In Section \ref{applications} we provide an overview of the diverse applications of PINNs.
Section \ref{uncertainty_quantification} focuses on uncertainty quantification methods in PINNs.
In Section \ref{theoretical_advances}, we summarize the developments in the theory behind PINNs.
Section \ref{computational_frameworks} reviews the various computational frameworks and software.
Finally, in Section \ref{discussion}, we provide a discussion and future outlook.

## 2.Physics-Informed Machine Learning (PIML)

Physics-Informed Machine Learning (PIML) has emerged as a powerful alternative to traditional numerical methods for solving partial differential equations (PDEs) in both forward and inverse problems.
PIML was first introduced in a series of papers by Raissi, Perdikaris, and Karniadakis~\citep{raissi2017machine} based on Gaussian processes regression (GPR); see also the patent by the same authors~\citep{raissi2021physics}.
In this paper, however, we will review the subsequent development of PIML using neural networks and automatic differentiation, starting with the two papers from 2017 on the arXiv~\citep{raissi2017physicsI, raissi2017physicsII}, which
were combined into a single paper later in~\citep{raissi2019physics}.
It is worth noting that earlier papers by~\citep{dissanayake1994neural,lagaris1998artificial} attempted to solve PDEs (forward problems) but without any data fusion or automatic differentiation.
The PIML we present in this paper employs a representation model, namely a multilayer perceptron (MLP) or a Kolmogorov-Arnold Network (KAN)~\citep{liu2024kan}, to approximate the solution of ordinary or partial differential equations (ODEs/PDEs) and match any given data and constraints by minimizing a loss function comprised of multiple terms.
In particular, this loss function is designed to fit observable data or other physical or mathematical constraints while enforcing the underlying physics, e.g., conservation laws~\citep{raissi2019physics,shukla2024comprehensive}.

Unlike traditional numerical methods, most PIML models do not rely on predefined grids or meshes, allowing them to handle complex geometries and high-dimensional problems efficiently.
By leveraging automatic differentiation, PIML models compute derivatives accurately without discretization, seamlessly integrating governing physical laws with data.
This flexibility allows PIML models to approximate solutions from partial information, making them optimal for uncovering hidden parameters~\citep{raissi2019physics}, as well as reconstructing~\citep{cai2021flow} or inferring hidden fields~\citep{raissi2020hidden} from real-world data.
Moreover, PIML models are well-suited for handling high-dimensional PDEs~\citep{hu2024tackling}, coupled systems~\citep{jin2021nsfnets, shukla2024neurosem}, stochastic differential equations~\citep{yang2019adversarial}, and fractional PDEs~\citep{pang2019fpinns}, all while maintaining scalability through parallelization on modern hardware such as GPUs~\citep{karniadakis2021physics}.
This enables PIML models to efficiently tackle multi-physics problems and large-scale simulations with reduced computational overhead compared to traditional methods.

PIML is agnostic to specific governing laws, so here we consider the following nonlinear ODE/PDE:

$$
\begin{aligned}
    \mathcal{F}_\tau[\hat{u}](x)  &= f(x), \quad x \in \Omega, \\
    \mathcal{B}_\tau[\hat{u}](x)  &= b(x), \quad x \in \Omega_B,
\end{aligned}
$$

where \( x \) represents the spatial-temporal coordinate, \( \hat{u} \) is the solution to the ODE/PDE, \( \tau \) are the parameters of the equation, \( f \) is the source term, \( b \) is the boundary term, and \( \mathcal{F} \) and \( \mathcal{B} \) are general nonlinear differential and boundary operators, respectively.
The PIML approach aims to approximate the solution to the problem defined by Eq.
\eqref{eq:problem} using a representation model, denoted as:

$$
\begin{aligned}
  \hat{u}(x) \approx u(\theta, x), x\in\Omega\cup\Omega_B,
\end{aligned}
$$

where \( u \) is the representation model, and \( \theta \) are its learnable parameters.
Since \( u \) is continuous and differentiable, it allows for the computation of the source and boundary terms \( f \) and \( b \) through automatic differentiation~\citep{baydin2018automatic}, expressed as \( \mathcal{F}_\tau[u] \) and \( \mathcal{B}_\tau[u] \)~\citep{raissi2019physics}.

The goal of PIML training is to find the optimal learnable parameters that minimize the cumulative error between the approximated solution and the known components of the true solution, such as the governing equation, boundary conditions, or data residuals.
This framework can also be easily extended to ODE/PDE systems by stacking constraints for each approximated solution~\citep{raissi2020hidden}.

In general, when the equation parameters \( \tau \) are known and the boundary conditions are prescribed, the problem is referred to as a \textit{forward problem}, where no observational data within the domain are required~\citep{raissi2019physics, meng2020ppinn}.
Conversely, when partial information, such as \( \tau \), boundary conditions, or hidden fields in the ODE/PDE system, is unknown, then the problem is referred to as an \textit{inverse problem}, where the objective is to infer simultaneously the unknown information and the solution from available data or observations~\citep{raissi2020hidden}.
A schematic of the overall PIML framework is shown in Fig.\ref{fig:PIML_compz}.

## 3.Algorithmic Developments of PIML

From the PIML framework outlined in Section~\ref{framework}, we can identify three key components: (1) a representation model to approximate the solution, (2) a governing equation (such as an ODE or PDE), and (3) an optimization process that minimizes a multi-objective loss function to find the optimal learnable parameters,
see Fig.\ref{fig:PIML_compz}. Ongoing research has greatly enhanced PIML's baseline performance through various methods targeting these three areas, namely, modifications to the representation model, advancements in the treatment of the governing equation, and optimization process improvements.

### 3.1.Representation Model Modifications

When the representation model is defined using an MLP, the PIML formulation is referred to as physics-informed neural networks (PINNs) ~\citep{raissi2019physics}. PINNs utilize MLPs to approximate the solutions of an ODE/PDE system ($\bm{\hat{u}} = \{\hat{u}_1, \ldots, \hat{u}_p\}$)  by leveraging the network's ability to model complex nonlinear functions. The approximated solution ($\bm{{u}} = \{{u}_1, \ldots, {u}_p\}$)  using MLPs can be mathematically expressed as:

$$
\begin{aligned}
  \bm{u}(\theta, \bm{x})  &= \text{MLP}(\theta, \bm{x})  \\
  &= \sigma\left(W^{(L) } \sigma\left(W^{(L-1) } \ldots \sigma\left(W^{(1) } \bm{x} + b^{(1) }\right)  \ldots + b^{(L-1) }\right)  + b^{(L) }\right)
\end{aligned}
$$

where ($\bm{x}= \{x_1, \ldots, x_n\}$)  are inputs and $\theta = \{W^{(l) }, b^{(l) }\}$ are the trainable parameters of the network; $W^{(l) }$ and $b^{(l) }$ denote the weights and biases of the $l$-th layer, respectively. The network consists of $L$ layers, and $\sigma$ denotes a suitable activation function. The output of each layer serves as the input for the subsequent layer, culminating in the final output \( u \).

The ability of MLPs to approximate virtually any continuous function on compact subsets of \( \mathbb{R}^n \)  is supported by the Universal Approximation Theorem~\citep{hornik1989multilayer}. This theorem underpins the effectiveness of neural networks in modeling complex nonlinear relationships.

Building on the foundational work in~\citep{raissi2019physics}, many studies have explored enhancing the expressiveness of representation models in PINNs through various strategies. These include input and output normalization, feature expansions, hard constraint encoding, model decompositions, and architectural modifications. Each of these strategies aims to improve the network's ability to capture the underlying physics of the problem more accurately and efficiently.

#### 3.1.1.Input/Output Transformations

One of the most straightforward ways to improve the stability and accuracy of a representation model $\mathcal{M}$ is by transforming the model inputs $\bm{x} \in \mathbb{R}^n$ or outputs $\bm{u} \in \mathbb{R}^p$ using suitable mappings, $I(\cdot) $and $\Gamma(\cdot) $ (see Fig.~\ref{RM_enh}). Under this reformulation, the solutions of an ODE/PDE ($\bm{\hat{u}}$)  can be approximated as:

$$
\begin{aligned}
    \hat{\bm{u}}(\bm{x})  \approx \bm{u}(\theta, \bm{x})  = \Gamma(\mathcal{M}(\theta, I(\bm{x}) ) ),
\end{aligned}
$$

where $\bm{u} = \{u_1, \ldots, u_p\}) $ are the approximated solutions from the representation model $\mathcal{M}$, and  $I: \mathbb{R}^n \rightarrow \mathbb{R}^m$ is a mapping that transforms the multivariate inputs and enhances the model’s expressivity. The choice of $I$ often depends on the activation function $\sigma$, as different activation functions are more effective over specific input domains. For example, Cai et al.~\citep{cai2021artificial} proposed normalizing inputs to the range $[-1, 1]$ when using $\sin(\cdot)$ or $\tanh(\cdot)$ activation functions. Similarly, Raissi et al.~\citep{raissi2020hidden} recommended normalizing inputs based on their mean and standard deviation when using the $swish(\cdot)$ activation function.

On the other hand, the function $\Gamma: \mathbb{R}^p \rightarrow \mathbb{R}^q$ is selected based on the specific characteristics of the problem. For instance, Anagnostopoulos et al.~\citep{anagnostopoulos2024residual} used scalars $a_1, \ldots, a_p \in \mathbb{R}$ to create different linear maps to handle various scales in the model outputs, represented as $\bm{u} = \{a_1u_1, \ldots, a_pu_p\}$, which aids in improving convergence. Similarly, $\Gamma$ can be used to constrain the range of predicted outputs. For example, absolute value $\Gamma(u) =|u|$ or exponential $\Gamma(u) =e^{u}$ functions can enforce the non-negativity required for the density field in high-speed flows~\citep {mao2020physics}. Likewise, Zapf et al.~\citep{zapf2022investigating} used the sigmoid function to constrain the range of the predicted diffusivity.

##### Feature Expansions

These modifications aim to address some fundamental weaknesses in the conventional PIML formulation, particularly in learning high-frequency functions, known as spectral bias~\citep{rahaman2019spectral,cao2019towards}, and capturing other complex relations~\citep{wang2021eigenvector}. To mitigate these issues, researchers have modified the baseline model formulation by transforming its input $\bm{x} \in \mathbb{R}^n$ into an expanded input $I(\bm{x})  \in \mathbb{R}^m$, which is then fed into the representation model as shown in Fig.~\ref{RM_enh}. Several expansion maps have been explored, with their selection typically based on the specific problem. For example, Wang et al.~\citep{wang2021eigenvector} proposed using random Fourier features and demonstrated that this modification helps to mitigate spectral bias. Other types of expansions, such as polynomials, exponentials, Chebyshev polynomials, and even gated recurrent units (GRU), have also been explored or proposed in previous studies~\citep{cai2019multi,liu2020multi,wang2020multi,liu2022linearized,ahmadi2024ai,zhang4957859pkan}.

##### Hard Constraints

Training in PIML involves minimizing an objective function that often combines multiple constraints, significantly complicating the optimization process~\citep{toscano2024inferring}. Research has shown that improper enforcement of boundary conditions can degrade both the performance and stability of neural network training~\citep{dong2021method,wang2021understanding,chen2020comparison}. Thus, accurately imposing boundary conditions is crucial for improving model reliability and efficiency. In particular, Zeinhofer et al.~\citep{zeinhofer2023unified} theoretically demonstrated that hard constraints can lead to lower error estimates in linear problems. One practical way to address these challenges is by embedding boundary conditions directly into the model's structure, either through input/output transformations or specialized architectures, thereby simplifying the optimization and enhancing the overall performance.


###### Dirichlet Boundary Conditions.

Several methods have been developed to enforce Dirichlet boundary conditions exactly in PIML problems. Berrone et al.~\citep{berrone2023enforcing} introduced the Nitsche's method, which applies a variational approach to enforce these conditions. Another systematic technique is the Theory of Functional Connections, which imposes constraints through functional connections, as detailed by Leake et al.~\citep{leake2020deep}. On the other hand, hPINNs utilize penalty methods and the augmented Lagrangian approach to impose hard constraints, providing a flexible framework for handling various boundary conditions~\citep{lu2021physics}. Notably, Sukumar et al.~\citep{sukumar2022exact} introduced Approximate Distance Functions (ADF), which impose boundary conditions through output transformations. Under the ADF framework, the constrained expression for Dirichlet boundary conditions is represented as:

$$
\begin{aligned}
    \hat{\bm{u}}(\bm{x})  \approx \bm{u}(\theta, \bm{x})  = \Gamma(\mathcal{M}(\theta, I(\bm{x}) ) ) =\bm{g}(\bm{x})  + \bm{\phi}(\bm{x}) \mathcal{M}(\theta, I(\bm{x}) ),
\end{aligned}
$$

where $\Gamma(\cdot) $ transform the network output in terms of a function that satisfies the solution $\hat{\bm{u}}$ along the boundaries $\bm{g}(\cdot) $  and a composite distance function $\bm{\phi}(\cdot) $ that equals zero when evaluated on the boundaries. If the boundary is composed of $M$ partitions, denoted as $[S_1, \ldots, S_M]$, the composite distance function for Dirichlet boundary conditions can be expressed as $\bm{\phi}(\phi_1, \phi_2, \ldots, \phi_M)  = \prod_{i=1}^M \phi_i$, where $[\phi_1, \ldots, \phi_M]$ are the individual distance functions. Notice that if $\bm{x} \in S_i$, then $\phi(\bm{x})  = 0$, ensuring that the neural network approximation exactly satisfies the boundary conditions, i.e., $\bm{u}(\bm{x})  = \bm{g}(\mathbf{x}) $.

###### Periodic Boundary Conditions.

On the other hand, this type of boundary conditions can be strictly enforced as hard constraints by selecting a suitable input transformation \(I(\bm{x}) \)  (see Fig.~\ref{RM_enh}). For instance, the periodic nature of a smooth univariate function \(u(x) \)  can be encoded into a model using a one-dimensional Fourier feature embedding, $I(x)  = [1, \cos(\omega_x x), \sin(\omega_x x), \ldots, \cos(m\omega_x x), \sin(m\omega_x x) ]$. Dong et al.~\citep{dong2021method} demonstrated that any representation model, such as \(u(\theta, I(x) ) \), is periodic along the \(x\)  coordinate when using this Fourier feature embedding. Similar expansion maps have been explored for higher dimensions by Wang et al.~\citep{wang2022respecting}.

Other methods to impose hard constraints for periodic boundary conditions include using hPINNs, which employ the penalty and augmented Lagrangian methods to enforce such constraints~\cite {lu2021physics}. Additionally, hybrid approaches that combine various techniques for the exact imposition of periodic boundary conditions have been investigated~\cite {barschkis2023exact}.

###### Complex Constraints.

Complex constraints can be addressed through specialized architectures that incorporate domain-specific knowledge into the learning process. One such approach is the use of Divergence-Free Networks, which apply an appropriate output transformation to ensure that the learned vector fields satisfy divergence-free conditions (see Fig.\ref{RM_enh}), as required in certain fluid dynamics applications\citep{toscano2024inferring,wang2021understanding,anagnostopoulos2024learning}. Zeinhofer et al.\citep{zeinhofer2023unified} theoretically demonstrated that enforcing divergence-free constraints leads to improved error estimates in linear problems. Another example is SympNets, a specialized architecture for identifying Hamiltonian systems from data, which preserves the symplectic structure through different Jacobian matrix factorization techniques\citep{jin2020sympnets}. Finally \citep{zhou2023flow} proposed a method that uses the theory of functional connections to exactly enforce the data constraints in inverse problems.

#### 3.1.2.Architectures

##### Perceptron Modifications

The original PIML formulation uses an MLP as the representation model~\citep{raissi2019physics}. The building blocks of an MLP are perceptrons, which define a layer $l$ and can be defined as follows:

$$
\begin{aligned}
    \bm{z}^{(l) }&=\sigma^{(l) }(W^{(l) }\bm{z}^{(l-1) } + b^{(l) }),
\end{aligned}
$$

where $\bm{z}^{(l) }=\{z_{0}^{(l) },\cdots,z_{H}^{(l) }\}$ are the outputs of layer $l$, $\sigma^{(l) }$ is a non-linear activation function (e.g., $\text{sigmoid}(\cdot) $, $\tanh(\cdot) $, $\sin(\cdot) $, etc.) , and $W^{(l) }$ and $b^{(l) }$ are trainable parameters that linearly transform the inputs $\bm{z}^{(l-1) }=\{z_{0}^{(l-1) },\ldots,z_{H}^{(l-1) }\}$. Several approaches have been proposed to improve the perceptron’s capabilities. For instance, Jagtap et al.~\citep{jagtap2020adaptive,jagtap2020locally,jagtap2022deep} proposed modifying the activation function in as
$\sigma^{(l) }(\cdot)  = \sum_i a_{i}^{(l) } \sigma(f_{i}^{(l) } \cdot) $, where $a_{i}^{(l) }$ and $f_{i}^{(l) }$ are trainable parameters. The authors showed both theoretically and empirically that these modifications significantly improve model performance.

Other approaches proposed modifying the weight matrix $W^{(l) }$; for instance,~\citep{salimans2016weight,raissi2020hidden} proposed decomposing $W^{(l) }$ into its magnitude and its direction via weight normalization described as $W^{(l) }=\frac{g^{(l) }}{\lVert\mathbf{v}^{(l) }\rVert_{2}}\mathbf{v}^{(l) }$. This re-parameterization speeds up the model convergence with minimal computational overhead~\citep{salimans2016weight}. Similarly, ~\citep{wang2022random}  proposed  weight factorization $W^{(l) }=diag (\bm{s} ^{(l) })\cdot V^{(l) }$, where $\bm{s}^{(l) }$ are trainable parameters. The authors experimentally and theoretically showed that this reparameterization significantly improves the model performance~\citep{wang2022random}.

##### Other Representation Models

One natural extension to other representation models, given their similarity to MLPs, is Kolmogorov-Arnold Networks (KANs)~\citep{liu2024kan}, which were introduced as PIKANs in~\citep{shukla2024comprehensive}. Each PIKAN layer can be described as follows:

$$
    \bm{z}^{(l)} = \sum_{i=1}^{H}\Phi_i\left(\sum_{j=1}^{H}\phi_{i,j}(z^{(l-1)}_{j})\right),
$$

where $\bm{z}^{(l-1) }=\{z_{0}^{(l-1) },\cdots,z_{H}^{(l-1) }\}$ is the multivariate input, $H$ denotes the number of neurons and $\Phi_{i,j}$ are the outer and $\phi_{i,j}$ are the inner univariate functions. The specific form of $\phi(\cdot)$ and $\Phi(\cdot)$ defines the types of KAN architectures. Among these variations,~\citep{shukla2024comprehensive} introduced cPIKANs, which use Chebyshev polynomials as inner and outer univariate functions defined as $ \phi(\zeta,\theta) =w_n\sum_n^d c_nT_n(\tanh(\zeta)) $, where $\zeta$ are the inputs, $\theta=(w_n,c_n) $ are trainable parameters, $d$ is the degree and $T_n$ is the $n$-th order Chebyshev polynomial, defined recursively as $T_n(\zeta) =2\zeta T_{n-1}(\zeta)  + T_{n-2}(\zeta) $~\citep{karniadakis2005spectral}. The authors found that this stable representation is more robust to noise~\citep{shukla2024comprehensive} and can lead to improved performance with fewer parameters~\citep{toscano2024inferring,shukla2024comprehensive} than MLPs. Subsequent studies extended the KAN framework and developed new architectures for PIML problems~\citep{howard2024finite,rigas2024adaptive,shuai2024physics,zhang4957859pkan, wang2024kolmogorovarnoldinformedneural,guilhoto2024deeplearningalternativeskolmogorov,koenig2024kan,patra2024physics,toscano2024inferring,wang2024expressiveness,liu2024kan2}.

Other representation models have also been explored, including convolutional neural networks (CNNs)~\citep{gao2021super}, Hermite spline CNNs~\citep{wandel2022spline}, generative adversarial neural networks (GANs)~\citep{yang2020physics,bullwinkel2022deqgan}, spiking neural networks~\citep{zhang2023artificial}, transformers~\citep{zhao2023pinnsformer}, long short-term memory (LSTM)  networks~\citep{cho2022lstm,nathasarma2023physics}, information-botleneck inspired architectures~\citep{guo2024ib} and reinforcement learning models~\citep{banerjee2023survey,ramesh2023physics,radaideh2021physics}.

##### Residual Connections

Another approach to improve the PIML architecture performance is by adding residual connections (see Fig.~\ref{RM_enh}), which enable accurate calculation of high-order derivatives. The general formulation for a single additive skip connection is defined as follows:

$$
\begin{aligned}
    \bm{z}^{(l) }(\bm{z}^{(l-1) },\theta) &=\mathcal{M}^{(l) }(\bm{z}^{(l-1) },\theta) +\bm{z}^{(l-1) },
\end{aligned}
$$

where $\bm{z}^{(l) }=\{z_{0}^{(l) },\ldots,z_{H}^{(l) }\}$ are the outputs of layer $l$, $\mathcal{M}^{(l) }$ is a layer of the representation model (e.g., MLP layer, KAN layer), and $\bm{z}^{(l-1) }=\{z_{0}^{(l-1) },\ldots,z_{H}^{(l-1) }\}$ are the inputs to layer $l$. Several studies have explored the advantages of incorporating skip connections via addition or multiplication. Wang et al.~\citep{wang2021understanding} pioneered this approach by introducing a method referred to as modified MLP, where two single-layer MLPs project the model inputs ($\bm{x}$)  into a high-dimensional feature space, which is then used to update the remaining hidden layers via element-wise multiplication and addition. Other approaches introduce multiplicative connections between every layer~\citep{jiang2024densely}. Finally, the modified MLP was further improved by incorporating adaptive residual connections that incorporate a new learnable parameter that controls the contribution of the deeper layers with respect to the input~\citep {wang2024piratenets}. This improved architecture enables the use of deeper networks without compromising the accuracy of PIML problems.

##### Model Decomposition

Further improvements can be achieved by splitting the network into several components. Cho et al.~\citep{cho2024separable} proposed separable PINNs (sPINNs), which utilize separate sub-networks to approximate the desired solution.Under this formulation, the solution of a PDE is approximated as $\hat{u}(x_1,\cdots,x_d)  = \sum_{j=1}^p \prod_{i=1}^d \mathcal{M}_{j}(x_i,\theta_i)$
, where $\{\mathcal{M}_j(x_i,\theta_i) \}_{j=1}^p$ are the outputs univariate representation models that encodes each dimension $x_i$ into a $p$-dimensional space. This decomposition addressed the
curse-of-dimensionality and was introduced to the deep learning community as tensor neural networks in~\citep{wang2022tensor_integral}. While~\citep{wang2022tensor_integral} focused on improving accuracy through enhanced numerical integration methods,~\citep{cho2024separable} prioritized computational efficiency, achieving up to 60-times speedups for high-dimensional problems, such as the 3D Helmholtz Equation and the 4D Navier-Stokes Equation. This formulation has been further explored and improved in subsequent studies~\citep{wang2024tensor,vemuri2024functional,hu2024tackling}.

Other types of decompositions have also been explored for inverse problems~\citep{toscano2024invivo,zhou2023flow}. For instance,~\citep{toscano2024invivo} extended the negative log-likelihood (NLL)  framework~\citep{lakshminarayanan2017simple} for linear PDEs in PIML, enabling the quantification of aleatoric uncertainty and improving model performance. The authors assumed that the observed data was corrupted by noise, so they decomposed the desired solution into mean fields and fluctuations as $u=\bar{u}+u'$. Then they used independent models to learn the data-driven mean fields \(\bar{u}\) and their corresponding fluctuations, standard deviations \(u'\)  by using the NLL criterion~\citep{quinonero2006machine,lakshminarayanan2017simple,cawley2005estimating}. On the other hand,~\citep {zhou2023flow} proposed a method to simultaneously reconstruct flow states and determine particle properties from Lagrangian particle tracking (LPT) using a neural network as a flow model and a data-constrained polynomial as a particle model.
