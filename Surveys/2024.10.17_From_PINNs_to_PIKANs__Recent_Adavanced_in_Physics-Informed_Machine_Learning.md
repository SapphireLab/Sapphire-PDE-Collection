# From PINNs to PIKANs: Recent Advances in Physics-Informed Machine Learning

<details>
<summary>基本信息</summary>

- 标题: "From PINNs to PIKANs: Recent Advances in Physics-Informed Machine Learning"
- 作者:
  - 01 Juan Diego Toscano,
  - 02 Vivek Oommen,
  - 03 Alan John Varghese,
  - 04 Zongren Zou,
  - 05 Nazanin Ahmadi Daryakenari,
  - 06 Chenxi Wu,
  - 07 George Em Karniadakis
- 链接:
  - [ArXiv](https://arxiv.org/abs/2410.13228)
  - [Publication]
  - [Github]
  - [Demo]
- 文件:
  - [ArXiv](2410.13228v2.pdf)
  - [Publication] #TODO

</details>

## Abstract: 摘要

**Physics-Informed Neural Networks (PINNs)** have emerged as a key tool in Scientific Machine Learning since their introduction in 2017, enabling the efficient solution of ordinary and partial differential equations using sparse measurements.
Over the past few years, significant advancements have been made in the training and optimization of **PINNs**, covering aspects such as network architectures, adaptive refinement, domain decomposition, and the use of adaptive weights and activation functions.
A notable recent development is the **Physics-Informed Kolmogorov–Arnold Networks (PIKANs)**, which leverage a representation model originally proposed by Kolmogorov in 1957, offering a promising alternative to traditional **PINNs**.
In this review, we provide a comprehensive overview of the latest advancements in **PINNs**, focusing on improvements in network design, feature expansion, optimization techniques, uncertainty quantification, and theoretical insights.
We also survey key applications across a range of fields, including biomedicine, fluid and solid mechanics, geophysics, dynamical systems, heat transfer, chemical engineering, and beyond.
Finally, we review computational frameworks and software tools developed by both academia and industry to support PINN research and applications.

## 1.Introduction: 引言

The finite element method (FEM) has been the cornerstone of Computational Science and Engineering (CSE) in the last few decades but it was viewed with skepticism when the first published works appeared in the early 1960s.
Despite their success in academic research and industrial applications, FEM cannot easily assimilate measured data unless elaborate data assimilation methods are employed that render large-scale computations prohibitively expensive.
FEM and other conventional numerical methods are effective in solving well-posed problem with full knowledge of the boundary and initial conditions as well as all material parameters.
Unfortunately, in practical applications, there are always gaps in such a setting and arbitrary assumptions have to be made, e.g.
to assume the thermal boundary conditions at the walls in power electronics cooling applications.
This may lead to erroneous results as in such a problem of interest is the highest temperature or the highest heat flux that is typically located at the surface where erroneous assumptions are employed.
What may be available instead are a few
sparse thermocouple measurements either on the surface or inside the domain of interest.
Unfortunately, current numerical methods like FEM cannot utilize such measurements effectively and hence important experimental information for the system is lost.
On the other hand, neural networks are trained based on data of any fidelity or any modality so data assimilation is a natural process in such settings.

Physics-Informed Neural Networks (PINNs) were developed to address precisely this need, considering different simulation scenarios where there is some knowledge of the governing physical laws but not complete knowledge, and there exist some sparse measurements for some of the state variables but not for all.
Hence, PINNs provide a framework to encode physical laws in neural networks~\citep{raissi2019physics} and resolve the disconnect between traditional physically grounded mathematical models and modern purely data-driven methods.
Specifically, PINNs incorporate the governing laws by having an additional `residual' loss term in the objective function that enforces the underlying PDE as a soft constraint.
They are effective in solving both forward and inverse problems across all scientific domains.
PINNs can incorporate sparse and noisy data, making them effective in scenarios where acquiring accurate measurements can be difficult or expensive.
A key innovation in PINNs is the use of automatic differentiation based on computational graphs that leads to accurate treatment of the differential operators employed in conservation laws but most importantly removes the tyranny of elaborate mesh generation that is time consuming and limits solution accuracy.

Since the original two papers appeared on the arXiv in 2017~\citep{raissi2017physicsI, raissi2017physicsII} and the subsequent publication of a combined paper in 2019~\citep{raissi2019physics}, there has been  great excitement in the CSE community and very important advances on many  aspects of the method have been proposed by research groups from around the world and across all scientific domains.
At the time of this writing, there have already been  over 11000 citations of~\citep{raissi2019physics} , with many studies investigating the applicability of PINNs  across different scientific domains while other studies proposing algorithmic improvements aimed at addressing the limitations of the original formulation.
In the current review paper, we provide a compilation of most of the major algorithmic developments and present a non-exhaustive list of applications of PINNs across different disciplines.
A comprehensive timeline of some of the important papers about PINNs is presented in the Appendix from PINNs~\citep{raissi2017physicsI} to PIKANs~\citep{liu2024kan}.

While existing reviews, such as those by~\citep{cuomo2022scientific, farea2024understanding, ganga2024exploringphysicsinformedneuralnetworks, raissi2024physics} summarize key aspects of PINNs, our paper differentiates itself by providing a more extensive overview of the latest algorithmic developments and by covering a broader range of applications of PINNs across scientific disciplines.
Reviews by~\citep{cuomo2022scientific} and~\citep{farea2024understanding} focus primarily on the methodology and applications of PINNs in various domains, with less emphasis on recent algorithmic improvements.
The review by~\citep{raissi2024physics} provides a concise overview of PINNs and their extensions, with an example on data-driven discovery of equations, but does not dive deep into applications of PINNs.
The review in~\citep{ganga2024exploringphysicsinformedneuralnetworks} includes a discussion of algorithmic developments, but limits the scope of their discussion on applications to thermal management and computational fluid dynamics.
Additionally, several reviews focus on specific domains of application.
For example,~\citep{chi2024comprehensive} and~\citep{cai2021physics} review the use of PINNs in fluid dynamics, while~\citep{huang2022applications} focuses on applications within power systems.
In contrast,~\citep{lawal2022physics} conducted a bibliometric analysis of 120 research articles, highlighting key publication trends, highly cited authors and leading countries in PINN research.

The structure of the paper is shown schematically in Fig.\ref{fig:schematic}.

In Section \ref{framework} we outline the general framework of Physics-Informed Machine Learning.
Section \ref{algorithmic_developments} provides a comprehensive summary of the major techniques aimed at improving PINNs.
In Section \ref{applications} we provide an overview of the diverse applications of PINNs.
Section \ref{uncertainty_quantification} focuses on uncertainty quantification methods in PINNs.
In Section \ref{theoretical_advances}, we summarize the developments in the theory behind PINNs.
Section \ref{computational_frameworks} reviews the various computational frameworks and software.
Finally, in Section \ref{discussion}, we provide a discussion and future outlook.
