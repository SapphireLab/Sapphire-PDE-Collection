# Singularity Enriched Neural Networks/SENNs

<details>
<summary>基本信息</summary>

- 标题: "Point Source Identification Using Singularity Enriched Neural Networks"
- 作者:
  - 01 Tianhao Hu
  - 02 Bangti Jin
  - 03 Zhi Zhou
- 链接:
  - [ArXiv](https://arxiv.org/abs/2408.09143)
  - [Publication]()
  - [Github]()
  - [Demo]()
- 文件:
  - [ArXiv]()
  - [Publication] #TODO

</details>

## 摘要

The inverse problem of recovering point sources represents an important class of applied inverse problems.
However, there is still a lack of neural network-based methods for point source identification, mainly due to the inherent solution singularity.
In this work, we develop a novel algorithm to identify point sources, utilizing a neural network combined with a singularity enrichment technique.
We employ the fundamental solution and neural networks to represent the singular and regular parts, respectively, and then minimize an empirical loss involving the intensities and locations of the unknown point sources, as well as the parameters of the neural network.
Moreover, by combining the conditional stability argument of the inverse problem with the generalization error of the empirical loss, we conduct a rigorous error analysis of the algorithm.
We demonstrate the effectiveness of the method with several challenging experiments.

恢复点源的反问题是一类重要的应用反问题。
然而，目前仍然缺乏基于神经网络的方法来识别点源，主要是由于固有解奇性。
在本文中，我们开发了一种新算法用于识别点源，使用神经网络，并结合奇性富集技术。
我们采用了基本解和使用神经网络来分别表示奇性和规则部分，然后最小化一个经验损失函数，该函数涉及未知点源的强度和位置以及神经网络的参数。
此外，通过将反问题的条件稳定性分析和经验损失的泛化误差相结合，我们构造了严格的误差分析。
我们通过数个挑战性实验来展示该方法的有效性。

## 引言

标准泊松方程:
$$
-\Delta u = F,\quad  \text{in}\quad \Omega.
$$

- $\Omega\subset (-1,1)^d$: 有界开集。
- $\partial \Omega$: 光滑边界。
- $\vec{\nu}$: 边界的单位外法向量。
- $\partial_{\vec{\nu}}u$: 外法向导数。

本工作关心的反问题是**逆源问题 (Inverse Source Problem, ISP)**，即从**柯西数据** $(f,g)=(u|_{\partial \Omega}, \partial_{\vec{\nu}}u)\in H^{\frac{3}{2}}(\partial \Omega)\times H^{\frac{1}{2}}(\partial \Omega)$ 中确定源 $F$。
特别地，我们只能获得噪声数据 $(f^{\delta},g^{\delta})\in (L^2(\partial\Omega))^2$，噪声级别 $\delta$，即
$$
\|f-f^{\delta}\|_{L^2(\partial\Omega)} \leq \delta.\quad \text{and}\quad \|g-g^{\delta}\|_{L^2(\partial\Omega)} \leq \delta.
$$

注：**柯西数据**通常用于描述在某个边界或初始时刻给定的函数值及其导数值，包括函数本身和法向导数在边界或初始时刻的值。

泊松问题的 ISP 已经被广泛研究了。
在只有一对柯西数据时，不可能唯一复原出一般的源 $F$: 给 $u$ 增加任意光滑函数 $v$ 和紧支集 $\overline{\text{supp}(v)}\subset \Omega$ 不会改变柯西数据。
因此，关于 $F$ 的合适的先验知识 (物理本质) 是必须的，以便恢复唯一辨识性。
一个柯西数据对可以唯一确定未知源 $F$，例如乘积源 $F$ 的一个因子 (即 $F$ 是分离且一个因子已知)，或**球面分层源的角向因子 (Angular Factor of A Spherically Layered Source)**，或 $F=\chi_{D}\rho(x)$ 或 $F=\nabla\cdot (\rho(x)\chi_{D}(x)a)$ ($\chi_{D}$ 是多边形 $D\subset \Omega$ 的特征函数，$a$ 是非零常数向量)。

本工作研究 $F$ 是点源的线性组合的情形:

$$
F=\sum_{j=1}^{M} c_j \delta_{\mathbf{x}_{j}},\quad\mathbf{x}_j\in\Omega
$$

- $\delta_{\mathbf{x}_j}$ 是中心在 $\mathbf{x}_j$ 的狄拉克Δ函数，定义为 $\int_{\Omega} \delta_{\mathbf{x}_j}v\text{d}x=v(\mathbf{x}_j)$ 对于所有 $v\in C(\bar{\Omega})$，$c_j\neq 0$ 是点源在 $\mathbf{x}_j$ 处的强度。

未知参数包括点源的数量 $M$，位置和强度 $\{(\mathbf{x}_j, c_j)\}_{j=1}^{M}$。

这一情形出现在多种实际应用中，例如脑电图，狄拉克函数可以描述大脑中的活动源。
特定的反问题满足唯一辨识性和 Holder 稳定性。

[^16] 开发了一种直接法来识别2维泊松问题中点源的数量和位置、强度。
从柯西数据中我们可以计算任意调和函数 $w$ 的值 $\sum_{j=1}^{M} c_j w(\mathbf{x}_j)$。通过选择合适的 $w$ 可以确定点源的数量和密度。它对二维和三维情形都运作良好，$w$ 从复杂解析解中构造得到。
这在高维情形中更为复杂，构造调和测试函数 $w$ 和计算**互易性间隙泛函 (Reciprocity Gap Functional)** 都是非平凡的。

[^16]: An Inverse Source Problem in Potential Analysis. 2000.

注：**调和函数**：满足拉普拉斯方程的函数，具有平均值性质、最大模原理等性质。
注：**平均值性质**：调和函数在定义域内任一点的值等于以该点为中心的球面或球体上的平均值。
注: **最大模原理**：调和函数无法再定义域内部达到最值，除非是常数函数。

我们的目的是开发新的神经网络算法。
其中关键挑战在于点源和 PDE 解的低正则性。
注：**低正则性**：即函数的平滑程度较低或导数性质较弱的情形。

我们基于分析见解，即 $u$ 可以分解为奇性部分（由基本解 Fundamental Solutions 表示）和正则部分（由神经网络表示）。
所提出的方法由两步组成：
首先遵循 [^16] 检测点源的数量，然后使用最小二乘形式的经验损失同时学习光滑部分和未知点源的参数（强度和未知）。

总的来说我们做出了如下贡献：
1. 开发了易于实现的算法用于恢复点源。
2. 提供了算法的误差界限。（这是通过将条件稳定性和对经验损失的先验估计恰当组合得到的，使用来自统计学习理论的技术。我们对近似解和精确解之间的 Hausdorff 距离、规则部分和强度的误差进行界限处理，具体由噪音级别、神经网络架构和采样点数量决定）
3. 我们展示了数个数值实验，包括部分柯西数据和 Helmholtz 问题，以说明算法的灵活性和精度。

使用神经网络求解 PDE 反问题吸引了诸多关注，包括监督学习和无监督学习。
但是，用于 ISP 的神经网络求解器仍非常有限。
- Zhang et al. [^43] 采用了深度伽辽金方法 (Deep Galerkin Method, DGM) 来从椭圆问题中恢复源项，使用了解在子区域的度量？并建立了对于解析形式源情形方法的收敛性分析。
- [^44] 则对抛物情形进行了类似处理。
- [^33] 研究了 DNN 对于随机 ISP 的使用。该工作专注于识别强非线性但满足 Holder 稳定性的点源，以确保所提方法的先验误差估计的推导。
- [^12] 开发了新式的分治法 divide-and-conquer 算法用于恢复 Helmholtz 问题的点源。该方法贡献了无监督求解器用于识别点源并提供了严格的数学保证。

[^43]: Solving an Inverse Source Problem by Deep Neural Network Method with Convergence and Error Analysis. 2023.
[^44]: On Stability and Regularization for Data-Driven Solution of Parabolic Inverse Source Problems. Elsevier JCP 2023.
[^33]: A Data-Assisted Two-Stage Method for the Inverse Random Source Problem. SIAM 2023.
[^12]: Divide-and-Conquer DNN Approach for the Inverse Point Source Problem Using a Few Single Frequency Measurements. 2023.

## 方法

现在我们描述所提方法，名为**奇性富集神经网络 (Singularity Enriched Neural Network, SENN)**，用于识别点源。
过程分为两步：
1. 检测点源的数量 $M$；
2. 借助奇性富集估计位置和强度。

对于一个调和函数 $w$，我们将以下泛函标记为 $\mathcal{R}(w;f^{\delta}, g^{\delta})$，简写为 $\mathcal{R}(w)$:

$$
\mathcal{R}(w;f^{\delta}, g^{\delta})=\int_{\partial\Omega} -g^{\delta} w + f^{\delta} \partial_{\vec{\nu}}w\text{d}\mathbf{x},
$$

该泛函也称为**互易性间隙泛函 (Reciprocity Gap Functional)**。它将在建立稳定性估计中发挥重要作用。

注：
- **互易性 (Reciprocity)**：物理学的基本原理，指某些系统中输入和输出的角色可以互换而不改变系统响应。
- **间隙泛函 (Gap Functional)**：衡量两个函数或数据集之间差异的工具。在反问题中通常用于比较观测数据和模型预测数据之间的差异。

由格林第二定理可知
$$
\begin{aligned}
\int_\Omega (u\Delta w-w\Delta u)\text{d}v
&= \int_{\partial \Omega} (u \partial_{\vec{\nu}} w - w\partial_{\vec{\nu}}u)\text{d}\mathbf{s}\\
\int_\Omega F\cdot w\text{d}v&=\int_{\partial\Omega}f^{\delta} \partial_{\vec{\nu}} w - g^{\delta}w\text{d}\mathbf{s}
\end{aligned}
$$

### 检测点源数量

首先回顾点源数量的检测过程。
我们只描述二维情形，遵循[^16]。

注意到复值多项式的实部和虚部都是调和的。
通过设置 $w_m(\mathbf{x})=w_m(x,y)=z^m=(x+iy)^m,m\in\mathbb{N}$，我们获得

$$
\mathcal{R}(w_m) = \sum_{j=1}^{M} c_j (x_j + i y_j)^m.
$$

给定一个上界 $\overline{M}\geq M$，我们可以发现使用 [^16] 中的引理 2 找到 $M$。

**命题 2.1.** 令 Hankel 矩阵 $A=(a_{ij})_{\overline{M}\times\overline{M}}$，其中元素 $a_{ij}=\mathcal{R}(w_{i+j-2})\in \mathbb{C}$。那么矩阵 $A$ 的秩为 $M$ 且 $A$ 的前 $M$ 列线性无关。

该命题建议我们可以从矩阵 $A$ 的第一列开始，每次前进一列，直到列变成线性相关。停止的索引将给出 $M$ 的值。实践中，由于在计算元素 $a_{ij}$ 中存在求积误差，停止判定可能不精确。令 $\mu_k$ 是 $A$ 的第 $k$ 列，令 $\sigma_s(A)$ 是 $A$ 的最小奇异值。如果由前 $k$ 列组成的 $A_k$ 的最小奇异值 $\sigma_s(A_k)$ 小于给定的容忍度 $\epsilon_{tol}$，那么我们可以称这 $k$ 个列是线性无关的。

对于该方法的稳定性，定义 Vandermonde 矩阵 $B$，对角矩阵 $D$，元素由 $x_j+i y_j$ 构成。那么 $A$ 的第 $k$ 列等于 $B D^{k-1} c$，那么 $A_M$ 可以拆分为 $B$ 和 $C$ 的乘积，$C$ 是 $c, Dc, \cdots, D^{M-1}c$ 的拼接。
而 $B$ 可以拆分出大小为 $M$ 的方阵，即 $B_M C$ 是 $A_M$ 的子矩阵，所以 $\sigma_s(A_M)\geq \sigma_s(B_M C)>0$。所以我们实践中会使用小于 $\sigma_s(B_M C)$ 的容忍度，且固定在噪声级别左右。

### 奇性富集

一旦估计好了点源数量，我们应用深度学习来学习解 $u$ 和点源的位置和密度。
由于点源的存在，$u$ 是非光滑的，而标准的神经 PDE 求解器会失效 [^24]。
我们采用奇性富集，分离出奇性部分，以解决这一挑战。
这一思想被广泛用于有限元方法背景中 [^7] [^8] [^18] [^19] 和神经 PDE 求解器 [^24] [^25]。我们扩展了这一思想以恢复点源。

[^7]: A Finite Element Method Using Singular Functions for the Poisson Equation: Corner Singularities. SIAM Numerical Analysis, 2001.
[^8]: Solution Methods for the Poisson Equation with Corner Singularities: Numerical Results. SIAM JSC 2001.
[^18]: On the Use of Singular Functions with Finite Element Approximations. Elsevier JCP 1973.
[^19]: The Extended/Generalized Finite Element Method: An Overview of the Method and Its Applications. 2010.

[^24]: Solving Elliptic Problems with Singular Sources Using Singularity Splitting Deep Ritz Method. SIAM JSC 2023. (自引)
[^25]: Solving Poisson Problems in Polygonal Domains with Singularity Enriched Physics Informed Neural Networks. SIAM JSC 2024. (自引)

我们采用对 Laplace 方程采用基本解，
$$
\Phi(\mathbf{x}) = c_d \begin{cases}-\log |\mathbf{x}|, &d=2\\ |\mathbf{x}|^{2-d}, &d\geq 3\end{cases}
$$

- $c_2=\dfrac{1}{2\pi}$
- $c_3=\dfrac{1}{d(d-2)\alpha(d)}$
- $\alpha(d)=\dfrac{\pi^\frac{d}{2}}{\Gamma(1+\frac{d}{2})}$

对于任意 $\mathbf{x}_0\in\mathbb{R}^d$，$-\Delta \Phi(\mathbf{x}-\mathbf{x}_0)=\delta_{\mathbf{x}_0}$，其中 $\delta_{\mathbf{x}_0}$ 是狄拉克函数。下面将 $\Phi(\mathbf{x}-\mathbf{x}_0)$ 缩写为 $\Phi_{\mathbf{x}_0}$。

我们可以将泊松问题的解 $u$ 分解为

$$
u = \sum_{j=1}^{M} c_j \Phi_{\mathbb{x}_j}+v
$$

前面一项用于捕获解 $u$ 由于点源 $\sum_{j=1}^{M} c_j\delta_{\mathbf{x}_j}$ 带来的奇性部分，后面一项是规则部分。

那么规则部分满足的方程则变为

$$
\begin{cases}
-\Delta v = 0, &\text{in}\quad \Omega\\
v = f - F(c,X), &\text{on}\quad \partial\Omega\\
\partial_{\vec{\nu}}v = g-\partial_{\vec{\nu}}F(c,X), &\text{on}\quad \partial\Omega
\end{cases}
$$

其中 $F(c,X)=\sum_{j=1}^{M} c_j\Phi_{\mathbb{x}_j}$。

在上述解分解式中我们已经使用了关于未知点源的信息。
这一改写使我们获得如下损失函数：

$$
L^{\delta}(v,c,X) = \| \Delta v \|_{L^2(\Omega)}^2 + \sigma_d \|v-f^{\delta} + F(c,X)\|_{L^2(\partial\Omega)}^2 + \sigma_n \|\partial_{\vec{\nu}}v - g^{\delta} + \partial_{\vec{\nu}}F(c,X)\|_{L^2(\partial\Omega)}^2
$$

其中 $\sigma_d, \sigma_n$ 是权重系数。$c$ 是点源强度向量。$X$ 是位置矩阵。

### SENN

为了近似规则部分 $v$，我们采用标准的全连接前馈神经网络。

标准的神经网络定义略过不写。设神经网络类为 $\mathcal{A}$，那么我们采用 $v_\theta\in \mathcal{A}$ 来近似 $v$，然后使用求积公式离散相关的积分，如蒙特卡洛积分。设 $U(D)$ 为 $D$ 上的均匀分布，$|D|$ 表示勒贝格测度。
那么损失函数可以重写为：

$$
\begin{aligned}
L^{\delta}(v_\theta,c,X)
&= |\Omega| \mathbb{E}_{U(\Omega)}[|\Delta v_{\theta}(Z)|^2]\\
&+ \sigma_d |\partial \Omega|\mathbb{E}_{U(\partial\Omega)}[|(v_{\theta}-f^{\delta}+F(c,X))(Y)|^2]\\
&+ \sigma_n |\partial \Omega|\mathbb{E}_{U(\partial\Omega)}[|(\partial_{\vec{\nu}}v_{\theta}-g^{\delta}+\partial_{\vec{\nu}}F(c,X))(Y)|^2]
\end{aligned}
$$

用 $X_{i_1}$ 和 $Y_{i_2}$ 分别表示从两个分布 $U(\Omega)$ 和 $U(\partial\Omega)$ 中独立同分布地采样的 $N_r$ 和 $N_b$ 个样本点。
那么经验损失 $\hat{L^{\delta}}$ 为

$$
\begin{aligned}
\hat{L^{\delta}}(v_\theta,c,X)
&= \frac{|\Omega|}{N_r}\sum_{i=1}^{N_r} |\Delta v_{\theta}(X_{i_1})|^2 \\
&+ \sigma_d \frac{|\partial \Omega|}{N_b}\sum_{i=1}^{N_b} |(v_{\theta}-f^{\delta}+F(c,X))(Y_{i_2})|^2 \\
&+ \sigma_n \frac{|\partial \Omega|}{N_b}\sum_{i=1}^{N_b} |(\partial_{\vec{\nu}}v_{\theta}-g^{\delta}+\partial_{\vec{\nu}}F(c,X))(Y_{i_2})|^2
\end{aligned}
$$

因为参数 $(\theta, c, X)$ 是有限维度且存在于紧集中 (因为 $l^{\infty}$ 界限 $|\theta|_{\infty} \leq B_{\theta}$)，进一步设 $|c|_{\infty} \leq B_c$，那么上述损失函数在参数空间中是连续的，直接意味着全局最小值的存在性。

因为神经网络的非线性，优化问题是非凸的。因此可能难以找到全局最优，实践中使用 Adam 或 L-BFGS。

## 理论

现在我们给出上述方法的误差分析以提供理论保证。
这一分析颇具挑战性因为 ISP 的非线性。
我们的分析结合了 ISP 的条件稳定性和对经验误差的一致性误差分析。
对于 PDE 神经求解器，经验误差的分析已经被研究了 [^28] [^24] [^34]。

[^28]: A Rate of Convergence of PINNs for the Linear Second Order Elliptic PDEs. CCP 2022.
[^34]: Machine Learning for Elliptic PDEs: Fast Rate Generalization Bound, Neural Scaling Law and Minimax Optimality. ICLR 2022.

### 条件稳定性

首先我们分析损失 $L(v_\theta,c,X)$ 的稳定性。

设样本点 $\mathbf{x}=(x_1,\cdots,x_d)^{\mathsf{T}}\in \Omega\subset \mathbb{R}^d$，那么样本点和边界的欧氏距离定义为
$$
\text{dist}(\mathbf{x},\partial\Omega) = \inf_{\mathbf{y}\in\partial\Omega} \text{dist}(\mathbf{x},\mathbf{y}).
$$

取 $M$ 个点，然后选取它们和边界之间的欧氏距离的最小值为 $\gamma$。
然后令 $\Omega_\gamma$ 表示样本点和边界之间的欧氏距离大于等于 $\gamma$ 的点集。显然这 $M$ 个点都属于 $\Omega_\gamma$。

令 $\beta$ 表示 $\Omega$ 的直径和 $\gamma$ 的差值。
然后我们假设最优的 $c_j$ 具有下界 $b_c$ 和上界 $B_c$，即 $0<b_c \leq |c_j^*|\leq B_c$。
那么整个参数空间为 $\mathbb{A}=\mathcal{A}\times I_c^{M}\times \Omega_{\gamma}^M$，其中 $I_c = [-B_c, b_c]\cup [b_c, B_c]$。

首先对经验风险最小值 $\hat{X}^*$ 的误差进行取界。
这部分的证明受到 ISP 的条件稳定性分析 [^13] [^14] 的启发。
其关键在于需要对原始的泊松方程乘以细致构造的测试函数，并利用互易性间隙泛函。然后使用 Vandermonde 类矩阵的解析性质，我们可以获得从柯西数据识别点源的条件稳定性。
这一构造依赖于复值解析函数的调和性质，只在二维复值平面存在。
更具体地，对于 $M$ 个点组成的 $X$，用 $P_k(\mathbf{x}_j)=x_{k,j}+i y_{k,j}$ 表示映射到 $x_k O x_{k+1}$ 复值平面上，其中 $k=1,\cdots,d-1$，然后 $\mathbf{P}_k(X)$ 为 $M$ 个点的 $P_k$ 函数和集合。
然后定义映射源 $\mathbf{P}_k(X)$ 的可分离系数 (Separability Coefficient) $\rho_k(X)$，以及距离 $\rho(X)$ 为

[^13]: Holder Stability Estimates for Some Inverse Pointwise Source Problems. 2012.
[^14]: Stability Estimates for an Inverse Source Problem of Helmholtz's Equation from Single Cauchy Data at a Fixed Frequency. 2013.

$$
\begin{aligned}
    \rho_k(X) = \min \text{dist}(P_k(\mathbf{x}_i), P_k(\mathbf{x}_j))\\
    \rho(X) = \min_{1\leq k< d} \rho_k(X)
\end{aligned}
$$

那么对于两组点源位置配置 $\hat{X}^* 和 X^*$，我们做出如下可分离性假设以便后续分析：

**假设 1.** 存在大于零的距离 $\rho_*>0$ 使得两个点源配置的距离都大于该距离，即 $\min(\rho(\hat{X}^*),\rho(X^*))\geq \rho_*>0$。

我们定义两组点源之间的 Hausdorff 距离为

$$
d_H(\hat{X}^*,X^*) = \max(\max_{1\leq i\leq M}\min_{1\leq j\leq \hat{M}} \text{dist}(\mathbf{x}^*_i,\hat{\mathbf{x}}^*_j), \max_{1\leq j\leq \hat{M}}\min_{1\leq i\leq M} \text{dist}(\mathbf{x}^*_i,\hat{\mathbf{x}}^*_j))
$$

然后我们对该 Hausdorff 距离有如下稳定性结果。

**定理 1.** 令 $X^*$ 和 $\hat{X}^*$ 分别为损失 $L$ 和经验损失 $\hat{L}^{\delta}$ 的最小值参数，那么在**假设 1** 下，有下式成立：

$$
d_H(\hat{X}^*,X^*)\leq C(\Omega,\beta,M,\hat{M},\sigma,b_c,\rho_*,d) L(v_{\hat{\theta}},\hat{c}^*,\hat{X}^*)^{\frac{1}{2\max(M,\hat{M})}}
$$

使用图理论中的 Hall-Rado 定理和定理 3.1，当取 $M=\hat{M}$ 时，我们有如下误差界限：

**定理 2.** 当满足**假设 1.** 时，对于两组点源配置 $\hat{X}^*$ 和 $X^*$，如果它们的点数相同，那么存在一个 $1\sim M$ 的排列 $\pi$ 使得
$$
\max_{1\leq j\leq M} \text{dist}(\mathbf{x}^*_j,\hat{\mathbf{x}}^*_{\pi(j)})\leq C(d-1) L(v_{\hat{\theta}},\hat{c}^*,\hat{X}^*)^{\frac{1}{2M}}
$$

这一结果直接由**定理 1** 给出（结合了的定理 3）。

**定理 3. (Hall-Rado)** 考虑一个具有 $2M$ 个节点的偶数图，使得对于每个 $k\in\{1,\cdots,M\}$ 和 $a_1,\cdots,a_M$ 的每个子序列 $a_{j_1}, \cdots, a_{j_k}$，$b_1,\cdots,b_M$ 至少 $k$ 个元素和它们连接，那么存在一个排列 $\pi$ 使得对于每个 $j$ 都有 $a_j$ 连接到 $b_{\pi(j)}$。

注：Hall-Rado 定理是匹配理论中 Hall 定理的一种推广，考虑了更一般的可交叉系统。
Hall 定理/婚配定理的内容是：给定一个二部图，对于左侧顶点集合的任意子集，其领域满足点数大于该子集的大小，那么存在一个完美匹配，使得左侧每个顶点都能匹配到右侧的不同顶点。
Hall-Rado 定理则将该结论推广到了更一般的集合系统上，即给定一个有限集合族，和一个取值为正整数的函数，使得对于该有限集合的任意子集，其可交叉覆盖数大于函数值之和，则可以选择一组两两互不相交的集合使得他们的大小正好等于函数值。

---

下面我们对复原的密度 $\hat{c}^*$ 取界。

**定理 4.** 对于两组配置 $(c^*, X^*)$ 和 $(\hat{c}^*, \hat{X}^*)$，$\pi$ 为**定理 2** 中的排列，那么在**假设 1** 下有

$$
\max_{1\leq j\leq M} |\hat{c}_j^*-c_{\pi(j)}|\leq C(\Omega, \beta, M, \sigma, b_c, B_c, \rho_*) L(v_{\hat{\theta}},\hat{c}^*,\hat{X}^*)^{\frac{1}{2M}}
$$

---

下面对规则部分 $v_{\hat{\theta}^*}$ 在整体损失进行误差分析。

**定理 5.** $v_{\hat{\theta}^*}$ 是 $\hat{L}^{\delta}$ 的最小值点，$v^*$ 是 $L$ 的最小值点，那么在**假设 1** 下有

$$
\| v^* - v_{\hat{\theta}^*} \|_{L^2(\Omega)} \leq C(\Omega, \beta, M, \sigma, b_c, B_c, \rho_*, \gamma, d) L(v_{\hat{\theta}^*},\hat{c}^*,\hat{X}^*)^{\frac{1}{2M}}
$$

---

最后给出解 $u^*$ 的近似的误差界限。

**推论 1.** $u_{\hat{\theta}^*}=v_{\hat{\theta}^*}+\sum_{j=1}^M\hat{c}_i^* \Phi_{\hat{\mathbf{x}}_J^*}$, $u^*$ 为原始问题的解，那么对于任意的 $p<\dfrac{d}{d-1}$ 有

$$
\| u^* - u_{\hat{\theta}^*} \|_{L^2(\Omega)} \leq C(\Omega, \beta, M, \sigma, b_c, B_c, \rho_*, \gamma, p, d) L(v_{\hat{\theta}^*},\hat{c}^*,\hat{X}^*)^{\frac{1}{2M}}
$$

### 经验损失的泛化误差分析

前面的分析是总体损失 $L^{\delta}$ 在经验误差 $\hat{L}^{\delta}$ 的最优值点处的分析。
然而在实践中我们只能优化经验损失，无法直接使用总体损失。

而总体损失和经验损失之间的误差分析通常在统计学习理论中称为泛化误差分析。

首先建立重要的误差划分，涉及到三个部分：
- 近似误差 $\inf_{v_\theta} \| v_\theta - v^* \|_{H^2(\Omega)}^2$
- 统计误差 $\sup_{\mathbb{A}} |\hat{L}^{\delta}(v_\theta,c,X) - L^{\delta}(v_\theta,c,X)|$
- 噪声级别 $\delta$

#TODO

## 实验

代码链接：https://github.com/hhjc-web/point-source-identification