# Singularity Enriched Neural Networks/SENNs

<details>
<summary>基本信息</summary>

- 标题: "Point Source Identification Using Singularity Enriched Neural Networks"
- 作者:
  - 01 Tianhao Hu
  - 02 Bangti Jin
  - 03 Zhi Zhou
- 链接:
  - [ArXiv](https://arxiv.org/abs/2408.09143)
  - [Publication]()
  - [Github]()
  - [Demo]()
- 文件:
  - [ArXiv]()
  - [Publication] #TODO

</details>

## 摘要

The inverse problem of recovering point sources represents an important class of applied inverse problems.
However, there is still a lack of neural network-based methods for point source identification, mainly due to the inherent solution singularity.
In this work, we develop a novel algorithm to identify point sources, utilizing a neural network combined with a singularity enrichment technique.
We employ the fundamental solution and neural networks to represent the singular and regular parts, respectively, and then minimize an empirical loss involving the intensities and locations of the unknown point sources, as well as the parameters of the neural network.
Moreover, by combining the conditional stability argument of the inverse problem with the generalization error of the empirical loss, we conduct a rigorous error analysis of the algorithm.
We demonstrate the effectiveness of the method with several challenging experiments.

恢复点源的反问题是一类重要的应用反问题。
然而，目前仍然缺乏基于神经网络的方法来识别点源，主要是由于固有解奇性。
在本文中，我们开发了一种新算法用于识别点源，使用神经网络，并结合奇性富集技术。
我们采用了基本解和使用神经网络来分别表示奇性和规则部分，然后最小化一个经验损失函数，该函数涉及未知点源的强度和位置以及神经网络的参数。
此外，通过将反问题的条件稳定性分析和经验损失的泛化误差相结合，我们构造了严格的误差分析。
我们通过数个挑战性实验来展示该方法的有效性。

## 引言

标准泊松方程:
$$
-\Delta u = F,\quad  \text{in}\quad \Omega.
$$

- $\Omega\subset (-1,1)^d$: 有界开集。
- $\partial \Omega$: 光滑边界。
- $\vec{\nu}$: 边界的单位外法向量。
- $\partial_{\vec{\nu}}u$: 外法向导数。

本工作关心的反问题是**逆源问题 (Inverse Source Problem, ISP)**，即从**柯西数据** $(f,g)=(u|_{\partial \Omega}, \partial_{\vec{\nu}}u)\in H^{\frac{3}{2}}(\partial \Omega)\times H^{\frac{1}{2}}(\partial \Omega)$ 中确定源 $F$。
特别地，我们只能获得噪声数据 $(f^{\delta},g^{\delta})\in (L^2(\partial\Omega))^2$，噪声级别 $\delta$，即
$$
\|f-f^{\delta}\|_{L^2(\partial\Omega)} \leq \delta.\quad \text{and}\quad \|g-g^{\delta}\|_{L^2(\partial\Omega)} \leq \delta.
$$

注：**柯西数据**通常用于描述在某个边界或初始时刻给定的函数值及其导数值，包括函数本身和法向导数在边界或初始时刻的值。

泊松问题的 ISP 已经被广泛研究了。
在只有一对柯西数据时，不可能唯一复原出一般的源 $F$: 给 $u$ 增加任意光滑函数 $v$ 和紧支集 $\overline{\text{supp}(v)}\subset \Omega$ 不会改变柯西数据。
因此，关于 $F$ 的合适的先验知识 (物理本质) 是必须的，以便恢复唯一辨识性。
一个柯西数据对可以唯一确定未知源 $F$，例如乘积源 $F$ 的一个因子 (即 $F$ 是分离且一个因子已知)，或**球面分层源的角向因子 (Angular Factor of A Spherically Layered Source)**，或 $F=\chi_{D}\rho(x)$ 或 $F=\nabla\cdot (\rho(x)\chi_{D}(x)a)$ ($\chi_{D}$ 是多边形 $D\subset \Omega$ 的特征函数，$a$ 是非零常数向量)。

本工作研究 $F$ 是点源的线性组合的情形:

$$
F=\sum_{j=1}^{M} c_j \delta_{\mathbf{x}_{j}},\quad\mathbf{x}_j\in\Omega
$$

- $\delta_{\mathbf{x}_j}$ 是中心在 $\mathbf{x}_j$ 的狄拉克Δ函数，定义为 $\int_{\Omega} \delta_{\mathbf{x}_j}v\text{d}x=v(\mathbf{x}_j)$ 对于所有 $v\in C(\bar{\Omega})$，$c_j\neq 0$ 是点源在 $\mathbf{x}_j$ 处的强度。

未知参数包括点源的数量 $M$，位置和强度 $\{(\mathbf{x}_j, c_j)\}_{j=1}^{M}$。

这一情形出现在多种实际应用中，例如脑电图，狄拉克函数可以描述大脑中的活动源。
特定的反问题满足唯一辨识性和 Holder 稳定性。

[^16] 开发了一种直接法来识别2维泊松问题中点源的数量和位置、强度。
从柯西数据中我们可以计算任意调和函数 $w$ 的值 $\sum_{j=1}^{M} c_j w(\mathbf{x}_j)$。通过选择合适的 $w$ 可以确定点源的数量和密度。它对二维和三维情形都运作良好，$w$ 从复杂解析解中构造得到。
这在高维情形中更为复杂，构造调和测试函数 $w$ 和计算**互易性间隙泛函 (Reciprocity Gap Functional)** 都是非平凡的。

注：**调和函数**：满足拉普拉斯方程的函数，具有平均值性质、最大模原理等性质。
注：**平均值性质**：调和函数在定义域内任一点的值等于以该点为中心的球面或球体上的平均值。
注: **最大模原理**：调和函数无法再定义域内部达到最值，除非是常数函数。

我们的目的是开发新的神经网络算法。
其中关键挑战在于点源和 PDE 解的低正则性。
注：**低正则性**：即函数的平滑程度较低或导数性质较弱的情形。

我们基于分析见解，即 $u$ 可以分解为奇性部分（由基本解 Fundamental Solutions 表示）和正则部分（由神经网络表示）。
所提出的方法由两步组成：
首先遵循 [^16] 检测点源的数量，然后使用最小二乘形式的经验损失同时学习光滑部分和未知点源的参数（强度和未知）。

总的来说我们做出了如下贡献：
1. 开发了易于实现的算法用于恢复点源。
2. 提供了算法的误差界限。（这是通过将条件稳定性和对经验损失的先验估计恰当组合得到的，使用来自统计学习理论的技术。我们对近似解和精确解之间的 Hausdorff 距离、规则部分和强度的误差进行界限处理，具体由噪音级别、神经网络架构和采样点数量决定）
3. 我们展示了数个数值实验，包括部分柯西数据和 Helmholtz 问题，以说明算法的灵活性和精度。

使用神经网络求解 PDE 反问题吸引了诸多关注，包括监督学习和无监督学习。
但是，用于 ISP 的神经网络求解器仍非常有限。
- Zhang et al. [^43] 采用了深度伽辽金方法 (Deep Galerkin Method, DGM) 来从椭圆问题中恢复源项，使用了解在子区域的度量？并建立了对于解析形式源情形方法的收敛性分析。
- [^44] 则对抛物情形进行了类似处理。
- [^33] 研究了 DNN 对于随机 ISP 的使用。该工作专注于识别强非线性但满足 Holder 稳定性的点源，以确保所提方法的先验误差估计的推导。
- [^12] 开发了新式的分治法 divide-and-conquer 算法用于恢复 Helmholtz 问题的点源。该方法贡献了无监督求解器用于识别点源并提供了严格的数学保证。

## 方法

现在我们描述所提方法，名为**奇性富集神经网络 (Singularity Enriched Neural Network, SENN)**，用于识别点源。
过程分为两步：
1. 检测点源的数量 $M$；
2. 借助奇性富集估计位置和强度。

对于一个调和函数 $w$，我们将以下泛函标记为 $\mathcal{R}(w;f^{\delta}, g^{\delta})$，简写为 $\mathcal{R}(w)$:

$$
\mathcal{R}(w;f^{\delta}, g^{\delta})=\int_{\partial\Omega} -g^{\delta} w + f^{\delta} \partial_{\vec{\nu}}w\text{d}\mathbf{x},
$$

该泛函也称为**互易性间隙泛函 (Reciprocity Gap Functional)**。它将在建立稳定性估计中发挥重要作用。

注：
- **互易性 (Reciprocity)**：物理学的基本原理，指某些系统中输入和输出的角色可以互换而不改变系统响应。
- **间隙泛函 (Gap Functional)**：衡量两个函数或数据集之间差异的工具。在反问题中通常用于比较观测数据和模型预测数据之间的差异。

由格林第二定理可知
$$
\begin{aligned}
\int_\Omega (u\Delta w-w\Delta u)\text{d}v
&= \int_{\partial \Omega} (u \partial_{\vec{\nu}} w - w\partial_{\vec{\nu}}u)\text{d}\mathbf{s}\\
\int_\Omega F\cdot w\text{d}v&=\int_{\partial\Omega}f^{\delta} \partial_{\vec{\nu}} w - g^{\delta}w\text{d}\mathbf{s}
\end{aligned}
$$

### 检测点源数量

首先回顾点源数量的检测过程。
我们只描述二维情形，遵循[^16]。

注意到复值多项式的实部和虚部都是调和的。
通过设置 $w_m(\mathbf{x})=w_m(x,y)=z^m=(x+iy)^m,m\in\mathbb{N}$，我们获得

$$
\mathcal{R}(w_m) = \sum_{j=1}^{M} c_j (x_j + i y_j)^m.
$$

给定一个上界 $\overline{M}\geq M$，我们可以发现使用 [^16] 中的引理 2 找到 $M$。

**命题 2.1.** 令 Hankel 矩阵 $A=(a_{ij})_{\overline{M}\times\overline{M}}$，其中元素 $a_{ij}=\mathcal{R}(w_{i+j-2})\in \mathbb{C}$。那么矩阵 $A$ 的秩为 $M$ 且 $A$ 的前 $M$ 列线性无关。

该命题建议我们可以从矩阵 $A$ 的第一列开始，每次前进一列，直到列变成线性相关。停止的索引将给出 $M$ 的值。实践中，由于在计算元素 $a_{ij}$ 中存在求积误差，停止判定可能不精确。令 $\mu_k$ 是 $A$ 的第 $k$ 列，令 $\sigma_s(A)$ 是 $A$ 的最小奇异值。如果由前 $k$ 列组成的 $A_k$ 的最小奇异值 $\sigma_s(A_k)$ 小于给定的容忍度 $\epsilon_{tol}$，那么我们可以称这 $k$ 个列是线性无关的。

对于该方法的稳定性，定义 Vandermonde 矩阵 $B$，对角矩阵 $D$，元素由 $x_j+i y_j$ 构成。那么 $A$ 的第 $k$ 列等于 $B D^{k-1} c$，那么 $A_M$ 可以拆分为 $B$ 和 $C$ 的乘积，$C$ 是 $c, Dc, \cdots, D^{M-1}c$ 的拼接。
而 $B$ 可以拆分出大小为 $M$ 的方阵，即 $B_M C$ 是 $A_M$ 的子矩阵，所以 $\sigma_s(A_M)\geq \sigma_s(B_M C)>0$。所以我们实践中会使用小于 $\sigma_s(B_M C)$ 的容忍度，且固定在噪声级别左右。

### 奇性富集

一旦估计好了点源数量，我们应用深度学习来学习解 $u$ 和点源的位置和密度。
由于点源的存在，$u$ 是非光滑的，而标准的神经 PDE 求解器会失效 [^24]。
我们采用奇性富集，分离出奇性部分，以解决这一挑战。
这一思想被广泛用于有限元方法背景中 18 7 8 19 和神经 PDE 求解器 24 25。我们扩展了这一思想以恢复点源。

我们采用对 Laplace 方程采用基本解 [17]，
$$
\Phi(\mathbf{x}) = c_d \begin{cases}-\log |\mathbf{x}|, &d=2\\ |\mathbf{x}|^{2-d}, &d\geq 3\end{cases}
$$

- $c_2=\dfrac{1}{2\pi}$
- $c_3=\dfrac{1}{d(d-2)\alpha(d)}$
- $\alpha(d)=\dfrac{\pi^\frac{d}{2}}{\Gamma(1+\frac{d}{2})}$

对于任意 $\mathbf{x}_0\in\mathbb{R}^d$，$-\Delta \Phi(\mathbf{x}-\mathbf{x}_0)=\delta_{\mathbf{x}_0}$，其中 $\delta_{\mathbf{x}_0}$ 是狄拉克函数。下面将 $\Phi(\mathbf{x}-\mathbf{x}_0)$ 缩写为 $\Phi_{\mathbf{x}_0}$。

我们可以将泊松问题的解 $u$ 分解为

$$
u = \sum_{j=1}^{M} c_j \Phi_{\mathbb{x}_j}+v
$$

前面一项用于捕获解 $u$ 由于点源 $\sum_{j=1}^{M} c_j\delta_{\mathbf{x}_j}$ 带来的奇性部分，后面一项是规则部分。

那么规则部分满足的方程则变为

$$
\begin{cases}
-\Delta v = 0, &\text{in}\quad \Omega\\
v = f - F(c,X), &\text{on}\quad \partial\Omega\\
\partial_{\vec{\nu}}v = g-\partial_{\vec{\nu}}F(c,X), &\text{on}\quad \partial\Omega
\end{cases}
$$

其中 $F(c,X)=\sum_{j=1}^{M} c_j\Phi_{\mathbb{x}_j}$。

在上述解分解式中我们已经使用了关于未知点源的信息。
这一改写使我们获得如下损失函数：

$$
L^{\delta}(v,c,X) = \| \Delta v \|_{L^2(\Omega)}^2 + \sigma_d \|v-f^{\delta} + F(c,X)\|_{L^2(\partial\Omega)}^2 + \sigma_n \|\partial_{\vec{\nu}}v - g^{\delta} + \partial_{\vec{\nu}}F(c,X)\|_{L^2(\partial\Omega)}^2
$$

其中 $\sigma_d, \sigma_n$ 是权重系数。$c$ 是点源强度向量。$X$ 是位置矩阵。

### SENN

为了近似规则部分 $v$，我们采用标准的全连接前馈神经网络。

标准的神经网络定义略过不写。设神经网络类为 $\mathcal{A}$，那么我们采用 $v_\theta\in \mathcal{A}$ 来近似 $v$，然后使用求积公式离散相关的积分，如蒙特卡洛积分。设 $U(D)$ 为 $D$ 上的均匀分布，$|D|$ 表示勒贝格测度。
那么损失函数可以重写为：

$$
\begin{aligned}
L^{\delta}(v_\theta,c,X)
&= |\Omega| \mathbb{E}_{U(\Omega)}[|\Delta v_{\theta}(Z)|^2]\\
&+ \sigma_d |\partial \Omega|\mathbb{E}_{U(\partial\Omega)}[|(v_{\theta}-f^{\delta}+F(c,X))(Y)|^2]\\
&+ \sigma_n |\partial \Omega|\mathbb{E}_{U(\partial\Omega)}[|(\partial_{\vec{\nu}}v_{\theta}-g^{\delta}+\partial_{\vec{\nu}}F(c,X))(Y)|^2]
\end{aligned}
$$

用 $X_{i_1}$ 和 $Y_{i_2}$ 分别表示从两个分布 $U(\Omega)$ 和 $U(\partial\Omega)$ 中独立同分布地采样的 $N_r$ 和 $N_b$ 个样本点。
那么经验损失 $\hat{L^{\delta}}$ 为

$$
\begin{aligned}
\hat{L^{\delta}}(v_\theta,c,X)
&= \frac{|\Omega|}{N_r}\sum_{i=1}^{N_r} |\Delta v_{\theta}(X_{i_1})|^2 \\
&+ \sigma_d \frac{|\partial \Omega|}{N_b}\sum_{i=1}^{N_b} |(v_{\theta}-f^{\delta}+F(c,X))(Y_{i_2})|^2 \\
&+ \sigma_n \frac{|\partial \Omega|}{N_b}\sum_{i=1}^{N_b} |(\partial_{\vec{\nu}}v_{\theta}-g^{\delta}+\partial_{\vec{\nu}}F(c,X))(Y_{i_2})|^2
\end{aligned}
$$

因为参数 $(\theta, c, X)$ 是有限维度且存在于紧集中 (因为 $l^{\infty}$ 界限 $|\theta|_{\infty} \leq B_{\theta}$)，进一步设 $|c|_{\infty} \leq B_c$，那么上述损失函数在参数空间中是连续的，直接意味着全局最小值的存在性。

因为神经网络的非线性，优化问题是非凸的。因此可能难以找到全局最优，实践中使用 Adam 或 L-BFGS。
