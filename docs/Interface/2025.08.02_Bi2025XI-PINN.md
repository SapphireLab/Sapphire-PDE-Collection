# Extended Interface Physics-Informed Neural Networks Method for Moving Interface Problems

<details>
<summary>基本信息</summary>

- 标题: "Extended Interface Physics-Informed Neural Networks Method for Moving Interface Problems."
- 作者:
  - 01 Ran Bi
  - 02 Weibing Deng
  - 03 Yameng Zhu
- 链接:
  - [ArXiv](https://arxiv.org/abs/2508.01463v1)
  - [Publication]()
  - [Github]()
  - [Demo]()
- 文件:
  - [ArXiv:2508.01463v1](D:\PDE\Sapphire-PDE-Collection\docs\Interface\_PDF\2025.08.02_2508.01463v1_Extended_Interface_Physics-Informed_Neural_Networks_Method_for_Moving_Interface_Problems.pdf)
  - [Publication] #TODO

</details>

## Abstract

Physics-Informed Neural Networks (PINNs) have emerged as a powerful class of mesh-free numerical methods for solving partial differential equations (PDEs), particularly those involving complex geometries.
In this work, we present an innovative Extended Interface Physics-Informed Neural Network (XI-PINN) framework specifically designed to solve parabolic moving interface problems.
The proposed approach incorporates a level set function to characterize the interface, which can be obtained either directly or through a neural network solution.
We conduct a rigorous a priori error analysis for the XI-PINN method, providing error bounds for the approximation.
Leveraging the Neural Tangent Kernel (NTK) theory, we further demonstrate that XI-PINN achieves a faster training convergence rate compared to conventional PINN approaches.
The method's versatility is further demonstrated by its application to the Oseen equations. 
We perform comprehensive numerical experiments to validate the efficacy, accuracy, and robustness of the proposed framework.

## 1·Introduction

Interface problems arise in many scientific and engineering models that involve multiple materials with different chemical or physical properties (see [^Greengard1994On], [^Sussman1999Efficient,] and references therein).

In these models, the computational domain is composed of subdomains that are separated by smooth curves (or surfaces) known as interfaces.

The interface geometry itself may involve certain dynamics, i.e., the interface between different subdomains varies in time.

The low global regularity of the solutions and evolving interface bring challenges to numerical simulations, in particular, when the interface has large deformations.

Let $\Omega \subseteq \mathbb{R}^d, d\in \mathbb{N}$ be a fixed bounded domain and $\Omega^+(t)$ and $\Omega^-(t)$ are two sub-domains of $\Omega$ partitioned by the evolving interface $\Gamma(t)$ on a time interval $[0,T_{\text{end}}]$ (see Fig.~[Sketch of domain](#Sketch of domain) for an illustration).

In this paper, we consider the following parabolic equation:

$$
\label{eq1.1}
	\left\{

$$
\begin{aligned}

	\partial_t u - \nabla \cdot \left( \beta \nabla u\right)  &= f \quad  &&\mathrm{in}\; \Omega = \Omega^+(t) \cup \Omega^-(t), \quad  t \in [0, T_{\text{end}}],  \\
	u\left( \cdot, t\right) &= g \quad  &&\mathrm{on}\; \partial \Omega, \quad t \in [0, T_{\text{end}}], \\
	u\left( \cdot, 0\right) &= u_0 \quad &&\mathrm{in}\; \Omega = \Omega^+(0) \cup \Omega^-(0),

\end{aligned}
$$

\right.

$$

with the jump conditions on the moving interface $\Gamma(t)$:

$$
\begin{aligned}

	[u]_{\Gamma(t)} &:= u^+|_{\Gamma(t)} - u^-|_{\Gamma(t)} = h_D, \quad t\in [0, T_{\text{end}}]  \label{eq1.4},\\
	[\beta \nabla u \cdot \mathbf{n}]_{\Gamma(t)}&:= \beta^+ \nabla u^+ \cdot \mathbf{n}|_{\Gamma(t)} - \beta^- \nabla u^- \cdot \mathbf{n}|_{\Gamma(t)} = h_N, \quad t\in [0, T_{\text{end}}]  \label{eq1.5},

\end{aligned}
$$

where {$f(x) \in L^2(0,T_\text{end}; L^2(\Omega))$, and} $\mathbf{n}$ is the unit normal vector to $\Gamma(t)$ pointing from $\Omega^-(t)$ to $\Omega^+(t)$ and the restrictions of $u$ on $\Omega^+(t)$ and $\Omega^-(t)$ are denoted by

$$

	u^+ = u|_{\Omega^+(t)}\quad \text{and} \quad u^- = u|_{\Omega^-(t)}.

$$

The coefficient function $\beta$ is a piece-wise positive constant defined as follows:

$$

	\beta(\mathbf{x}, t) = \left\lbrace 
	

$$
\begin{aligned}

		&\beta^+, \quad \mathrm{for}\;\mathbf{x} \in \Omega^+(t), \\
		&\beta^-, \quad \mathrm{for}\;\mathbf{x} \in \Omega^-(t). 
	
\end{aligned}
$$

\label{eq1.6}
	\right. 

$$

![](sketch_of_domain.eps)

<a id="Sketch of domain">A sketch of domain for the moving interface problem.</a>

Furthermore, we assume that there is a certain velocity field $\mathcal{V}(\mathbf{x}, t)$ that governs the movement of the interface, i.e.,

$$

	\dfrac{\mathrm{d}\mathbf{x}}{\mathrm{d}t} = \mathcal{V}(\mathbf{x}, t), \quad \mathbf{x} \in \Gamma(t).  \label{eq1.7}

$$

The parabolic moving interface problem ([eq1.1](#eq1.1))-([eq1.7](#eq1.7)) appears in many applications, such as the Stefan problem [^Chen1997Simple] and the Burton-Cabrera-Frank-type model [^Caflisch2003Analysis].

For instance, $u$ represents the temperature and the $\mathcal{V}$ is computed by the flux of temperature across the interface in Stefan problems.

Under appropriate assumptions concerning the initial, boundary and jump conditions (see Section~[sec3](#sec3) for detailed discussions), it can be proved that Eqs.([eq1.1](#eq1.1))-([eq1.7](#eq1.7)) admits a unique solution $u$ belonging to the space $L^2(0, T_{\text{end}}; H^1(\Omega))$.

It is well known that the moving interface problems pose significant challenges for numerical simulations due to the computational domain itself evolving.

When employing traditional finite element methods (FEMs), we must ensure the generation of body-fitted meshes conforming to the interface; otherwise, the accuracy of the numerical solution will be destroyed (c.f. [^Babu{\v{s}}ka2000Can]).

Chen and Zou [^Chen1997Simple] conducted a study on the linear FEM on nearly fitted quasi-uniform meshes and proved optimal order error estimates up to some logarithm factors.

In general, mesh regeneration should be minimized or avoided whenever possible, as mesh generation is notoriously laborious and time-consuming, particularly for complex interface geometries and higher-dimensional PDEs.

Consequently, arbitrary Lagrangian-Eulerian methods [^Lan2020Finite] and space-time finite element methods [^Tezduyar1992New] were proposed to solve this difficulty.

To completely eliminate the need for mesh generation in solving PDEs, unfitted methods have captured significant attention from researchers over recent decades.

Unfitted methods allow the interface to cut through mesh elements, while special techniques are required to incorporate jump conditions across the interface with these methods.

To handle interface-cutting elements, a class of methods enforces the jump condition by computation schemes such as {CutFEM [^Burman2015CutFEM], [^Chen2023Arbitrarily]} and IPFEM [^Huang2017Unfitted].

Another class of methods aims to construct specialized basis functions within interface elements, designed specifically to satisfy the interface jump conditions, such as GFEM [^Babu{\v{s}}ka1983Generalized], [^Zhang2022Condensed], XFEM [^Dolbow2001Extended], IFEM [^Li1998Immersed], [^Adjerid2024High].

Guo \cite {guo2021solving} presented the first discrete analysis and optimal error estimates for backward Euler IFEM for parabolic moving interface problems.

All the aforementioned methods are effective for solving interface problems and eliminating the need for body-fitted mesh regeneration at each time step.

However, they still require careful handling of small cut elements [^Chen2021Adaptive] near the interface or the construction of basis functions [^Adjerid2024High] within interface elements that satisfy the jump conditions. 

Solving interface problems via deep learning methods has attracted extensive attention in recent years.

As a mesh-free method, neural networks are particularly well-suited for addressing problems in computational domains with complex geometries and higher-dimensional settings.

Deep learning methods effectively mitigate the curse of dimensionality and eliminate the need for handling solution meshes.

The well-known PINNs [^Raissi2019Physics-Informed] and DeepONet [^Lu2021Learning], along with various improved methods  [^Tseng2023Cusp-Capturing], [^Hu2022Discontinuity], [^Wu2022Inn], [^Ying2024Accurate], [^Wu2024Solving], [^Bi2025XI-DeepONet], have been successfully used to solve elliptic interface problems, yielding promising results. 

However, solving the parabolic interface problems with neural networks remains unexplored to date.

In this paper, we aim to develop an improved PINN method called the {Extended Interface PINN} method for solving parabolic interface problems based on the extended variable technique (EVT)[^Tseng2023Cusp-Capturing], [^Hu2022Discontinuity], and derive the corresponding a priori error estimates.

The EVT enables solving interface problems effectively using only a single neural network, thereby avoiding assigning an independent neural network to each subdomain to approximate the solution within that subdomain.

This approach enables parameter sharing between solution functions across subdomains, leading to enhanced numerical performance.

Moreover, it can preserve the continuity across the interface if necessary \cite {li2025continuity}.

A key limitation is the requirement for an a priori defined level set function to describe the interface.

The level set function is typically obtained by solving the level set evolution equation, which presents significant challenges when no closed-form solution exists.

To address this issue, we propose a novel method that leverages neural networks to identify an appropriate level set function in such cases.

By ingeniously defining the neural network function and employing a temporal training strategy, our approach enables accurate approximation of the level set function.

Furthermore, we conduct an error analysis for the XI-PINN method, decomposing the total error into three components: the approximation error in the neural network space, the statistical error, and the optimization error.

Under appropriate assumptions, {XI-PINN is capable of effective approximation to the solution in Bochner space.}

Finally, leveraging the gradient flow analysis and the NTK theory, we find that XI-PINN indeed {improves} the training convergence rate compared to Vanilla-PINN for solving interface problems.

The rest of this paper is organized as follows.

In Section~[sec 2](#sec 2), we first present the fundamental framework of Deep Neural Networks (DNNs), then introduce the XI-PINN method and propose our novel approach for solving the level set function.

In Section~[sec3](#sec3), we conduct a detailed error analysis of the XI-PINN method and introduce the foundational theoretical concepts related to the NTK.

Numerical results are presented in Section~[sec4](#sec4) to demonstrate the accuracy and efficacy of XI-PINN.

Finally, we give some concluding remarks in Section~[sec5](#sec5).

## 2·Extended Interface PINN Method

\label{sec 2}

In this section, we first review preliminaries on DNNs and the EVT.

Subsequently, we introduce the XI-PINN method for solving parabolic interface problems.

This method efficiently handles problems with geometrically complex interfaces and can be seamlessly extended to higher-dimensional scenarios.

### DNNs

\label{sec2.1}

We use a simple standard fully connected feedforward neural network structure which consists of multiple linear transformations and nonlinear activation functions. 
Consider a DNN of depth $L \ge 2, L \in \mathbb{Z}$.

Let $\left\lbrace n_l \right\rbrace_{l=0}^L$ denote the layer dimensions,  where $n_0$ is the input dimension and $n_L$ is the output dimension.

For each layer $l = 1, \dots, L$, define an affine transformation $\mathbf{T}^l: \mathbb{R}^{n_{l-1}} \to \mathbb{R}^{n_l}$ as follows:

$$

	\mathbf{T}^l(\mathbf{X}^{l-1}):= \mathbf{A}^l \mathbf{X}^{l-1} + \mathbf{b}^l,

$$

where $\mathbf{A}^l \in \mathbb{R}^{n_l \times n_{l-1}}$ is the weight matrix and $\mathbf{b}^l \in \mathbb{R}^{n_l}$ is the bias vector.

The $l$-th hidden layer ($l = 1, \dots, L-1$) applies a component-wise nonlinear activation function $\sigma: \mathbb{R} \to \mathbb{R}$:

$$

	\mathbf{X}^{l} = f^{l}(\mathbf{X}^{l-1}) := \sigma\left( \mathbf{T}^l\left( \mathbf{X}^{l-1}\right) \right).

$$

The final layer (\(l = L\)) produces the output via a purely affine transformation without activation:

$$

	\mathbf{X}^L = \mathbf{T}^L \left( \mathbf{X}^{L-1}\right).

$$

Then the complete $L$-layer DNN is the composition:

$$

	\mathcal{N}\mathcal{N}(\mathbf{x}; \theta) := \mathbf{T}^L \circ f^{L-1} \circ \cdots \circ f^1 (\mathbf{X}^0), \quad \mathbf{X}^0 = \mathbf{x},

$$

where $\mathbf{x} \in \mathbb{R}^{n_0}$ is the input of the neural network, and the set of trainable parameters $\left\lbrace \mathbf{A}^l, \mathbf{b}^l\right\rbrace_{l=1}^L$ is stacked into a parameter vector $\theta$.

The integers $L$ and $W:=\max \left\lbrace n_l, l=1,...,L\right\rbrace$ are called the depth and width of the DNN, respectively.

In PINNs, the hyperbolic tangent function and the Sigmoid function are predominantly adopted owing to their sufficient smoothness.

For this work, we employ the hyperbolic tangent activation $\sigma(x) = \frac{e^x - e^{-x}}{e^x+e^{-x}}$.

Similar to the definition in [^Hu2024Solving], we denote by $\mathcal{H}_\sigma(L,N_\theta,B_\theta)$ the collection of DNN functions of depth $L$, containing $N_\theta$ nonzero trainable parameters, where each parameter is bounded in absolute value by $B_\theta$, i.e.,

$$
\begin{aligned}

	\mathcal{H}\left( L,N_\theta,B_\theta\right) := \left\lbrace \text{L-layer } \mathcal{N}\mathcal{N}(\mathbf{x}; \theta): \left| \theta\right|_{l^0} \le N_\theta, \left| \theta\right|_{l^\infty} \le B_\theta\right\rbrace.

\end{aligned}
$$

Here $\left| \cdot\right|_{l^0}$ and $\left| \cdot\right|_{l^\infty}$ denote the number of nonzero entries and the maximum norm of a vector, respectively.

We also use a simple notation $\mathcal{H}$ to signify {$\mathcal{H}\left( L,N_\theta,B_\theta\right)$}.

### EVT

\label{sec 2.2}

The EVT has demonstrated remarkable success in solving elliptic interface problems, enabling high accuracy approximations of solutions with low regularity across interfaces using a single neural network (see [^Tseng2023Cusp-Capturing], [^Hu2022Discontinuity], [^Ying2024Accurate], [^Bi2025XI-DeepONet]).

We first introduce two important functions: the level set function $\phi: \Omega \times [0,T_{\text{end}}] \to \mathbb{R}$ and the indicator function $\chi: \Omega \times [0,T_{\text{end}}] \to \mathbb{R}$.

The level set functions  characterizing the interfaces satisfy [^Osher2004Level]

$$

	\phi(\mathbf{x}, t) \left\lbrace 
	

$$
\begin{aligned}

		&<0, \quad \mathrm{if} \; \mathbf{x} \in \Omega^-(t), \\
		&=0, \quad \mathrm{if} \; \mathbf{x} \in \Gamma(t), \\
		&>0, \quad \mathrm{if} \; \mathbf{x} \in \Omega^+(t),
	
\end{aligned}
$$

	\right. 

$$

and the indicator function is defined as:

$$

	\chi(\mathbf{x}, t)= \left\lbrace
	

$$
\begin{aligned}

		& -1, \quad &\mathrm{if} \; \mathbf{x} \in \Omega^-(t), \\ 
		& 1, \quad &\mathrm{if} \; \mathbf{x} \in \Omega^+(t).
	
\end{aligned}
$$

	\right. 

$$

It is obvious that the indicator function can be defined by means of the level set function.

Consequently, we define an extension function $\widetilde{u}(\mathbf{x}, t, z)$ on $\widetilde{\Omega}:= \Omega \times [0, T_{\text{end}}] \times \mathbb{R} \in \mathbb{R}^{d+2}$ such that

$$
\label{eq2.5}
	\widetilde{u}(\mathbf{x}, t, z(\mathbf{x}, t)) = u(\mathbf{x}, t), \quad \mathbf{x} \in \Omega, \quad t\in[0,T_{\text{end}}],

$$

where $z$ represents the extension variable.

Depending on the interface jump conditions $h_D$, the extension variable is defined as:

$$

	z\left( \mathbf{x}, t\right)  = \left\lbrace 
	

$$
\begin{aligned}

		&\chi\left( \mathbf{x}, t\right) , \quad &\mathrm{if} \;h_D \neq 0,\\
		&\Phi\left( \mathbf{x}, t\right) , \quad &\mathrm{if} \;h_D = 0,\\
	
\end{aligned}
$$

	\right. 

$$

where $\Phi\left( \mathbf{x}, t\right) = | \phi\left( \mathbf{x}, t\right) |$.  % or $\Phi\left( \mathbf{x}, t\right) = ReLU\left( \phi\left( \mathbf{x}, t\right)\right)$.
As shown in [^Li2025Continuity-Preserved], the extension variable $z=\Phi\left( \mathbf{x}, t\right)$ for the case where $h_D=0$ can slight improve accuracy since the neural network function $u_{\mathcal{N}\mathcal{N}} \in \mathcal{H}$ enforces continuity conditions on the whole domain $\Omega$.

It can be observed that, by the definitions of $\widetilde{u}$ and the extended variables $z(\mathbf{x}, t)$, $\widetilde{u}$ naturally exhibits discontinuities or discontinuities in its derivatives at the interface $\Gamma(t)$. 

### XI-PINN

As discussed in Section~[sec 2.2](#sec 2.2), although the function $u$ (or the derivative of $u$) is discontinuous at the interface, the EVT can define a higher-dimensional continuous function $\widetilde{u} \in \mathbb{R}^{d+2}$.

Therefore, we can construct a DNN $\widetilde{u}_\theta \in \mathcal{H}$ with the input dimension $d+2$ to approximate the solution of Eqs. ([eq1.1](#eq1.1))-([eq1.7](#eq1.7)) through the following discrete optimization problem:

$$
\label{eq2.7}
	\mathop{\arg \min}_{\widetilde{u}_{\theta} \in \mathcal{H}} \mathop{\sum}_{i=1}^{5} l_i(\widetilde{u}_{\theta}),

$$

with

$$
\begin{aligned}

	&

$$
\begin{aligned}

		l_1(\widetilde{u}_{\theta}) 
		=\mathop{\sum}_{j=1}^{N_{\Omega}} &\left|\partial_t \widetilde{u}_{\theta}\left(\mathbf{x}_j, t_j, z\left( \mathbf{x}_j, t_j\right)  \right)-\nabla \cdot \left( \beta \nabla \widetilde{u}_{\theta}\left(\mathbf{x}_j,t_j,z\left(\mathbf{x}_j,t_j\right) \right)\right) \right. \\
		&\qquad\qquad\qquad\left.  - f\left( \mathbf{x}_j, t_j\right) \right|^2, \qquad (\mathbf{x}_j, t_j) \in \Omega \times [0, T_{\text{end}}],
	
\end{aligned}
$$

\label{eq2.8}
	\\
	&l_2(\widetilde{u}_{\theta}) = \mathop{\sum}_{j=1}^{N_{\partial \Omega}} \left| \widetilde{u}_{\theta}\left(\mathbf{x}_j,t_j,z\left(\mathbf{x}_j,t_j\right) \right) - g\left( \mathbf{x}_j,t_j\right) \right|^2, \quad   (\mathbf{x}_j, t_j) \in \partial\Omega \times [0, T_{\text{end}}], \label{eq2.9}
	\\
	&l_3(\widetilde{u}_{\theta}) = \mathop{\sum}_{j=1}^{N_{\Omega_0}} \left| \widetilde{u}_{\theta}\left(\mathbf{x}_j,0,z\left(\mathbf{x}_j, 0\right) \right) - u_0\left(\mathbf{x}_j\right)\right| ^2, \quad \mathbf{x}_j \in \Omega, \label{eq2.10}
	\\
	&

$$
\begin{aligned}
l_4(\widetilde{u}_{\theta}) =& \mathop{\sum}_{j=1}^{N_{\Gamma}} \left| \left[ \beta \nabla \widetilde{u}_{\theta}\left(\mathbf{x}_j,t_j,z\left(\mathbf{x}_j,t\right) \right) \cdot \mathbf{n} \right]_{\Gamma(t)} - h_N\left(\mathbf{x}_j,t_j\right) \right|^2 , \\& \qquad\qquad\qquad\qquad\qquad\qquad\qquad(\mathbf{x}_j, t_j) \in \Gamma(t) \times [0, T_{\text{end}}], \label{eq2.12}
		
\end{aligned}
$$

		\\
	&

$$
\begin{aligned}
l_5(\widetilde{u}_{\theta}) = &\mathop{\sum}_{j=1}^{N_{\Gamma}} \left| \left[ \widetilde{u}_{\theta}\left(\mathbf{x}_j,t_j,z\left(\mathbf{x}_j,t_j\right) \right) \right]_{\Gamma(t)} - h_D\left(\mathbf{x}_j,t_j\right) \right|^2 , \\
		&\qquad\qquad\qquad\qquad\qquad\qquad\qquad  (\mathbf{x}_j, t_j) \in \Gamma(t) \times [0, T_{\text{end}}], \label{eq2.11}
	
\end{aligned}
$$

\end{aligned}
$$

where $N_\Omega$, $N_{\partial \Omega}$, $N_{\Omega_0}$ and $N_\Gamma$ are the numbers of sampling points used in training the neural network $\widetilde{u}_\theta$ within their respective domains.

\begin{remark} \label{remark2}
	For the derivative terms in the discrete loss function items ([eq2.8](#eq2.8)) and ([eq2.12](#eq2.12)), we have the following three equalities based on the chain rule:
	

		
- [(1)] $\partial_t \widetilde{u}_\theta = D_t \widetilde{u}_\theta + D_z \widetilde{u}_\theta \partial_t z$;
		
- [(2)] $\nabla \widetilde{u}_\theta = \nabla_\mathbf{x} \widetilde{u}_\theta + D_z \widetilde{u}_\theta \nabla z$;
		
- [(3)] $\nabla \cdot \nabla \widetilde{u}_\theta = \Delta_\mathbf{x} \widetilde{u}_\theta + 2 \nabla z \cdot \nabla_\mathbf{x}\left( D_z \widetilde{u}_\theta\right) + |\nabla z|^2 D_z^2 \widetilde{u}_\theta + D_z \widetilde{u}_\theta \Delta z$.
	

	Here, $D_t$ and $D_z$ denote the partial derivative of $\widetilde{u}_\theta$ with respect to the components in $t$ and $z$, respectively. $\nabla_\mathbf{x} \widetilde{u}_\theta \in \mathbb{R}^{d}$ represents a vector with partial derivatives of $\widetilde{u}_\theta$ with respect to the components in $\mathbf{x}$.

And $\Delta_\mathbf{x}$ is the Laplace operator with respect to the components in $\mathbf{x}$ of $\widetilde{u}_\theta$.
\end{remark}

\begin{remark}
	When the interface jump condition satisfies $h_D=0$, the extension variable $z$ can be set to $z(\mathbf{x},t) = \Phi(\mathbf{x},t)$.

Consequently, the $l_4$ term can be omitted from the optimization formulation~([eq2.7](#eq2.7)).
\end{remark}

### Solving the level set functions

\label{sec2.4}

The level set function plays a crucial role in the EVT, as it determines the position of points within subdomains and thereby defines the {indicator} function.

Moreover, it can construct the extension variable directly when $h_D=0$.

Consequently, obtaining a time-evolving level set function is essential for implementing the proposed methodology.

At the initial time $t=0$, suppose the level set function is known and denoted by $\phi_0$.

When the velocity field $\mathcal{V}$ is extendable to the entire computational domain $\Omega$ -- such that for any $\mathbf{x} \in \Omega$ evolves according to the ODE dynamics ([eq1.7](#eq1.7)) -- the level set function $\phi(\mathbf{x}, t)$ can be obtained by solving the following advection equation:

$$
\begin{aligned}

	\dfrac{\partial \phi(\mathbf{x}, t)}{\partial t} + \mathcal{V} \cdot \nabla \phi(\mathbf{x}, t) = 0&, \quad \mathbf{x} \in \Omega, \; t \in [0, T_{\text{end}}],\\
	\phi(\mathbf{x}, 0) = \phi_0(\mathbf{x})&,\quad \mathbf{x} \in \Omega.

\end{aligned}
$$

The governing equation may be solved using the method of characteristics.

For any point $\mathbf{x}_0 \in \Omega$ at $t=0$, provided that its position at $t=T$ admits an explicit representation $\mathbf{x}_T = \mathbf{x}_0 + \mathcal{F}(\mathbf{x}_0, T)$, where $\mathcal{F}$ is the displacement function, i.e., $\mathcal{F}(\mathbf{x}_0, T) = \Delta \mathbf{x} = \mathbf{x}_T - \mathbf{x}_0$.

Then, the level set function is constructed as:

$$

	\phi(\mathbf{x}, T) = \phi_0\left( \mathbf{x}_T - \mathcal{F}(\mathbf{x}_0, T)\right). 

$$

Alternatively, we can bypass solving the evolution equation altogether by employing neural networks to directly construct an appropriate level set function.

Given $t_0 \in [0,T_{\text{end}}]$, we define the flow map as follows:

$$
\label{eq2.16}
	\mathbf{X}: \Gamma(t_0) \times [t_0, T_{\text{end}}] \to \Gamma(t), \quad \mathbf{x}_t = \mathbf{X}(\mathbf{x}, t; t_0),\, t\in[t_0,T_{\text{end}}],

$$

where $\mathbf{x} \in \Gamma(t_0)$ and $\mathbf{x}_t \in \Gamma(t)$.

The flow map can be obtained using the solution of the initial-value problem~([eq1.7](#eq1.7))

$$

	\dfrac{\mathrm{d}}{\mathrm{d}t} \mathbf{X}(\mathbf{x},t; t_0) = \mathcal{V}\left( \mathbf{X}(\mathbf{x},t; t_0), t\right) , \quad \mathbf{X}(\mathbf{x},t_0; t_0) = \mathbf{x}, \quad \mathbf{x} \in \Gamma(t_0).

$$

The inverse of $\mathbf{X}(\mathbf{x},t; t_0)$ is denoted by $\widetilde{\mathbf{X}}(\mathbf{x}, t; t_0): \Gamma(t) \times [t_0 ,T_{\text{end}}]  \to \Gamma(t_0).$
We extend the mapping $\widetilde{\mathbf{X}}$ and define the extension mapping as

$$

	\widehat{\mathbf{X}} : \Omega \times [t_0,T_{\text{end}}] \to \mathbb{R}^d,

$$

which should satisfy the following properties:

$$
\label{eq2.19}
	\left. \widehat{\mathbf{X}}(\mathbf{x},t;t_0)\right|_{\mathbf{x} \in \Gamma(t)} =  \widetilde{\mathbf{X}}(\mathbf{x},t;t_0) \quad \text{and} \quad
	\widehat{\mathbf{X}}(\mathbf{x},t;t_0) \text{\;is injective}.

$$

Therefore, the level set function at time $t$ can be defined as:

$$
\label{eq2.20}
	\phi\left( \mathbf{x}, t\right) = \phi_0 \left( \widehat{\mathbf{X}}(\mathbf{x},t;0)\right).

$$

The derivative of this function can be computed via the chain rule, as follows:

$$

	\dfrac{\mathrm{d}}{\mathrm{d}x}\phi = \dfrac{\mathrm{d}}{\mathrm{d}\widehat{\mathbf{X}}} \phi_0\left( \widehat{\mathbf{X}}\right) \dfrac{\mathrm{d}}{\mathrm{d}x} \widehat{\mathbf{X}}.

$$

Next, we employ neural networks to approximate the mapping $\widehat{\mathbf{X}}$.

Denote by \(\mathcal{F}_\theta\) the neural network function, and define the approximating function for \(\widehat{\mathbf{X}}\) as:% Denote the neural network function by $\mathcal{F}_\theta$, and the approximating function for $\widehat{\mathbf{X}}$ is specifically defined as:

$$
\label{eq2.21}
	\widehat{\mathbf{X}}_\theta\left( \mathbf{x}, t; 0\right)  =  \mathcal{F}_\theta\left( \mathbf{x}, t\right) + \mathbf{x}. 

$$

Let $\mathbf{X}_{RK}$ be the approximation of $\mathbf{X}$ computed using the 4th-order explicit Runge-Kutta method [^Verner1978Explicit].

Thus, we determine the approximate displacement function $\mathcal{F}_\theta$ and the resulting flow map $\widehat{\mathbf{X}}$ through optimization of the following loss function:

$$

	{l_{lf}} = \dfrac{1}{N_{\Omega}} \mathop{\sum}_{i=1}^{N_{\Omega}}\left| \mathcal{F}_\theta\left( \mathbf{x}_{i, \Omega}, 0\right) \right|^2 + \dfrac{1}{N_t N_\Gamma}  \mathop{\sum}_{j=1}^{N_t} \mathop{\sum}_{i=1}^{N_\Gamma} \left| \widehat{\mathbf{X}}_\theta\left( \mathbf{X}_{RK}\left(\mathbf{x}_{i,\Gamma}, t_j; 0\right),t_j;0\right) - \mathbf{x}_{i,\Gamma}\right|^2,

$$

where $\left\lbrace \mathbf{x}_{i,\Omega} \right\rbrace_{i=1}^{N_{\Omega}}$ and $\left\lbrace \mathbf{x}_{i,\Gamma} \right\rbrace_{i=1}^{N_{\Gamma}}$ represent the training points sampled from the domain $\Omega$ and the initial interface $\Gamma(0)$ respectively, and the set $\left\lbrace t_j\right\rbrace_{j=1}^{N_{t}}$ forms a sufficiently fine temporal discretization of the interval $[0,T_{\text{end}}]$.  Notably, we impose the condition that $\widehat{\mathbf{X}}_\theta$ must satisfy the properties specified in equation ([eq2.19](#eq2.19)). 
If the approximate flow map \(\widehat{\mathbf{X}}_\theta\) is non-injective, the level set function  
{\(
\phi_\theta(\mathbf{x}, t) := \phi_0(\widehat{\mathbf{X}}_\theta(\mathbf{x}, t; 0))  
\)}
may become invalid.

This occurs because \(\widehat{\mathbf{X}}_\theta\) could erroneously map points \(\mathbf{x}_t \in \Omega^+(t)\) into \(\Omega^-(0)\), analogous to the mesh-entanglement constraints in finite element methods.

To mitigate this issue, we adopt a temporal partitioning strategy, dividing \([0, T_{\text{end}}]\) into smaller sub-intervals.  

$$
[0, T_{\text{end}}] = \mathop{\bigcup}_{k=1}^{K}[T_{k-1}, T_k], \quad T_0 = 0, T_
{K} = T_{\text{end}},
$$ 
where the set $\left\lbrace T_k\right\rbrace_{k=1}^{K} \subseteq \left\lbrace t_j\right\rbrace_{j=1}^{N_t} $ can be adaptively chosen by monitoring the Jacobian determinant of  $\widehat{\mathbf{X}}$ {(or \(\widehat{\mathbf{X}}_\theta\)).}

Therefore, we decompose $\widehat{\mathbf{X}}\left(\mathbf{x}, t; 0\right), t \in [T_{k-1}, T_{k}]$ into a composition of multiple sub-mappings

$$
\label{eq2.24}
	\widehat{\mathbf{X}}\left(\mathbf{x}, t;0\right) = \widehat{\mathbf{X}}\left(\mathbf{x}, t;T_{k-1}\right) \circ \widehat{\mathbf{X}}\left(\mathbf{x}, T_{k-1}; T_{k-2}\right) \circ \cdots \circ \widehat{\mathbf{X}}\left(\mathbf{x}, T_{1}; 0\right).

$$

For each short time sub-interval \([T_{k-1}, T_k]\), we compute a high-precision approximate sub-mapping \(\widehat{\mathbf{X}}_{\theta, k}\) using a {small} neural network.

Note that employing an excessively large network may lead to overfitting, which in turn could introduce undesirable map-entanglement {phenomena}. 

Moreover, the displacement of interface points, $\Delta \mathbf{x}$, remains moderate over short time intervals, effectively preventing mesh entanglement,
particularly for problems involving large  interface deformation.

Correspondingly, we employ a series of sub-networks \(\widehat{\mathbf{X}}_{\theta, k}\) to approximate \(\widehat{\mathbf{X}}(\mathbf{x}, t; T_{k-1})\) for \(t \in [T_{k-1}, T_k]\), with the loss function adjusted as follows:

$$
\label{eq2.25}
	

$$
\begin{aligned}

		{l_{lf, k}} = &\dfrac{1}{N_{\Omega}} \mathop{\sum}_{i=1}^{N_{\Omega}}\left| \mathcal{F}_\theta\left(\mathbf{x}_{i, \Omega}, T_{k-1}\right) \right|^2 + \dfrac{1}{N_{t,k}

N_{\Gamma}}  \mathop{\sum}_{j=1}^{N_{t,k}} \mathop{\sum}_{i=1}^{N_{\Gamma}}\\
		& \qquad\left| \widehat{\mathbf{X}}_\theta\left( \mathbf{X}_{RK}\left(\mathbf{x}_{i,\Gamma}, t_j; 0\right)  ,t_j; T_{k-1}\right) - \mathbf{X}_{RK}\left(\mathbf{x}_{i,\Gamma}, T_{k-1}; 0\right)\right|^2,
	
\end{aligned}
$$

$$

where \(\left\lbrace t_j\right\rbrace_{j=1}^{N_{t,k}}\) denotes a discrete time series within \([T_{k-1}, T_k]\), sampled from the global set \(\left\lbrace t_j\right\rbrace_{j=1}^{N_t}\).

The injectivity of the composite mapping is guaranteed when each nonlinear sub-mapping $\widehat{\mathbf{X}}_{\theta, k}$ (for $k=1,\dots,K$) is injective.

Leveraging this property, we develop an adaptive time-stepping algorithm that simultaneously handles temporal partitioning and learns the corresponding sub-mappings.

To detect potential mesh entanglement (indicated by negative Jacobian determinants), we locally linearize the mapping via first-order Taylor expansion.

This approach employs uniform sampling over the domain $\Omega$ using a sufficiently small grid spacing $h$, which ensures the validity of the local linear approximation:

$$
\label{eq2.26}
	\widehat{\mathbf{X}}_{\theta, k}\left(\mathbf{x},t;T_{k-1}\right)  \approx \widehat{\mathbf{X}}_{\theta, k}\left(\mathbf{x_0},t;T_{k-1}\right) + \mathbf{J}\left( \mathbf{x}_0\right)\left( \mathbf{x} - \mathbf{x}_0\right), \quad \forall \mathbf{x} \in B_\varepsilon\left( \mathbf{x}_0\right) , 

$$

where $\mathbf{J(\mathbf{x}_0)}:=\nabla \widehat{\mathbf{X}}_{\theta,k}$ denotes the Jacobian matrix of the mapping and $B_\varepsilon\subset\Omega$ represents an $\varepsilon$-neighborhood of the sample point $\mathbf{x}_0 \in \Omega$. 
A negative Jacobian determinant ($\det(\mathbf{J}) <0$) indicates that the mapping $\widehat{\mathbf{X}}_{\theta, k}$ fails to preserve orientation, violating the injectivity requirement.

To enforce this condition numerically, we impose a positive-definite constraint by maintaining $\det(\mathbf{J}) >\delta$, where $\delta>0$ is a prescribed threshold value. %Namely,

A Jacobian determinant falling below the threshold $\delta$ ($\det(\nabla \widehat{\mathbf{X}}_{\theta,k}) < \delta$) serves as a topological early warning signal.  At this critical point, we initialize a new neural network $\widehat{\mathbf{X}}_{\theta,k+1}$ to learn the deformation mapping for the subsequent interval $[T_k, T_{k+1}]$. 

We therefore develop an adaptive time-stepping algorithm that generates: (1) a sequence of sub-mappings $\lbrace \widehat{\mathbf{X}}_{\theta, k}\rbrace_{k=1}^{K+1}$ and (2) corresponding time sub-intervals $\left\lbrace T_{k-1}, T_k\right\rbrace_{k=1}^{K+1}$, operating on a discrete temporal sequence $\left\lbrace t_j\right\rbrace_{j=1}^{N_t}$ spanning $[0,T_{\text{end}}]$.

Each sub-network $\widehat{\mathbf{X}}_{\theta, k}$ is trained on a temporal subsequence  $\left\lbrace t_j\right\rbrace_{j=1}^{N_{t,k}} \subseteq \left\lbrace t_j\right\rbrace_{j=1}^{N_t}$ through the loss function ([eq2.25](#eq2.25)).

The subsequences are constructed such that the initial time $t_0$ of $\left\lbrace t_j\right\rbrace_{j=1}^{N_{t,k}}$ coincides with the terminal time $t_{N_{t,k-1}}$ of the preceding subsequence $\left\lbrace t_j\right\rbrace_{j=1}^{N_{t,k-1}}$, while the terminal time $t_{N_{t,k}}$ is adaptively determined by monitoring the minimum Jacobian determinant of $\widehat{\mathbf{X}}_{\theta, k}$ across all spatial grid points.

When the Jacobian determinant condition $\det(\mathbf{J}) < \delta$ is triggered, we initialize a new neural network $\widehat{\mathbf{X}}_{\theta, k+1}$ to learn the subsequent deformation mapping.

This adaptive network growth strategy ensures global injectivity preservation across the entire temporal domain.

The complete time-adaptive training procedure is implemented as described in Algorithm~[algorithm 1](#algorithm 1).

\begin{algorithm}
\caption{Time-Stepping Neural Network Training for ([eq2.24](#eq2.24))}
	\label{algorithm 1}

	\SetAlgoLined
	\DontPrintSemicolon
	\KwIn{Initial time $t_0=0$, threshold $\delta > 0$; Time sequence $\{t_i\}_{i=0}^{N_t}$, sample points $\{\mathbf{x}_{i,\Omega}\}_{i=1}^{N_{\Omega}}$ and $\left\lbrace \mathbf{x}_{i,\Gamma} \right\rbrace_{i=1}^{N_{\Gamma}}$}
	\KwOut{Trained neural networks $\widehat{\mathbf{X}}_{\theta,k}$ for each adaptive interval $[T_{k-1}, T_k]$, $k=1,\cdots,K$}
	
	Set network index $k \leftarrow 1$\;
	
	Set current time index $i \leftarrow 0$\;
	
	Initialize neural network $\widehat{\mathbf{X}}_{\theta,k}$\;
	
	Set current start time $t_s \leftarrow t_i$, $T_0 \leftarrow t_i$\;
	
	\While{$i \le N_t$}{
		$i \leftarrow i + 1$\;
		
		Train $\widehat{\mathbf{X}}_{\theta,k}$ on time steps $\{t_s, \dots, t_i\}$ using loss function ([eq2.25](#eq2.25))\;
		
		Compute Jacobian determinant $|\mathbf{J}|$ of $\widehat{\mathbf{X}}_{\theta,k}$\;
		
		\eIf{$|\mathbf{J}| > \delta$ **and** $i \le N_t$}{
			Continue to next time step (no action)\;
		}{
			\If{$|\mathbf{J}| \leq \delta$}{
				**Terminate training** of $\widehat{\mathbf{X}}_{\theta,k}$ for current interval $[t_s, t_i]$\;
				
				Save the sub-network $\widehat{\mathbf{X}}_{\theta,k}$\;
				
				$T_k \leftarrow t_i$\;
			}
			$k \leftarrow k + 1$ \tcp*{Move to next network}
			Initialize new network $\widehat{\mathbf{X}}_{\theta,k}$\;
			
			$t_s \leftarrow t_i$ \tcp*{Reset start time to current endpoint}
		}
	}
	$K \leftarrow k, T_K \leftarrow T_{\text{end}}$\;

\end{algorithm}

\begin{remark}In Algorithm~[algorithm 1](#algorithm 1), we assume that the sampling density satisfies the local linearization validity condition.

Note $|\mathbf{J}|$ represents the minimum Jacobian determinant calculated over all uniformly sampled points in $\Omega$.
\end{remark}

\begin{remark}
	To ensure mapping accuracy at interface points, we solely use these points as the training data for the neural network.

Furthermore, through deliberate architectural design ([eq2.21](#eq2.21)), the network $\mathcal{F}_\theta$ is constructed to learn coordinate transformations $\Delta \mathbf{x}$. %Hence at initial time $t=t_0$, the mapping $\widehat{\mathbf{X}}_\theta$ reduces to an identity mapping over $\Omega$. 
	Combined with our time-stepping training strategy, this approach effectively alleviates mesh entanglement issues, even when exclusively using interface points as the training set.
\end{remark}

## 3·Error Analysis

\label{sec3}

In this section, we present the error analysis of the XI-PINN framework introduced in Section~[sec 2](#sec 2).

We assume that the domain $\Omega \subseteq \mathbb{R}^d$ is bounded with a Lipschitz boundary $\partial \Omega$, and $\Gamma(t)$ is a closed smooth curve (or surface) within $\Omega$.

To streamline the analysis, we assume that $h_D = 0$.

This implies that the neural network functions $\widetilde{u}_{\mathcal{N}\mathcal{N}} \in \mathcal{H}$ are continuous across the interface, enforced through the level set function.  

The space-time domain is denoted by $Q:=\Omega \times [0,T_{\text{end}}]$ and $\Sigma:= \partial \Omega \times [0,T_{\text{end}}]$.

The extension space-time domain is defined as $$\widetilde{Q}:= Q \times [\mathop{\min}_{(\mathbf{x}, t)\in Q} z(\mathbf{x}, t), \mathop{\max}_{(\mathbf{x}, t)\in Q} z(\mathbf{x}, t)].$$ For each subdomain $\omega \subseteq \Omega$, we define $H^k(\omega)$ as the standard Sobolev space with the norm $\|\cdot\|_{H^k(\omega)}$, {and define the time-dependent Bochner space $H^l\left( 0, T; H^k\left( \omega\right) \right) $ consist of all measurable functions $u: [0,T] \to H^k\left( \omega\right) $ with the norm $\|\cdot\|_{H^l\left( 0, T; H^k\left( \omega\right) \right)}$}.

Here, we denote by $H^k(\Omega^+(t) \cup \Omega^-(t)):=H^k(\Omega^+(t)) \cap H^k(\Omega^-(t))$ the piecewise $H^k$ space on $\Omega^+(t) \cup \Omega^-(t)$, which is equipped with the norm
$$
\|v\|_{H^k(\Omega^+(t) \cup \Omega^-(t))}:= \|v\|_{H^k(\Omega^+(t))} + \|v\|_{H^k(\Omega^-(t))} \quad \forall v \in H^k(\Omega^+(t) \cup \Omega^-(t)).
$$
We also employ the Besov space $B_{2,1}^{1/2}(\omega):=\left( L^2(\omega), H^1(\omega) \right)_{1/2, 1}$ defined by interpolation (see [^Tartar2007Introduction]).

The piecewise Besov space {$B_{2,1}^{1/2}(\Omega^+(t) \cup \Omega^-(t))$} and its norm $\|v\|_{B_{2,1}^{1/2}(\Omega^+(t) \cup \Omega^-(t))}$ are defined analogously.

Furthermore, we have the embedding $H^{1}(\omega) \subseteq B_{2,1}^{1/2}(\omega)$.

Let $r$ and $s$ be two non-negative real numbers, we define the space $H^{r,s}(Q) := L^2(0,T; H^{r}(\Omega)) \cap H^s(0, T; L^2(\Omega))$ with the norm $\|\cdot\|_{H^{r,s}}$, which is a Hilbert space (see [^Lions1972Non-Homogeneous]).

For a function $v(t)$, we denote its time derivative as $v' := \frac{\mathrm{d}}{\mathrm{d} t} v$, and let $v_0:=v(0)$ represent its initial value. {For simplicity, we use the shorthand notation $A \lesssim B$ for the inequality $A \le CB$, where $C>0$ is a generic constant independent of $N_\Omega$, $N_{\partial \Omega}$, $N_{\Omega_0}$, and $N_\Gamma$, the numbers of training points specified in loss functions ([eq2.8](#eq2.8))-([eq2.12](#eq2.12)).}

### Error analysis of the XI-PINN method

We first establish an energy estimate for the solution $u$ to Eqs. ([eq1.1](#eq1.1))-([eq1.7](#eq1.7)).

\begin{lemma}\label{lemma1}

Assume $h_N \in L^2\left( 0, T_{\text{end}}; L^2(\Gamma(t))\right)$,  $u_0 \in H^1(\Omega)$, $g \in H^{2, 1}(\Sigma)$, with $u_0$ and $g$ satisfying the compatibility conditions.

Then there exists a unique solution $u \in L^2\left( 0, T; H^1(\Omega)\right)$ to ([eq1.1](#eq1.1))-([eq1.7](#eq1.7)) satisfying the estimate:
	

$$

		

$$
\begin{aligned}

			\|u\|_{L^2\left( 0, T; H^1(\Omega)\right) }^2 \lesssim& \|f\|_{L^2(0,T_{\text{end}};L^2(\Omega))}^2 + \|u_0\|_{H^1(\Omega)}^2 + \|h_N\|_{L^2\left( 0, T_{\text{end}}; L^2(\Gamma(t))\right) }^2 \\
			&+ \|g\|_{L^2\left( 0, T_{\text{end}}, H^2(\partial \Omega)\right) }^2 + \|g'\|_{L^2\left( 0, T_{\text{end}}, L^2(\partial \Omega)\right)}^2.
		
\end{aligned}
$$

	
$$

\end{lemma}

\begin{proof} %(Theorem 2.1 and Remark 4.1 in Chapter 4 of [^Lions1972Non-Homogeneous])
	The first step involves homogenizing the boundary conditions.

Invoking the Trace Theorem, there exists $w \in H^{2,1}(Q)$ satisfying
	

$$

		\left. w(\mathbf{x}, t)\right|_{\Sigma} = g(\mathbf{x},t), \quad
		\left. w(\mathbf{x}, 0)\right|_{\Omega} = u_0(\mathbf{x}),
	
$$

	and
	

$$
\label{eq3.2}
		\|w\|_{H^{2,1}(Q)} \lesssim \|g\|_{H^{2, 1}(\Sigma)} + \|u_0\|_{H^1(\Omega)}.
	
$$

	Setting $v=u-w$, we reformulate problem ([eq1.1](#eq1.1))-([eq1.5](#eq1.5)) as
	

$$
\label{eq3.3}
	\left\{	

$$
\begin{aligned}

			\partial_t v - \nabla \cdot \left( \beta \nabla v\right)  &= \widetilde{f} \quad \mathrm{in}\; \Omega = \Omega^+(t) \cup \Omega^-(t), \quad  t \in [0, T_{\text{end}}], \\
			v\left( \cdot, t\right) &= 0 \quad \mathrm{on}\; \partial \Omega, \quad t \in [0, T_{\text{end}}],\\
			v\left( \cdot, 0\right) &= 0 \quad \mathrm{in}\; \Omega = \Omega^+(0) \cup \Omega^-(0),
		
\end{aligned}
$$

\right.
	
$$

	with the interface condition
	

$$
\label{eq3.7}
		

$$
\begin{aligned}

		[u]_{\Gamma(t)} &= 0, \quad t\in [0,T_{\text{end}}],\\
		[\beta \nabla v \cdot \mathbf{n}]_{\Gamma(t)}& = \widetilde{h}_N, \quad t\in [0,T_{\text{end}}], 			
		
\end{aligned}
$$

	
$$

	where $\widetilde{f} := f - w' + \nabla \cdot (\beta \nabla w) \in L^2(0, T_{\text{end}}; L^2(\Omega))$, and $\widetilde{h}_N:=h_N - [\beta \nabla w \cdot \mathbf{n}]_{\Gamma(t)} \in L^2(0, T_{\text{end}}; L^2(\Gamma(t)))$. %, which parallels standard existence proofs for parabolic equations
	{The existence of a weak solution \( v \in L^2(0,T_{\text{end}}; H_0^1(\Omega)) \) with \( v' \in L^2(0,T_{\text{end}}; H^{-1}(\Omega)) \) to ([eq3.3](#eq3.3))-([eq3.7](#eq3.7)) follows by the Galerkin approximation method (cf. \cite[Section 7.1]{evans2022partial}). 
	Then, we have
	

$$
\label{eq3.8}
		

$$
\begin{aligned}

			\int_\Omega \partial_t v(t) v(t) + \int_\Omega \beta \nabla v(t) \cdot \nabla v(t) =& \int_\Omega f(t) v(t) - \int_\Omega w'(t) v(t)\\
			&- \int_\Omega \beta \nabla w(t) \cdot \nabla v(t) + \int_{\Gamma(t)} h_N(t) v(t).
		
\end{aligned}
$$

	
$$

It is obvious that
	

$$
\label{eq3.9}
		\text{LHS of ([eq3.8](#eq3.8))} = \dfrac{1}{2} \dfrac{\mathrm{d}}{\mathrm{d} t} \|v(t)\|_{L^2(\Omega)}^2 + \beta \|\nabla v(t)\|_{L^2(\Omega)}^2.
	
$$

Applying the Cauchy-Schwarz, Young's inequalities and Trace Theorem, we have
	

$$
\label{eq3.10}
		
\begin{split}
			\text{RHS of ([eq3.8](#eq3.8))} \le& \dfrac{1}{2}\|f(t)\|_{L^2(\Omega)}^2 + \| w'(t)\|_{L^2(\Omega)}^2 + \|v(t)\|_{L^2(\Omega)}^2 + \dfrac{\beta\varepsilon}{2} \|\nabla v(t)\|_{L^2(\Omega)}^2 \\
			& + \dfrac{\beta}{2\varepsilon} \|\nabla w(t)\|_{L^2(\Omega)}^2 + \dfrac{1}{2 \varepsilon}\|h_N(t)\|_{\Gamma(t)}^2 + \dfrac{C\varepsilon }{2} \|v(t)\|_{H^1(\Omega)}^2.
		\end{split}

	
$$

Then, assuming $\varepsilon$ is small enough, it follows from ([eq3.9](#eq3.9)) and ([eq3.10](#eq3.10)) that
	

$$
\label{eq3.11}
		

$$
\begin{aligned}

			\dfrac{\mathrm{d}}{\mathrm{d} t} \|v(t)\|_{L^2(\Omega)}^2 + \|v(t)\|_{H^1(\Omega)}^2 \lesssim& \|f(t)\|_{L^2(\Omega)}^2 + \|h_N\|_{L^2(\Gamma(t))}^2 + \|v(t)\|_{L^2(\Omega)}^2 \\
			&\|w(t)\|_{H^1(\Omega)}^2 + \left\|w'(t)\right\|_{L^2(\Omega)}^2. 
		
\end{aligned}
$$

	
$$

	Consequently, applying the differential form of Gronwall's inequality and ([eq3.2](#eq3.2)) produces the following estimate
	

$$

		

$$
\begin{aligned}

			\mathop{\max}_{t \in [0,T_{\text{end}}]} \|v(t)\|_{L^2(\Omega)}^2
			\lesssim& \|f\|_{L^2(0,T_{\text{end}}; L^2(\Omega))}^2 +  \|h_N\|_{L^2(0, T_{\text{end}}; L^2(\Gamma(t)))}^2 + \|w\|_{H^{1,1}(Q)}^2\\
			\lesssim& \|f\|_{L^2(0,T_{\text{end}}; L^2(\Omega))}^2 +  \|h_N\|_{L^2(0, T_{\text{end}}; L^2(\Gamma(t)))}^2\\
			&+ \|g\|_{H^{2,1}(\Sigma)}^2 + \|u_0\|_{H^1(\Omega)}^2.
		
\end{aligned}
$$

	
$$

	Returning to ([eq3.11](#eq3.11)), integration on $\left[ 0, T_{\text{end}}\right]$ yields
	

$$

		

$$
\begin{aligned}

			&\mathop{\max}_{t \in [0,T_{\text{end}}]} \|v(t)\|_{L^2(\Omega)}^2 + \|v(t)\|_{L^2(0,T;H^1(\Omega))}^2 \\
			&\qquad \lesssim  \|f\|_{L^2(0,T; L^2(\Omega))}^2 +  \|h_N\|_{L^2(0, T_{\text{end}}; L^2(\Gamma(t)))}^2 + \|u_0\|_{H^1(\Omega)}^2 + \|g\|_{H^{2,1}(\Sigma)}^2.
		
\end{aligned}
$$

	
$$

	Finally, applying the triangle inequality, we obtain:
	

$$

		

$$
\begin{aligned}

			\|u\|_{L^2(0, T_{\text{end}}; H^1(\Omega))}^2 
			&\lesssim \|v\|_{L^2(0, T_{\text{end}}; H^1(\Omega))}^2  + \|w\|_{L^2(0, T; H^1(\Omega))}^2 \\
			&\lesssim \|v\|_{L^2(0, T_{\text{end}}; H^1(\Omega))}^2 +  \|g\|_{H^{2,1}(\Sigma)}^2 + \|u_0\|_{H^1(\Omega)},
		
\end{aligned}
$$

	
$$

	which combines the definition of norm $\|g\|_{H^{2,1}(\Sigma)}$  immediately yields the result.}
\end{proof}

For subsequent error analysis, we define the continuous loss function as:

$$
\label{eq3.12}
	{\mathcal{L}}(\widetilde{u}_{\theta}) = \mathop{\sum}_{i=1}^4 {\mathcal{L}_i}(\widetilde{u}_{\theta}), 

$$

with

$$
\begin{aligned}

	&{\mathcal{L}_1}(\widetilde{u}_{\theta}) = \int_{0}^{T_{\text{end}}}\int_{\Omega} \left| \partial_t \widetilde{u}_{\theta}(\mathbf{x}, t, z(\mathbf{x}, t)) - \nabla \cdot (\beta \nabla\widetilde{u}_{\theta}(\mathbf{x}, t, z(\mathbf{x}, t))) - f(\mathbf{x}, t) \right|^2 \mathrm{d}\mathbf{x} \mathrm{d}t, 
	\\
	&

$$
\begin{aligned}

		{\mathcal{L}_2}(\widetilde{u}_{\theta}) = &\int_{0}^{T_{\text{end}}}\int_{\partial \Omega} { \mathop{\sum}_{|\alpha|=0}^2 }\left| \partial^\alpha_\mathbf{x} \left( \widetilde{u}_{\theta}(\mathbf{x}, t, z(\mathbf{x}, t)) - g(\mathbf{x}, t)\right)  \right|^2 \mathrm{d}\mathbf{x} \mathrm{d}t\\
		&+\int_{0}^{T_{\text{end}}}\int_{\partial \Omega} \left| \partial_t \widetilde{u}_{\theta}(\mathbf{x}, t, z(\mathbf{x}, t)) - \partial_t g(\mathbf{x}, t) \right|^2 \mathrm{d}\mathbf{x} \mathrm{d}t 
	
\end{aligned}
$$

	\\
	&{\mathcal{L}_3}(\widetilde{u}_{\theta}) = \int_{\Omega} {\mathop{\sum}_{|\alpha| = 0}^1 }\left| \partial^\alpha_\mathbf{x} \left( \widetilde{u}_{\theta}(\mathbf{x}, 0, z(\mathbf{x}, 0)) - u_0(\mathbf{x}) \right) \right|^2 \mathrm{d}\mathbf{x} \mathrm{d}t,
	\\
	&{\mathcal{L}_4}(\widetilde{u}_{\theta}) = \int_{0}^{T_{\text{end}}}\int_{\Gamma(t)} \left| [\beta \nabla \widetilde{u}_{\theta}(\mathbf{x}, t, z(\mathbf{x}, t)) \cdot \mathbf{n}]_{\Gamma(t)} - h_N(\mathbf{x}, t)\right|^2 \mathrm{d}\mathbf{x} \mathrm{d}t,

\end{aligned}
$$

where {$\alpha = (\alpha_1, \alpha_2, \dots, \alpha_d) \in \left( \mathbb{N}\cup \left\lbrace 0\right\rbrace \right) ^d$} is a multi-index for spatial coordinates with $|\alpha| = \sum_{k=1}^d \alpha_k$, and $\partial_{\mathbf{x}}^\alpha = \dfrac{\partial^{|\alpha|}}{\partial x_1^{\alpha_1} \cdots \partial x_d^{\alpha_d}}$.

Further, the corresponding discrete form of loss function \eqref{eq3.12} is defined by

$$
\label{eq3.17}
	 {\mathcal{L}}^d(\widetilde{u}_{\theta}) = \mathop{\sum}_{i=1}^4 \widehat{l}_i(\widetilde{u}_{\theta}),

$$

with

$$
\begin{aligned}

	&

$$
\begin{aligned}

		\widehat{l}_1(\widetilde{u}_{\theta}) 
		=\mathop{\sum}_{j=1}^{N_{\Omega}} &\left|\partial_t \widetilde{u}_{\theta}\left(\mathbf{x}_j, t_j, z\left( \mathbf{x}_j, t_j\right)  \right) \right. \\
		&\left. -\nabla \cdot \left( \beta \nabla \widetilde{u}_{\theta}\left(\mathbf{x}_j,t,z\left(\mathbf{x}_j,t\right) \right)\right) - f\left( \mathbf{x}_j, t_j\right) \right|^2, 
	
\end{aligned}
$$

	\quad (\mathbf{x}_j, t_j) \in \Omega \times [0, T_{\text{end}}],
	\\
	&

$$
\begin{aligned}

		\widehat{l}_2(\widetilde{u}_{\theta}) 
		&= \mathop{\sum}_{j=1}^{N_{\partial \Omega}} { \mathop{\sum}_{|\alpha|=0}^2 }\left| \partial^\alpha_\mathbf{x} \left( \widetilde{u}_{\theta}((\mathbf{x}_j, t_j), z(\mathbf{x}_j, t_j)) - g(\mathbf{x}, t)\right)  \right|^2 \\
		&\qquad+ \mathop{\sum}_{j=1}^{N_{\partial \Omega}} \left| \left( \partial_t \widetilde{u}_{\theta}((\mathbf{x}_j, t_j), z(\mathbf{x}_j, t_j)) -  \partial_t g(\mathbf{x}, t)\right)  \right|^2 ,
	
\end{aligned}
$$

	\quad   (\mathbf{x}_j, t_j) \in \partial\Omega \times [0, T_{\text{end}}],
	\\
	&\widehat{l}_3(\widetilde{u}_{\theta}) = \mathop{\sum}_{j=1}^{N_{\Omega_0}} { \mathop{\sum}_{|\alpha| = 0}^1 } \left| \partial^\alpha_\mathbf{x} \widetilde{u}_{\theta}\left(\mathbf{x}_j,0,z\left(\mathbf{x}_j,0\right) \right) - u_0\left(\mathbf{x}_j\right)\right| ^2, \quad \mathbf{x}_j \in \Omega,
	\\
	&\widehat{l}_4(\widetilde{u}_{\theta}) = \mathop{\sum}_{j=1}^{N_{\Gamma}} \left| \left[ \beta \nabla \widetilde{u}_{\theta}\left(\mathbf{x}_j,t,z\left(\mathbf{x}_j,t\right) \right) \cdot \mathbf{n} \right]_{\Gamma(t)} - h_N\left(\mathbf{x}_j,t\right) \right|^2 , \quad  (\mathbf{x}_j, t_j) \in \Gamma(t) \times [0, T_{\text{end}}].

\end{aligned}
$$

Compared to ([eq2.7](#eq2.7))-([eq2.12](#eq2.12)), the term \( l_4 \) has been eliminated by exploiting the level-set function's continuity, while derivative information has been incorporated into both the initial condition ([eq2.10](#eq2.10)) and boundary term ([eq2.9](#eq2.9)) inspired by the energy estimates.

We remark that although the error analysis motivates including derivative terms in the loss function, our neural network implementation retains the original loss function defined in ([eq2.7](#eq2.7))-([eq2.12](#eq2.12)).

Let $\widetilde{u}^*$ and $\widetilde{u}^*_d$ denote the best approximations in the function class $\mathcal{H}$ according to formulas ([eq3.12](#eq3.12)) and ([eq3.17](#eq3.17)), respectively.

Namely,

$$

	\widetilde{u}^* := \mathop{\arg \min}_{v_\theta \in \mathcal{H}} {\mathcal{L}}(v_\theta), \quad \widetilde{u}_d^* := \mathop{\arg \min}_{v_\theta \in \mathcal{H}}  {\mathcal{L}}^d(v_\theta).

$$

Further denote by $\widetilde{u}^*_{d\mathscr{A}}$ the numerical solution obtained via optimization procedures based on the loss function ([eq3.17](#eq3.17)).

We then have the following result:

\begin{theorem} \label{thm1}
	Under the assumption of Lemma~[lemma1](#lemma1), if we further assume that there exits an extension function $\widetilde{u}(\mathbf{x},t, z(\mathbf{x}, t))$ of the solution $u(\mathbf{x}, t)$ satisfying $\widetilde{u} \in H^4(\widetilde{Q})$, then we have
	

$$

		

$$
\begin{aligned}

			\|u - u^*_{d \mathscr{A}}\|_{L^2(0,T, H^1(\Omega))} \lesssim & \mathop{\inf}_{v_\theta \in \mathcal{H}} \|v_\theta - \widetilde{u}\|_{H^4(\widetilde{Q})} + \mathop{\sup}_{v_\theta \in \mathcal{H}} \left|  {\mathcal{L}}^d(v_\theta) - {\mathcal{L}}(v_\theta) \right| \\
			&+ \left|  {\mathcal{L}}^d(\widetilde{u}^*_{d\mathscr{A}}) -  {\mathcal{L}}^d(\widetilde{u}^*_{d}) \right|.
		
\end{aligned}
$$

	
$$

\end{theorem}

\begin{proof}
	Let $e:=u - u^*_{d \mathscr{A}}$, where $u^*_{d \mathscr{A}}\left( \mathbf{x}, t\right) := \widetilde{u}^*_{d \mathscr{A}}\left( \mathbf{x}, t, z\left( \mathbf{x}, t\right) \right) ,  \mathbf{x} \in \Omega,  t \in [0,T_\text{end}].$
It is easy to see that $e$ satisfies
	

$$

		\left\{
		

$$
\begin{aligned}

			\partial_t e - \nabla \cdot \left( \beta \nabla e\right)  &= f - \partial_t u^*_{d \mathscr{A}} + \nabla \cdot \left( \beta \nabla u^*_{d \mathscr{A}}\right) \quad  &&\mathrm{in}\; \Omega, \quad  t \in [0, T_{\text{end}}],  \\
			e\left( \cdot, t\right) &= g - u^*_{d \mathscr{A}} \quad  &&\mathrm{on}\; \partial \Omega, \quad t \in [0, T_{\text{end}}], \\
			e\left( \cdot, 0\right) &= u_0 - u^*_{d \mathscr{A},0}\quad &&\mathrm{in}\; \Omega,
		
\end{aligned}
$$

\right.
	
$$

	with the interface condition
	

$$

		

$$
\begin{aligned}

			[e]_{\Gamma(t)} &= 0, \quad t\in [0,T_{\text{end}}],\\
			[\beta \nabla e \cdot \mathbf{n}]_{\Gamma(t)}& = h_N - [\beta \nabla u^*_{d \mathscr{A}}  \cdot \mathbf{n}]_{\Gamma(t)}, \quad t\in [0,T_{\text{end}}]. 			
		
\end{aligned}
$$

	
$$

Thus, from Lemma~[lemma1](#lemma1) and the fact $L(\widetilde{u}) = 0$, it follows that:
	

$$
\begin{aligned}

		\|e\|_{L^2(0,T, H^1(\Omega))}^2 
		\lesssim & \| \partial_t u^*_{d \mathscr{A}} - \nabla \cdot \left( \beta \nabla u^*_{d \mathscr{A}} \right) - f \|_{L^2(0,T_{\text{end}};L^2(\Omega))}^2  \\
		&+ \|u^*_{d \mathscr{A}} - g\|_{L^2\left( 0, T_{\text{end}}, H^2(\partial \Omega)\right) }^2 + \| u^{*'}_{d \mathscr{A}} - g' \|_{L^2\left( 0, T_{\text{end}}, L^2(\partial \Omega)\right)}^2\\
		&+\|u^*_{d \mathscr{A}, 0} - u_0\|_{H^1(\Omega)}^2  + \|[\beta \nabla u^*_{d \mathscr{A}} \cdot \mathbf{n}]_{\Gamma(t)} - h_N\|_{L^2\left( 0, T_{\text{end}}; L^2(\Gamma(t))\right) }^2\\

		=& \mathcal{L}(\widetilde{u}^*_{d \mathscr{A}})={\mathcal{L}}(\widetilde{u}^*_{d \mathscr{A}}) -  {\mathcal{L}}\left( \widetilde{u}\right).
	
\end{aligned}
$$

Further, it follows from the definitions of $\widetilde{u}^*$ and $\widetilde{u}^*_d$, and the fact $L(\widetilde{u}) = 0$ again that
	

$$

		

$$
\begin{aligned}

			 {\mathcal{L}}(\widetilde{u}^*_{d \mathscr{A}})& -  {\mathcal{L}}\left( \widetilde{u}\right)
			= \left(  {\mathcal{L}}(\widetilde{u}^*_{d \mathscr{A}}) -  {\mathcal{L}}^d(\widetilde{u}^*_{d \mathscr{A}})\right) + \left(  {\mathcal{L}}^d(\widetilde{u}^*_{d \mathscr{A}}) -  {\mathcal{L}}^d(\widetilde{u}^*_{d})\right)  \\
			&\qquad\qquad+ \left(  {\mathcal{L}}^d(\widetilde{u}^*_{d}) -  {\mathcal{L}}^d(\widetilde{u}^*) \right)+ \left(  {\mathcal{L}}^d(\widetilde{u}^*)  -  {\mathcal{L}}(\widetilde{u}^*) \right) + \left(  {\mathcal{L}}(\widetilde{u}^*) - {\mathcal{L}}\left( \widetilde{u}\right)\right) \\
			&\le \left(  {\mathcal{L}}(\widetilde{u}^*) -  {\mathcal{L}}\left( \widetilde{u}\right)\right) + \left(  {\mathcal{L}}^d(\widetilde{u}^*)  -  {\mathcal{L}}(\widetilde{u}^*) \right) + \left(  {\mathcal{L}}(\widetilde{u}^*_{d \mathscr{A}}) -  {\mathcal{L}}^d(\widetilde{u}^*_{d \mathscr{A}})\right)\\&\quad + \left(  {\mathcal{L}}^d(\widetilde{u}^*_{d \mathscr{A}}) -  {\mathcal{L}}^d(\widetilde{u}^*_{d})\right) \\
			&\le \mathop{\inf}_{v_\theta \in \mathcal{H}} \left|  {\mathcal{L}}(v_\theta) -  {\mathcal{L}}(\widetilde{u}) \right|  + 2\mathop{\sup}_{v_\theta \in \mathcal{H}} \left|  {\mathcal{L}}^d(v_\theta) -  {\mathcal{L}}(v_\theta) \right| + \left|  {\mathcal{L}}^d(\widetilde{u}^*_{d\mathscr{A}}) -  {\mathcal{L}}^d(\widetilde{u}^*_{d}) \right|.
		
\end{aligned}
$$

	
$$

Then, it suffices to bound the term $ {\mathcal{L}}(v_\theta) -  {\mathcal{L}}(\widetilde{u})$.

From the definition of $ {\mathcal{L}}$ and Trace Theorem, it follows that
	

$$

		

$$
\begin{aligned}

			 {\mathcal{L}}(v_\theta) -  {\mathcal{L}}(\widetilde{u}) 
			&= \int_{0}^{T_{\text{end}}}\int_{\Omega} \left| \partial_t v_\theta - \nabla \cdot (\beta \nabla v_\theta) - \partial_t \widetilde{u} +  \nabla \cdot (\beta \nabla \widetilde{u})\right|^2 \mathrm{d}\mathbf{x} \mathrm{d}t \\
			&\quad+\int_{0}^{T_{\text{end}}}\int_{\partial \Omega} { \mathop{\sum}_{|\alpha|=0}^2}\left| \partial^\alpha_\mathbf{x} \left( v_\theta - \widetilde{u}\right)   \right|^2 \mathrm{d}\mathbf{x} \mathrm{d}t\\
			&\quad+\int_{0}^{T_{\text{end}}}\int_{\partial \Omega}  \left|\partial_t v_\theta - \partial_t \widetilde{u} \right|^2 \mathrm{d}\mathbf{x} \mathrm{d}t \\	
			&\quad+\int_{\Omega} {\mathop{\sum}_{|\alpha| = 0}^1} \left| \partial^\alpha_\mathbf{x} \left(v_\theta(\mathbf{x}, 0, z(\mathbf{x}, 0)) - \widetilde{u}(\mathbf{x}, 0)\right)  \right|^2 \mathrm{d}\mathbf{x} \\
			&\quad+\int_{0}^{T_{\text{end}}}\int_{\Gamma(t)} \left| [\beta \nabla_\mathbf{x} v_\theta \cdot \mathbf{n}]_{\Gamma(t)} - [\beta \nabla_\mathbf{x} \widetilde{u} \cdot \mathbf{n}]_{\Gamma(t)})\right|^2 \mathrm{d}\mathbf{x} \mathrm{d}t\\
			&\lesssim \|v_\theta - \widetilde{u}\|_{H^4(\widetilde{Q})}.
		
\end{aligned}
$$

	
$$

This completes the proof immediately.
\end{proof}

Following Theorem~[thm1](#thm1), we decompose the error into three distinct components:  

	
-  **Approximation error $\mathcal{E**_\text{approx}$:}

Best approximation error of the target function $\widetilde{u} \in H^4(\widetilde{Q})$ in the neural network function class $\mathcal{H}$ under the Sobolev norm on bounded domain $\widetilde{Q}$;
	
-  **Statistical error $\mathcal{E**_\text{stat}$:}

Error induced by finite sampling;
	
-  **Optimization error $\mathcal{E**_\text{opt}$:}

Error from numerical optimization procedures.

Under over-parameterized assumptions, optimization error converges to 0 as established in the literature [^Liu2022Loss]. {Therefore, we assume that the optimization error $\mathcal{E}_{\text{opt}}$ can be  arbitrarily small.

For approximation error $\mathcal{E}_\text{approx}$ in Sobolev space,} the following approximation theorem holds {\cite[Proposition 4.8]{guhring2021approximation}:  }

\begin{theorem} \label{thm2}
	Let $s\in \mathbb{N} \cup \left\lbrace 0\right\rbrace $ be fixed, and $u \in H^k\left( D \right) $ with $k \ge s+1$, where $D$ is a bounded domain in $\mathbb{R}^{d}$.

Then for any tolerance $\varepsilon > 0$, there exists at least one $v_\theta$ of depth $\mathcal{O}\left(\log\left( d+k\right) \right) $ with $|\theta|_{l^0}$ bounded by $\mathcal{O}\left( \varepsilon^{-\frac{d}{k-s-\mu(s=2)}}\right) $ and $|\theta|_{l^\infty}$ by $\mathcal{O}\left( \varepsilon^{-2 -\frac{9d/2+2s+2\mu(s=2)}{k-s-\mu(s=2)}}\right)  $, where $\mu$ is arbitrarily small, and the notation $(s=2)$ equals to $1$ if $s=2$ and $0$ otherwise,  such that
	

$$

		\|v_\theta - u\|_{H^k(D)} \le \varepsilon.
	
$$

\end{theorem}

Theorem~[thm2](#thm2) demonstrates that the approximation error $\mathcal{E}_{\text{approx}}$ of the neural network can be effectively controlled.

Therefore, for $\varepsilon>0$, there exists a DNN $v_\theta \in \mathcal{H}$ such that $\|v_\theta - \widetilde{u}\|_{H^4(\widetilde{Q})} \le \varepsilon$.

Next, we bound the statistical error via the Rademacher complexity which measures the complexity of a collection of function by the correlation between function values with Rademacher random variables, i.e., with probability $P\left( \omega =1\right) = P\left( \omega = -1\right) = \frac{1}{2}$ (cf. [^Goar2024Foundations]). 

\begin{definition}
	Let $\mathcal{F}$ be a real-valued function class defined on the domain $D$ and $\xi = \left\lbrace \xi_j\right\rbrace_{j=1}^n$ be i.i.d. samples from the distribution $\mathcal{U}(D)$.

Then the Rademacher complexity $\mathfrak{R}_n(\mathcal{F})$ is defined by
	

$$

		\mathfrak{R}_n(\mathcal{F}) = \mathbb{E}_{\xi, \omega} \left[ \mathop{\sup}_{f\in \mathcal{F}} n^{-1} \left| \mathop{\sum}_{j=1}^n \omega_j f\left( \xi_j\right) \right| \right], 
	
$$

	where $\omega = \left\lbrace \omega_j\right\rbrace_{j=1}^n $ are i.i.d.

Rademacher random variables.
\end{definition}

To derive an upper bound estimate for the statistical error $\mathcal{E}_\text{stat}$, we invoke the following several lemmas:

\begin{lemma}\cite[Theorem 3.3]{goar2024foundations}  \label{lemma2}
	Let $\mathcal{F}$ be a family of functions mapping from $D$ to $\mathbb{R}$ such that $\mathop{\sup}_{f\in\mathcal{F}} \|f\|_{L^\infty(D)} \le M_{\mathcal{F}} < \infty$. $X_1, X_2,...,X_n$ be a set of i.i.d. random variables.

Then for any $0<\delta<1$, with probability at least $1-\delta$ the following holds
	

$$

		\mathop{\sup}_{f\in\mathcal{F}}\left| n^{-1} \mathop{\sum}_{j=1}^n f(X_j) - \mathbb{E}\left[ f(X)\right] \right| \le 2 \mathfrak{R}_n(\mathcal{F}) + 2M_{\mathcal{F}} \sqrt{\dfrac{\log \frac{1}{\delta}}{2n}}.
	
$$

\end{lemma}

\begin{lemma}\cite[Lemma 2]{siegel2023greedy}\label{lemma3}
	Let $\mathcal{F}$ a family of real-functions on $D$ and suppose that $\psi :\mathbb{R} \to \mathbb{R}$ is L-Lipschitz.

Let $\psi \circ \mathcal{F} = \left\lbrace \psi\left( f(x)\right) : f\in \mathcal{F} \right\rbrace $, then 
	

$$

		\mathfrak{R}_n\left( \psi \circ \mathcal{F} \right)  \le L \mathfrak{R}_n\left( \mathcal{F}\right).
	
$$

\end{lemma}

The next lemma implies that the  function $v_\theta \in \mathcal{H}$ is uniformly bounded and Lipschitz continuous {with respect to $\theta$}. 

\begin{lemma} \label{lemma4}
	Let $L$, $W$ and $B_\theta$ be the depth, width, and maximum weight bound of a DNN function class $\mathcal{H}$ {mapping from $D$ to $\mathbb{R}$} with $N_\theta$ nonzero weights.

Then {for any $v_\theta, v_{\widetilde{\theta}} \in \mathcal{H}$,} the following estimates hold:
	

		
- [(1)] $\|v_\theta\|_{L^\infty(D)}\le WB_\theta$, 
		$\|v_\theta - v_{\widetilde{\theta}} \|_{L^\infty(D)} \le 2LW^LB_\theta^{L-1}|\theta - \widetilde{\theta}|_{l^\infty}$;
		
- [(2)] $\|\partial_{x} v_\theta\|_{L^\infty(D)} \le  W^{L-1}

B_\theta^L$, 
		$\|\partial_{x}(v_\theta - v_{\widetilde{\theta}})\|_{L^\infty(D)} \le L^2 W^{2L-2}B_\theta^{2L-2}|\theta - \widetilde{\theta}|_{l^\infty}$;
		
- [(3)] $\|\partial_{x}^2 v_\theta\|_{L^\infty(D)} \le L W^{2L-2}

B_\theta^{2L}$, \\
		$\|\partial_{x}^2(v_\theta - v_{\widetilde{\theta}})\|_{L^\infty(D)} \le 4 L^2 N_\theta W^{3L-3}B_\theta^{3L-3}|\theta - \widetilde{\theta}|_{l^\infty}$.
	

\end{lemma}

The proof of Lemma~[lemma4](#lemma4) can be found in \cite[Section 5]{jin2023solving}.

\begin{definition}
	Let $(\mathcal{M}, m)$ be a metric space of real-valued functions, and $\mathcal{G} \subset \mathcal{M}$.

A set $\left\lbrace x_i\right\rbrace_{i=1}^p \subset \mathcal{G}$ is called an $\epsilon$-cover of $\mathcal{G}$ if for any $x\in\mathcal{G}$, there exists an element $x' \in \left\lbrace x_i\right\rbrace_{i=1}^p$ such that $m(x, x') \le \epsilon$.

The $\epsilon$-covering number, denoted by $\mathcal{C}(\mathcal{G},m ,\epsilon)$, is the minimum cardinality among all $\epsilon$-covers of $\mathcal{G}$.

That is,
	

$$

		\mathcal{C}(\mathcal{G},m ,\epsilon) = \min \left\lbrace p\in \mathbb{N}: \exists \text{ an } \epsilon \text{-cover } \left\lbrace x_1,...,x_p\right\rbrace \text{ of } \mathcal{G}\right\rbrace. 
	
$$

\end{definition}

\begin{lemma}[Dudley's Lemma]\cite[Lemma A.6.]{hu2024solving}\label{lemma5}
	Let $\mathcal{F}$ be a real-valued functions mapping from $D$ to $\mathbb{R}$ class and $M_\mathcal{F}:=\mathop{\sup}_{f\in\mathcal{F}} \|f\|_{L^\infty(D)}$.

Then the Rademacher complexity $\mathfrak{R}_n(\mathcal{F})$ is bounded by
	

$$

		\mathfrak{R}_n(\mathcal{F}) \le \mathop{\inf}_{0<s<M_\mathcal{F}} \left( 4s + 12 n^{-1/2} \int_{s}^{M_\mathcal{F}} \left( \log \mathcal{C}\left( \mathcal{F}, \|\cdot\|_{L^\infty(D)}, \epsilon\right) \right) ^{1/2} \mathrm{d}\epsilon\right) .
	
$$

\end{lemma}

\begin{theorem}
	Let $L$, $W$ and $B_\theta$ be the depth, width, and maximum weight bound of a DNN function class $\mathcal{H}$ with $N_\theta$ nonzero weights.

We assume the assumptions in Lemma~[lemma1](#lemma1) hold and the level set function $\phi(\mathbf{x}, t) \in C^2(Q)$. {Then,} for any small $0<\delta < 1$, with probability at least $\left( 1 - \delta\right)^4$, {it} holds
	

$$

		

$$
\begin{aligned}

			\mathop{\sup}_{v_\theta \in \mathcal{H}} \left|  {\mathcal{L}}^d(v_\theta) -  {\mathcal{L}}(v_\theta) \right| 
			\lesssim & \dfrac{\log^{\frac{1}{2}}

N_{\Omega} + 1}{\sqrt{N_{\Omega}}} + \dfrac{\log^{\frac{1}{2}}

N_{\partial \Omega} + 1}{\sqrt{N_{\partial \Omega}}} \\
		&	+ \dfrac{\log^{\frac{1}{2}}

N_{\Omega_0} + 1}{\sqrt{N_{\Omega_0}}} + \dfrac{\log^{\frac{1}{2}}

N_{\Gamma}+ 1}{\sqrt{N_{\Gamma}}},
		
\end{aligned}
$$

	
$$

	where $N_{\Omega}$, $N_{\partial \Omega}$, $N_{\Omega_0}$ and $ N_{\Gamma}$ denote the numbers of uniform sample points in their corresponding domain.
\end{theorem}

\begin{proof}

It is obvious that
	

$$

		\mathop{\sup}_{v_\theta \in \mathcal{H}} \left|  {\mathcal{L}}^d(v_\theta) -  {\mathcal{L}}(v_\theta) \right| \le \mathop{\sup}_{v_\theta \in \mathcal{H}} \mathop{\sum}_{i=1}^4 \left|  {\mathcal{L}}_i(v_\theta) - \widehat{l}_i(v_\theta) \right|.
	
$$

For the first item $\mathop{\sup}_{v_\theta \in \mathcal{H}}\left|  {\mathcal{L}}_1(v_\theta) - \widehat{l}_1(v_\theta) \right|$, by use of Lemma~[lemma2](#lemma2), we can obtain the following inequality with probability at least $(1-\delta)$
	

$$

		\mathop{\sup}_{v_\theta \in \mathcal{H}}\left|  {\mathcal{L}}_1(v_\theta) - \widehat{l}_1(v_\theta) \right|
		\lesssim \mathfrak{R}_{N_{\Omega}}\left( \psi \circ \left( \mathcal{D}\left( \mathcal{H}\right) - f\right) \right) + N_{\Omega}^{-1/2},
	
$$

where $\psi(x) = x^2$ and $
		\mathcal{D}\left( \mathcal{H}\right) := \left\lbrace \partial_t v_\theta - \nabla \cdot (\beta \nabla v_\theta) | v_\theta \in \mathcal{H} \right\rbrace. $
	
The assumptions on neural network function class $\mathcal{H}$ combined with Lemmas~[lemma3](#lemma3) yield:
	

$$
\label{eq3.34}
		\mathfrak{R}_{N_{\Omega}}\left( \psi \circ \left( \mathcal{D}\left( \mathcal{H}\right) + f\right) \right) \lesssim \mathfrak{R}_{N_{\Omega}}\left( \mathcal{D} \left( \mathcal{H}\right)\right) + \mathfrak{R}_{N_{\Omega}}(f).
	
$$

	
	By Remark~[remark2](#remark2), for $v_\theta \in \mathcal{H}$ we can explicitly write the following equation
	

$$

		\partial_t v_\theta - \nabla \cdot(\beta \nabla v_\theta) = D_t v_\theta + D_z v_\theta \partial_t z - \beta \left( \Delta_\mathbf{x} + 2 \nabla z \cdot \nabla_\mathbf{x} (D_z v_\theta) + |\nabla z|^2 D_z^2 + D_z v_\theta \Delta z\right). 
	
$$

	
	Since $\phi(\mathbf{x}, t) \in C^2(Q)$ and $Q$ is a bounded domain, we know from Lemma~[lemma4](#lemma4) that $\mathcal{D}(\mathcal{H})$ constitutes a class of bounded functions which are Lipschitz continuous for parameter $\theta$.

We denote the Lipschitz constant by $L_\mathcal{\mathcal{D}(\mathcal{H})}$ and the upper bound of $\mathcal{D}(\mathcal{H})$ by $M_{\mathcal{D}(\mathcal{H})}$, both of {which} depend on $W$, $B_\theta$, $L$ and $N_\theta$.

Fixed $B_\theta \in [1, \infty)$ and $\epsilon \in (0,1)$, applying the result of \cite[Proposition 5]{cucker2001on} yields
	

$$

		\log \mathcal{C} \left( \mathcal{D}(\mathcal{H}), \|\cdot\|_{L^\infty(Q)}, \epsilon \right) \le \log \mathcal{C} \left( \theta, |\cdot|_{l^\infty}, L_{\mathcal{D}(\mathcal{H})}^{-1} \epsilon\right) \le N_\theta \log( 4 B_\theta L_{\mathcal{D}(\mathcal{H})} \epsilon^{-1}).
	
$$

Further, setting $s = N_{\Omega}^{-1/2}$ in Lemma~[lemma5](#lemma5), we have
	

$$

		

$$
\begin{aligned}

			\mathfrak{R}_{N_{\Omega}}(\mathcal{D}(\mathcal{H}))
			&\le 4 N_{\Omega}^{-1/2} + 12 N_{\Omega}^{-1/2} \int_{N_{\Omega}^{-1/2}}^{M_{\mathcal{D}(\mathcal{H})}} \left( N_\theta \log\left(4 B_\theta L_{\mathcal{D}(\mathcal{H})} \epsilon^{-1}\right) \right) ^{1/2} \mathrm{d}\epsilon \\
			&\le 4 N_{\Omega}^{-1/2} + 12 N_{\Omega}^{-1/2} \left( M_{\mathcal{D}(\mathcal{H})} - N_{\Omega}^{-1/2} \right) \left( N_\theta \log\left(4 B_\theta L_{\mathcal{D}(\mathcal{H})}

N_{\Omega}^{1/2}\right) \right) ^{1/2} \\
			&\le 4 N_{\Omega}^{-1/2} + 12 M_{\mathcal{D}(\mathcal{H})}N_\theta^{1/2}

N_{\Omega}^{-1/2}\left( \log^{1/2} (4 B_\theta L_{\mathcal{D}(\mathcal{H})}) + \log^{1/2} \left( N_{\Omega}^{1/2}\right) \right) \\
			&\lesssim N_{\Omega}^{-1/2} \left( \log^{1/2}

N_{\Omega} + 1\right). 
		
\end{aligned}
$$

	
$$

	
	For the second term of ([eq3.34](#eq3.34)), using Jensen's inequality and the fact that $\mathbb{E}[\omega_i \omega_j] = \delta_{ij}$, we have
	

$$

		

$$
\begin{aligned}

			\mathfrak{R}_{N_{\Omega}}\left( f\right) 
			&= \mathbb{E}_{\xi,\omega} \left[ N_{\Omega}^{-1} \left| \mathop{\sum}_{j=1}^{N_{\Omega}} \omega_j f(\xi_j)\right| \right]\le \mathbb{E}_\xi \left[ \mathbb{E}_\omega \left( N_{\Omega}^{-1} \mathop{\sum}_{j=1}^{N_{\Omega}} \omega_j f(\xi_j)\right) ^2\right] ^{1/2} \\
			&= N_{\Omega}^{-1} \mathbb{E}_\xi \left[ \mathop{\sum}_{j=1}^{N_{\Omega}} f^2(\xi_j)\right] ^{1/2} = N_{\Omega}^{-1/2} \left( \int_0^{T_{\text{end}}} \int_{\Omega(t)} f^2(\mathbf{x}, t) \mu(\mathbf{x}, t) \mathrm{d}\mathbf{x}\mathrm{d}t\right)^{1/2} \\
			&= \left|Q\right|^{-1/2}

N_{\Omega}^{-1/2}\|f\|_{L^2(0,T_{\text{end}}; L^2(\Omega))} \lesssim N_{\Omega}^{-1/2},
		
\end{aligned}
$$

	
$$

	where $\left\lbrace \xi_j\right\rbrace_{j=1}^{N_{\Omega}}$ are i.i.d. samples from the uniform distribution $\mathcal{U}\left( Q\right) $,  $\mu(\mathbf{x}, t)$ is the density function of the uniform distribution on the domain $Q$, $\omega = \left\lbrace \omega_j\right\rbrace_{j=1}^n $ are i.i.d.

Rademacher random variables, and $|Q|$ denotes the measure of domain $Q$. 
Then we obtain that 
	

$$

		\mathop{\sup}_{v_\theta \in \mathcal{H}}\left|  {\mathcal{L}_1(v_\theta) - \widehat{l}_1(v_\theta)} \right| \lesssim \dfrac{\log^{\frac{1}{2}}

N_{\Omega} + 1}{\sqrt{N_{\Omega}}}.
	
$$

The proof is completed by analogous arguments for the other terms.
\end{proof}

As established in Section~[sec2.4](#sec2.4), we have presented a method for approximating the level set function using neural networks, which is denoted by $\phi_\theta(\mathbf{x}, t)$.

The zero level set of $\phi_\theta(\mathbf{x}, t)$ represents the approximate interface

$$

	\Gamma_\theta(t) :=\left\{ \mathbf{x} | \phi_\theta(\mathbf{x}, t) = 0, \mathbf{x} \in \Omega\right\}, \quad t\in[0, T_{\text{end}}].

$$

Although \(\Gamma_\theta(t)\) deviates from the true interface \(\Gamma(t)\), we establish that the solution remains controllable relative to the original problem's solution under suitable assumptions with sufficiently small interface discrepancy.

We quantify this interface deviation via the Hausdorff distance:

$$

	\delta = \mathop{\max}_{t \in [0, T_{\text{end}}]} d_H\left( \Gamma(t), \Gamma_\theta(t)\right), 

$$

where $d_H$ denotes Hausdorff distance:

$$

	d_H(A, B) := \mathop{\max}\left\lbrace \mathop{\sup}_{a\in A} \mathop{\inf}_{b\in B} \mathrm{dist}\left( a, b\right) , \mathop{\sup}_{b\in B} \mathop{\inf}_{a\in A} \mathrm{dist}\left( a, b\right) \right\rbrace. 

$$

We define a tubular neighborhood of width $\delta > 0$  about the interface $\Gamma(t)$ as follows:

$$

	S_{\delta}\left( t\right) :=\left\lbrace \mathbf{x}| \mathrm{dist}(\mathbf{x}, \Gamma(t)) < \delta \right\rbrace,
	\quad S_\delta:= \mathop{\bigcup}_{t \in [0, T_{\text{end}}]}  S_\delta(t).

$$

By use of the Lemma 2.1 in [^Li2010Optimal], we have:

\begin{lemma} \label{lemma6}
	For any $v \in B_{2,1}^{1/2}\left( \Omega^+(t)\cup \Omega^-(t)\right), t\in[0,T_{\text{end}}]$, it holds
	

$$

		\|v\|_{L^2(S_\delta(t))}^2 \lesssim \delta \|v\|_{B_{2,1}^{1/2}\left( \Omega^+(t)\cup \Omega^-(t)\right)}.
	
$$

\end{lemma}

Finally, we conclude the following result:

\begin{theorem}
	Assume that the conditions of Eqs.([eq1.1](#eq1.1))-([eq1.7](#eq1.7)) satisfy {$h_D=0$ and $h_N=0$}.

Then, we have the following estimate
	

$$

		\|u-u_\delta\|_{L^2(0,T_{\text{end}}; H^1(\Omega))} \lesssim \delta^{1/2},
	
$$

	where $u$ is the solution to the equations with the true interface $\Gamma(t)$, and $u_\delta$ is the solution corresponding to the approximate interface $\Gamma_\theta(t)$.
\end{theorem}

\begin{proof}
	According to the assumptions, we have
	

$$

		\int_\Omega \partial_t u(t) w(t) + \int_\Omega \beta \nabla u(t) \cdot \nabla w(t) = \int_\Omega f(t) w(t) ,\quad w(t) \in H^1_0(\Omega), t\in[0,T_{\text{end}}],
	
$$

	

$$

		\int_\Omega \partial_t u_\delta(t) w(t) + \int_\Omega \beta_\delta \nabla u_\delta(t) \cdot \nabla w(t) = \int_\Omega f(t) w(t) ,\quad w(t) \in H^1_0(\Omega), t\in[0,T_{\text{end}}],
	
$$

	where
	

$$

		\beta_\delta(\mathbf{x}, t) = \left\lbrace 
		

$$
\begin{aligned}

			&\beta^+, \quad \mathrm{for}\;\mathbf{x} \in \Omega_\delta^+(t), \\
			&\beta^-, \quad \mathrm{for}\;\mathbf{x} \in \Omega_\delta^-(t), 
		
\end{aligned}
$$

		\right. 
	
$$

	and
	

$$

		\Omega_\delta^+(t) = \left\lbrace \mathbf{x}| \phi_\theta(\mathbf{x}, t)>0, \mathbf{x} \in \Omega \right\rbrace, \quad \Omega_\delta^-(t) = \left\lbrace \mathbf{x}| \phi_\theta(\mathbf{x}, t)<0, \mathbf{x} \in \Omega \right\rbrace, \quad  t\in [0,T_{\text{end}}].
	
$$

	It follows that
	

$$

		\int_\Omega \partial_t\left( u_\delta - u\right) w + \int_\Omega \beta_\delta \nabla\left( u_\delta - u\right) \cdot \nabla w = \int_{S_{\delta}(t)} \left( \beta - \beta_\delta\right) \nabla u \cdot \nabla w.
	
$$

Further, letting $w = u_\delta - u$ and applying the Cauchy-Schwartz and Young's inequalities, it follows from Lemma~[lemma6](#lemma6) that
	

$$

		\dfrac{\mathrm{d}}{\mathrm{d}t} \|\left( u_\delta - u\right)(t)\|_{L^2(\Omega)}^2 + \|\nabla\left( u_\delta - u \right) (t)\|_{L^2(\Omega)}^2 \lesssim \|\nabla u\|_{L^2(S_\delta(t))}^2.
	
$$

	Then, adding $\| \left( u_\delta - u\right)(t)\|_{L^2(\Omega)}$ to both sides of the inequality yields
	

$$
\label{eq3.44}
		
\begin{split}
		\dfrac{\mathrm{d}}{\mathrm{d}t} \|\left( u_\delta - u\right)(t)\|_{L^2(\Omega)}^2 &+ \|\left( u_\delta - u \right) (t)\|_{H^1(\Omega)}^2\\ & \lesssim \|(u_\delta - u)(t)\|_{L^2(\Omega)}^2 + \|\nabla u\|_{L^2(S_\delta(t))}^2.
		\end{split}

	
$$

Thus the differential form of Gronwall's inequality yields the estimate
	

$$

		\mathop{\max}_{t \in [0,T_{\text{end}}]} \|\left( u_\delta - u\right)(t)\|_{L^2(\Omega)}^2 \lesssim \|(u_\delta - u)(0)\|_{L^2(\Omega)}^2 + \int_0^{T_{\text{end}}} \|\nabla u\|_{L^2(S_\delta(t))}^2.
	
$$

	Since the interface at time $t=0$ is known exactly, we integrate Equation ([eq3.44](#eq3.44)) with respect to time to obtain:
	

$$

		

$$
\begin{aligned}

			&\mathop{\max}_{t \in [0,T_{\text{end}}]} \|\left( u_\delta - u\right)(t)\|_{L^2(\Omega)}^2 + \int_{0}^{T_{\text{end}}}\|\left( u_\delta - u \right) (t)\|_{H^1(\Omega)}^2 \\
			&\qquad\qquad\quad\lesssim \int_0^{T_{\text{end}}}\|\nabla u\|_{L^2(S_\delta(t))}^2 
			\lesssim \int_0^{T_{\text{end}}}\|\nabla u\|_{B_{2,1}^{1/2}(\Omega^+(t) \cup \Omega^-(t))} \lesssim \delta.
		
\end{aligned}
$$

	
$$

This completes the proof of the theorem.
\end{proof}

### NTK analysis

Although numerous studies have demonstrated that neural network optimization error converges to zero under certain conditions, concerns persist regarding the convergence speed.

In [^Jacot2018Neural], the authors introduced a novel approach termed the Neural Tangent Kernel for analyzing neural network training.

This work demonstrates that, under specific parameter initialization conditions, as the width of the hidden layer(s) in a neural network {approaches infinity}, the NTK becomes constant during training.

Consequently, training the neural network becomes equivalent to performing a deterministic kernel regression process.

That is to say, under appropriate conditions, analysis based on gradient flow can be viewed as kernel regression analysis.

We now briefly review {the theory of NTK and show how} the NTK governs the convergence rate of gradient descent during neural network optimization.

Consider the problem minimizing the square loss on a dataset $\left\lbrace (\mathbf{x}_i, y_i)\right\rbrace_{i=1}^N$:

$$

	\mathcal{L}(\theta) = \dfrac{1}{2} \mathop{\sum}_{i=1}^N \left( u(\mathbf{x}_i;\theta) - y_i\right)^2, 

$$

where $u$ is parametrized by $\theta$.

A continuous version of gradient flow can be written as

$$

	\dfrac{\mathrm{d} \theta(t)}{\mathrm{d}t} = -\nabla_\theta \mathcal{L} = - \mathop{\sum}_{i=1}^{N} \left( u(\mathbf{x}_i, \theta(t)) - y_i\right)  \nabla_\theta u(\mathbf{x}_i, \theta(t)).

$$

This implies that

$$
\label{eq3.49}
	
\begin{split}
	\dfrac{\mathrm{d} u(\mathbf{x}_i, \theta(t))}{\mathrm{d} t} &= \nabla_\theta  u(\mathbf{x}_i, \theta(t)) \cdot \dfrac{\mathrm{d} \theta(t)}{\mathrm{d} t} \\
	&= - \mathop{\sum}_{i=1}^{N}  \left( u(\mathbf{x}_i, \theta(t)) - y_i\right) \left[ \nabla_\theta  u(\mathbf{x}_i, \theta(t)) \right]^T  \left[ \nabla_\theta  u(\mathbf{x}_i, \theta(t)) \right].
	\end{split}

$$

Therefore, the NTK is defined as

$$

	\mathbf{K}(\mathbf{x}, \mathbf{x}')(t) = \left\langle \dfrac{\partial u(\mathbf{x}, \theta(t))}{\partial \theta}, \dfrac{\partial u(\mathbf{x}', \theta(t))}{\partial \theta}\right\rangle,

$$

where $\left\langle \cdot, \cdot\right\rangle $ denotes an inner product over $\theta$.

Then over the dataset $\left\lbrace (\mathbf{x}_i, y_i)\right\rbrace_{i=1}^N$, ([eq3.49](#eq3.49)) can be rewritten as

$$

	\dfrac{\mathrm{d}}{\mathrm{d}t}
	\left[ 
	
\begin{matrix}
		u(\mathbf{x}_1, \theta(t))  \\ \addlinespace[1ex]
		u(\mathbf{x}_2, \theta(t))  \\ \addlinespace[1ex]
		\vdots						\\ \addlinespace[1ex]
		u(\mathbf{x}_N, \theta(t))
	\end{matrix}

	\right] 
	\approx - \mathcal{K}(t) 
	\left[ 
	
\begin{matrix}
		u(\mathbf{x}_1, \theta(t)) - y_1  \\ \addlinespace[1ex]
		u(\mathbf{x}_2, \theta(t)) - y_2  \\ \addlinespace[1ex]
		\vdots						\\ \addlinespace[1ex]
		u(\mathbf{x}_N, \theta(t)) - y_N
	\end{matrix}

	\right] , 

$$

where $\mathcal{K}(t) \in \mathbb{R}^{N \times N}$ and the $(i,j)$-th component of $\mathcal{K}(t)$ is $\mathcal{K}_{ij}(t) = \mathbf{K}\left( \mathbf{x}_i, \mathbf{x}_j\right) $.

For a neural network with a single hidden layer, under certain assumptions(see [^Jacot2018Neural], [^Wang2022When]), the NTK {matrix} remains invariant as the network width $W \to \infty$, i.e., $\mathcal{K}(t) = \mathcal{K}(0) = \mathcal{K}^*$.

Therefore, under the assumption that the NTK {matrix} is a positive definite matrix, we can leverage its spectral decomposition $\mathcal{K}^* = \mathbf{Q} \mathbf{\Lambda} \mathbf{Q}^T$ to obtain

$$

	\mathbf{Q}\left( 
	\left[ 
	
\begin{matrix}
		u(\mathbf{x}_1, \theta(t))   \\ \addlinespace[1ex]
		u(\mathbf{x}_2, \theta(t))   \\ \addlinespace[1ex]
		\vdots						 \\ \addlinespace[1ex]
		u(\mathbf{x}_N, \theta(t)) 
	\end{matrix}

	\right]
	-
	\left[ 
	
\begin{matrix}
		y_1  \\ \addlinespace[1ex]
		y_2  \\ \addlinespace[1ex]
		\vdots \\ \addlinespace[1ex]
		y_N
	\end{matrix}

	\right]
	\right) 
	\approx e^{- \mathbf{\Lambda} t} \mathbf{Q}
	\left[ 
	
\begin{matrix}
		y_1  \\ \addlinespace[1ex]
		y_2  \\ \addlinespace[1ex]
		\vdots \\ \addlinespace[1ex]
		y_N
	\end{matrix}

	\right].

$$

It indicates that the convergence rate for neural network data fitting is approximately {$e^\mathbf{{-\Lambda t}}$}.

This implies that the eigenvalues of the NTK {matrix} govern the decay rate of the training error: larger kernel eigenvalues correspond to faster learning speeds. {In [^Wang2022When], the authors }generalized the relevant conclusions of the NTK to PINNs.

Similar to [^Wang2022When], we measure the convergence rate in PINNs by

$$
\label{eq3.53}
	c_{\text{total}} = \dfrac{Tr(\mathcal{K})}{N}, \quad c_{\text{partial}} = \mathop{\sum}_{i=1}^M \dfrac{Tr\left( \mathcal{K}_i\right) }{N_i},

$$

where $M$ represents the number of PDEs, $\mathcal{K}_i$ denotes the NTK matrix corresponding to the $i$-th equation, and $N_i$ is the number of sampling points within the domain defined by the $i$-th equation.

The $\mathcal{K}$ denotes the aggregate NTK matrix encompassing all terms of the {PDEs} ([eq1.1](#eq1.1))-([eq1.6](#eq1.6)), and $N$ represents the total number of training points.

{In the following section, we utilize NTK theory and numerical experiments to demonstrate that XI-PINN with extended variables achieves a higher convergence rate compared to Vanilla-PINN.}

## 4·Numerical Experiments

\label{sec4}

In this section, we demonstrate the effectiveness and accuracy of the XI-PINN method for solving moving interface problems.

The neural network architecture used is consistent with that described in Section~[sec2.1](#sec2.1), specifically a 4-layer DNN with 64 units per hidden layer.

Training points are uniformly sampled within the computational domain.

Furthermore, the array $(N_\Omega, N_{\partial \Omega}, N_{\Omega_0}, N_{\Gamma})$ denotes the number of training points sampled within the corresponding subdomains.

The optimization algorithm employed is the Levenberg-Marquardt (LM) algorithm [^Transtrum2012Improvements].

The maximum number of optimization iterations is set to 5,000, or the optimization process terminates if the loss function value falls below $1 \times 10^{-13}$.

The test points in set $M_{test}$ are sampled from the equispaced grid.

We let $u_\theta$ denote the numerical solution obtained by the neural network and compute the $L^2(0, T_{\text{end}}; L^2(\Omega))$ and $L^2(0, T_{\text{end}}; H^1(\Omega))$ errors by

$$

	e_0 = \sqrt{\dfrac{1}{N_{test}}\mathop{\sum}_{i=1}^{N_{test}} \left[ u_\theta\left( \mathbf{x}_i , t_i \right) - u \left( \mathbf{x}_i , t_i \right)\right]^2}

$$

and

$$

	e_1 = \sqrt{\dfrac{1}{N_{test}}\mathop{\sum}_{i=1}^{N_{test}}  \mathop{\sum}_{\alpha = 0}^{1} \partial_{\mathbf{x}}^\alpha \left[ u_\theta\left( \mathbf{x}_i , t_i \right) - u \left( \mathbf{x}_i , t_i \right)\right]^2},

$$

where $N_{test}$ is the number of points used to compute error. 
{
Similarly, we compute the $L^2(\Omega)$ and $H^1(\Omega)$ errors of $u_\theta$ at time $t$ by

$$

	e_{0, t} =  \sqrt{\dfrac{1}{N_{test}}\mathop{\sum}_{i=1}^{N_{test}} \left[ u_\theta\left( \mathbf{x}_i , t \right) - u \left( \mathbf{x}_i , t\right)\right]^2}

$$

and

$$

	e_{1, t} = \sqrt{\dfrac{1}{N_{test}}\mathop{\sum}_{i=1}^{N_{test}}  \mathop{\sum}_{\alpha = 0}^{1} \partial_{\mathbf{x}}^\alpha \left[ u_\theta\left( \mathbf{x}_i , t\right) - u \left( \mathbf{x}_i , t \right)\right]^2}.

$$

}

All trials are run on an NVIDIA RTX 4090D GPU. 

\begin{example}\label{exa1}
	Consider the 2D moving interface problem ([eq1.1](#eq1.1))-([eq1.7](#eq1.7)) defined over the spatial domain $\Omega=[-1,1]^2$ and the time interval $[0, 1]$.

At time $t = 0$, the interface $\Gamma(0)$ is a circle centered at $(0.3, 0)$, represented by the level set function $\phi_0(x, y) = (x-0.3)^2 + y^2 - (\pi/6)^2$.

Driven by a velocity field $\mathcal{V} = \left[ -0.3 \pi \sin(\pi t), 0.3 \pi \cos(\pi t)\right]^T $, the interface rotates about the origin $(0, 0)$.

Thus, we can express the evolving level set function as:
	$$
	\phi(x, y, t) = (x - 0.3\cos(\pi t))^2 + (y - 0.3\sin(\pi t))^2 - (\pi/6)^2.
	$$
	The exact solution is as follows:
	

$$

		u(x, y, t) = \left\lbrace 
		

$$
\begin{aligned}

			& \dfrac{\left( \left( x - 0.3 \cos(\pi t)\right)^2 + \left( y - 0.3 \sin(\pi t) \right)^2\right) ^{5/2} \left( \pi / 6\right) ^{-1}  }{\beta^+}\\
			&\qquad\qquad \qquad\qquad\qquad\qquad+ \left( \pi / 6\right)^4\left( \dfrac{1}{\beta^-} -  \dfrac{1}{\beta^+}\right) , \quad \mathrm{in}\; \Omega^+(t), \\
			& \dfrac{\left( \left( x - 0.3 \cos(\pi t)\right)^2 + \left( y - 0.3 \sin(\pi t) \right)^2\right) ^{5/2} \left( \pi / 6\right) ^{-1}  }{\beta^-}, \quad  \mathrm{in}\; \Omega^-(t),
		
\end{aligned}
$$

		\right. 
	
$$

	where $\beta^+ = 10$ and $\beta^- = 1$.
\end{example}

In this example, we investigate the impact of neural network depth, width, and training dataset size on solution accuracy, as detailed in Tables~[Table1](#Table1) and~[Table2](#Table2).

With a neural network depth of $L=3$, width $W=64$, and training points $(N_\Omega, N_{\partial \Omega}, N_{\Omega_0}, N_{\Gamma}) = (15\text{K}, 4\text{K}, 3\text{K}, 0.5\text{K})$, the solution errors reach $e_0 = 1.11 \times 10^{-7}$ and $e_1 = 1.85 \times 10^{-6}$, demonstrating high accuracy.

As shown in Table~[Table1](#Table1), increasing the number of neural network parameters enhances the network's approximation of the exact solution.

Additionally, deeper networks more accurately approximate the target function's spatial derivatives. %an increase in the number of neural network parameters correlates with enhanced approximation capability of the neural network towards the exact solution.

Furthermore, deeper neural networks demonstrate superior performance in approximating the spatial derivatives of the target function. 

The numerical results in Table~[Table2](#Table2) validate Theorem~[thm2](#thm2), demonstrating that the generalization error is controlled by the number of sampling points: decreasing this number correspondingly increases the neural network solution error.

Visualizations of the approximate solution $u_\theta$ at different times, along with corresponding errors, are presented in Fig.~[Fig2](#Fig2) and Table~[Table3](#Table3).

<a id="Table1">Example~[exa1](#exa1): $L^2(0,1; L^2(\Omega))$ and $L^2(0,1; H^1(\Omega))$ errors for different widths and depths.</a>

<a id="Table2">Example~[exa1](#exa1): $L^2(0,1; L^2(\Omega))$ and $L^2(0,1; H^1(\Omega))$ errors for different numbers of training  points.</a>

![](Ex1_pred_t=0.1)

<a id="Fig2">Example~[exa1](#exa1): The profile of solution $u_\theta$ and absolution point-wise error $|u_\theta - u|$.

Left:$t=0.1$, Middle:$t=0.5$, Right:$t=0.9$.</a>

<a id="Table3">Example~[exa1](#exa1): $L^2(\Omega)$ and $H^1(\Omega)$ errors at different times.</a>

Furthermore, we compute the eigenvalues of the NTK matrices for XI-PINN and Vanilla-PINN (see Fig~[Fig3](#Fig3)).

A neural network with a single hidden layer of 512 neurons is used for this computation.

The number of sampling points employed for calculating the NTK matrices is $(N_\Omega, N_{\partial \Omega}, N_{\Omega_0}, N_{\Gamma}) = (1000, 400, 200, 400)$.

For the moving interface problem using Vanilla-PINN, the loss function associated with the interface condition can be computed via a finite difference scheme, namely:

$$

	\left[ \beta \nabla u \cdot \mathbf{n}\right]_\Gamma \approx  \beta^+ \nabla u(\mathbf{x},t) \cdot \mathbf{n} - \beta^- \nabla u(\mathbf{x+\epsilon \mathbf{n}},t) \cdot \mathbf{n}, \quad \mathbf{x} \in \Gamma(t).

$$

Here, we let $\epsilon=10^{-6}$.

It can be observed that the eigenvalue distribution of the NTK for XI-PINN is significantly more favorable than that of Vanilla-PINN.

This implies that XI-PINN exhibits a faster convergence rate.

The convergence rate metrics calculated according to formulas ([eq3.53](#eq3.53)) are presented in Table~[Table4](#Table4).

![](NTK1)

<a id="Fig3">Example~[exa1](#exa1): The eigenvalue distribution of the NTK for XI-PINN and Vanilla-PINN.</a>

<a id="Table4">Example~[exa1](#exa1): The convergence rate metrics for XI-PINN and Vanilla-PINN.</a>

\begin{example}\label{exa2}
	In this example, we consider the moving interface problem ([eq1.1](#eq1.1))-([eq1.7](#eq1.7)) defined in 3D space $\Omega = [-1,1]^3$ over the time interval $[0, 1]$.

The interface is a rotating and rising ellipsoidal surface.

At the initial time $t = 0$, it is represented by the level set function 
	$$\phi_0(x,y,z) = \dfrac{x^2}{0.7^2} + \dfrac{y^2}{0.5^2} + \dfrac{(z + 0.25)^2}{0.5^2} - 1.$$
	
	The motion of the interface can be described by a velocity field $$\mathcal{V} = \left[ -\pi y/2,\pi x /2, 0.5\right] ^T.$$ Therefore, we can obtain the level set function
	

$$

		
\begin{split}
		\phi(x,y,z,t) &= \dfrac{\left( x\cos(\pi t /2) + y \sin(\pi t / 2)\right)^2 }{0.7^2} + \dfrac{\left( -x\sin(\pi t /2) + y \cos(\pi t / 2)\right)^2}{0.5^2} \\
		&\qquad\qquad+ \dfrac{\left( z - 0.5t + 0.25\right)^2}{0.5^2} - 1.		
		\end{split}

	
$$

	The exact solution is as follows:
	

$$

		u(x,y,z,t) = \left\lbrace 
		

$$
\begin{aligned}

			&\exp\left( x^2 + y^2 + z^2\right) \cos t, \quad &\mathrm{in}\; \Omega^+(t), \\
			& 0.1 \sin(x) \cos(t) \exp(z) \exp(-t) , \quad  &\mathrm{in}\; \Omega^-(t),
		
\end{aligned}
$$

		\right. 
	
$$

	where $\beta^+ = 10$ and $\beta^- = 1$.
\end{example}

{This example is set to demonstrate}

XI-PINN's effectiveness in handling higher-dimensional problems.

The computational results at different times are presented in Fig.~[Fig5](#Fig5) and Table~[Table5](#Table5). {Moreover, the error $e_0$ reaches $4.68 \times 10^{-7}$ and the error $e_1$ is $5.09 \times 10^{-7}$ in this test. }These highly accurate results demonstrate XI-PINN's effectiveness in solving 3D moving interface problems.

Crucially, the method maintains its performance without fundamental degradation in higher dimensions.

<a id="Table5">Example~[exa2](#exa2): $L^2(\Omega)$ error and $H^1(\Omega)$ error at different times.</a>

![](Ex2_pred_t=0.1)

<a id="Fig5">Example~[exa2](#exa2): The profile of solution $u_\theta$ and absolution point-wise error $|u_\theta - u|$.

Left:$t=0.1$, Middle:$t=0.5$, Right:$t=0.9$.</a>

The XI-PINN method can also be applied to solve more complex time-varying interface problems such as Oseen equations [^Ma2023High-Order].

The model is described as follows:

$$
\label{eq4.1}
\left\{

$$
\begin{aligned}

	\partial_t \mathbf{u} + \left( \mathcal{V} \cdot \nabla\right)  \mathbf{u} &- \nu \nabla \mathbf{u} + \nabla p = \mathbf{f}, \quad \mathrm{div} \mathbf{u} = 0, \\ &\qquad \qquad\quad \mathrm{in}\; \Omega = \Omega^+(t) \cup \Omega^-(t), \quad  t \in [0, T_{\text{end}}],  \\
	\mathbf{u}|_{t=0} &= \mathbf{u}_{0}, \quad  \mathrm{in}\; \Omega = \Omega^+(0) \cup \Omega^-(0), \\
	\left[ \nu \partial_{\mathbf{n}} \mathbf{u} - p \mathbf{n} \right]_{\Gamma(t)}& = \mathbf{h}_N, \quad \left[ \mathbf{u}\right]_{\Gamma(t)} = \mathbf{h}_D, \quad  \mathrm{in}\; \Gamma\left( t\right) , \quad  t \in [0, T_{\text{end}}],\\
	\mathbf{u} & = \mathbf{g}, \quad \mathrm{in} \; \partial \Omega, \quad t \in [0, T_{\text{end}}],

\end{aligned}
$$

\right.

$$

where $\mathcal{V}$ is the advection velocity which governs the variation of the interface $\Gamma(t)$.

The viscosities $\nu$ is piece-wise positive constants, which is defined as

$$
\label{eq4.5}
	\nu(\mathbf{x}, t) = \left\lbrace 
	

$$
\begin{aligned}

		&\nu^+, \quad \mathrm{for}\;\mathbf{x} \in \Omega^+(t), \\
		&\nu^-, \quad \mathrm{for}\;\mathbf{x} \in \Omega^-(t). 
	
\end{aligned}
$$

	\right. 

$$

\begin{example}\label{exa3}
	Consider the 2D moving interface problem ([eq4.1](#eq4.1))-([eq4.5](#eq4.5)), the domain $\Omega$ is a disk which is centered at $(0.5, 0.5)$ and has radius $0.5$.

The initial interface $\Gamma(0)$ is a star-shape smooth curve.

The initial level set function is parametrized by
	$$
	\phi_0(r, \theta) = r - \dfrac{3}{10} \left[ 2.5 + 1.5 \sin\left( 5 \theta + 5\pi/36\right) \right] ^{-1/4}.
	$$
	Driven by the velocity field $\mathcal{V} = \left( 0.5- y, x -0.5\right) ^T$, the interface rotates counterclockwise around $(0.5, 0.5)$.

The final time is set by $T_{\text{end}}=1$.

In this example, we employ the methodology described in Section~[sec2.4](#sec2.4) to approximate a level set function $\phi(x,y,t)$ using a neural network.
	The true solution is set by
	

$$

		u(x, y, t) = \left\lbrace 
		

$$
\begin{aligned}

			& \left( \exp(x) \sin \left( \pi y + \pi t\right) , \pi^{-1} \exp(x) \cos\left( \pi y + \pi t\right) \right)^T, \quad &\mathrm{in}\; \Omega^+(t), \\
			& \left( \cos\left( \pi x\right) \sin \left( \pi y\right) , -\sin\left( \pi x\right) \cos\left( \pi y\right) \right)^T \cos t, \quad  &\mathrm{in}\; \Omega^-(t),
		
\end{aligned}
$$

		\right. 
	
$$

	

$$

		p(x, y, t) = \left\lbrace 
		

$$
\begin{aligned}

			& \sin\left( 0.5 \pi x\right) \cos \left( 0.5 \pi y\right) , \quad &\mathrm{in}\; \Omega^+(t), \\
			& \cos\left( 0.5 \pi x\right) \sin \left( 0.5 \pi y\right) , \quad  &\mathrm{in}\; \Omega^-(t),
		
\end{aligned}
$$

		\right. 
	
$$

	and the coefficients of viscosity are taken as $\nu^+=10^{-3}$ and $\nu^- = 1$.
\end{example}

![](Ex3_pred_u_t=0)

<a id="Fig6">Example~[exa3](#exa3): The profile of solution $u_\theta$ and absolution point-wise error $|u_\theta - u|$ at different times.</a>

It can be observed that obtaining an explicit expression for the level set function in this example is challenging.

Therefore, we resort to neural networks to construct a level set function.

In this example, a neural network of depth $L=4$ and width $W=64$ is employed to approximate the inverse mapping $\widehat{\mathbf{X}}$.

The approximation error of the neural network is quantified as follows:

$$

	E_{\widehat{\mathbf{X}}_\theta} = \dfrac{1}{N_t N_\Gamma}  \mathop{\sum}_{j=1}^{N_t} \mathop{\sum}_{i=1}^{N_\Gamma} \left| \widehat{\mathbf{X}}_\theta\left( \mathbf{X}_{RK}\left(\mathbf{x}_{i,\Gamma}, t_j;0\right)  ,t_j;0\right) - \mathbf{x}_{i,\Gamma}\right|^2, \, \mathbf{x}_{i, \Gamma} \in \Gamma(0), t_j \in [0, T_{\text{end}}].

$$

The computed error metric yields $E_{\widehat{\mathbf{X}}_\theta} = 2.13 \times 10^{-8}$. 

Visualizations depicting the neural network solution and its absolute error distribution are presented in Fig.~[Fig6](#Fig6) for different times.

The error $e_0$ of the neural network solution $u_\theta$ is $8.53 \times 10^{-8}$, and its error $e_1$ is $1.09 \times 10^{-6}$.

It is noteworthy that a single neural network with three outputs was utilized to solve the Oseen equations in this example.

These error levels also {correspond to highly satisfactory accuracy.

The result} demonstrates that XI-PINN can be successfully applied to solve more complex Oseen equation models.

\begin{example} \label{exa4}
	In this example, we consider the 2D moving interface problem~([eq4.1](#eq4.1))-([eq4.5](#eq4.5)) involving large deformations of the interface geometry.

The domain is given by $\Omega = [0,1 ]^2$.

The initial interface $\Gamma(0)$ is the disk whose radius is $0.15$ and its center is $(0.5, 0.75)$ which is determined by the level set function $\phi_0\left( x_1, x_2\right) = x_1^2 + x_2^2 - 0.15^2$.

The flow velocity which drives the motion of interface is given by
	$$
	\mathcal{V} = \cos\left( \pi t/3\right) \left( \sin^2\left( \pi x\right)  \sin\left( 2 \pi y\right) , -\sin^2\left( \pi y\right) \sin\left( 2 \pi x\right) \right) ^T.
	$$
	At the finial time $T_{\text{end}}=1$, $\Omega^-(T_{\text{end}})$ is stretched into a snake-shape domain, which is  illustrated in Fig~[Fig7](#Fig7).

The true solution is set as same as that of Example~[exa3](#exa3).
	
![](Gamma0)

<a id="Fig7">Exaple~[exa4](#exa4): The interface $\Gamma(t)$ at different times.</a>

\end{example}

For moving interface problems involving large geometric deformations, approximating the inverse mapping $\widehat{\mathbf{X}}$ using a neural network is highly prone to mesh entanglement issues.

Therefore, we adopted a strategy of composite piecewise approximation with multiple neural networks, as detailed in ([eq2.24](#eq2.24)).

In this specific example, an ensemble of five neural networks was utilized to approximate $\widehat{\mathbf{X}}$, achieving the following error metrics: $E_{\widehat{\mathbf{X}}_\theta} = 5.27 \times 10^{-7}$. 

The numerical results for the Oseen equations are presented in Fig.~[Fig8](#Fig8).

The error $e_0$ is $4.51 \times 10^{-8}$ and the error $e_1$ is $6.45 \times 10^{-7}$.

This demonstrates that XI-PINN, as a mesh-free method, can effectively handle moving interface problems involving large geometric deformations.

![](Ex4_pred_u_t=0)

<a id="Fig8">Example~[exa4](#exa4): The profile of solution $u_\theta$ and absolution point-wise error $|u_\theta - u|$ at different times.</a>

## 5·Conclusion

\label{sec5}
{
This paper proposes XI-PINN, a novel neural network approach leveraging the EVT to solve moving interface problems.

The method characterizes the interface $\Gamma(t)$ using level set functions.

When explicit level set expressions are unavailable, we introduce a neural network-based adaptive algorithm for their construction.

Furthermore, we present an error analysis for XI-PINN, decomposing the total error into three distinct components: the best approximation error in the neural network space $\mathcal{E}_\text{approx}$, the statistical error $\mathcal{E}_\text{stat}$, and the optimization error $\mathcal{E}_\text{opt}$.

Under appropriate assumptions, both $\mathcal{E}_\text{approx}$ and $\mathcal{E}_\text{opt}$ can be reduced to arbitrary precision.

Specifically, we establish an upper bound for $\mathcal{E}_\text{stat}$, providing theoretical guarantees for efficacy of XI-PINN in solving parabolic interface problems.

Numerical experiments demonstrate that XI-PINN effectively addresses moving interface problems, including challenging high-dimensional and large-deformation cases, achieving errors of $\mathcal{O}(10^{-8})$ in the $L^2(0, T_{\text{end}}; L^2(\Omega))$ norm.

Additionally, leveraging the NTK theory, we show numerically that XI-PINN improves the convergence rate compared to Vanilla-PINN, further highlighting its enhanced performance for interface problems.
}

\bibliographystyle{siamplain}
\input{main.bbl}
\end{document}

## References

[^Greengard1994On]: On the Numerical Evaluation of Electrostatic Fields in Composite Materials. Acta Numerica 1994.
[^Sussman1999Efficient,]: An Efficient, Interface-Preserving Level Set Redistancing Algorithm and Its Application to Interfacial Incompressible Fluid Flow. SIAM Journal on Scientific Computing 1999.
[^Chen1997Simple]: A Simple Level Set Method for Solving Stefan Problems. Journal of Computational Physics 1997.
[^Caflisch2003Analysis]: Analysis of Island Dynamics in Epitaxial Growth of Thin Films. Multiscale Modeling \& Simulation 2003.
[^Babu{\v{s}}ka2000Can]: Can a Finite Element Method Perform Arbitrarily Badly?. Mathematics of Computation 2000.
[^Lan2020Finite]: Finite Element Analysis of an Arbitrary Lagrangian--Eulerian Method for Stokes/Parabolic Moving Interface Problem With Jump Coefficients. Results in Applied Mathematics 2020.
[^Tezduyar1992New]: A New Strategy for Finite Element Computations Involving Moving Boundaries and Interfaces—the Deforming-Spatial-Domain/Space-Time Procedure: II. Computation of Free-Surface Flows, Two-Liquid Flows, and Flows With Drifting Cylinders. Computer Methods in Applied Mechanics and Engineering 1992.
[^Burman2015CutFEM]: CutFEM: Discretizing Geometry and Partial Differential Equations. International Journal for Numerical Methods in Engineering 2015.
[^Chen2023Arbitrarily]: An Arbitrarily High Order Unfitted Finite Element Method for Elliptic Interface Problems With Automatic Mesh Generation. Journal of Computational Physics 2023.
[^Huang2017Unfitted]: An Unfitted Interface Penalty Finite Element Method for Elliptic Interface Problems. Computer Methods in Applied Mechanics and Engineering 2017.
[^Babu{\v{s}}ka1983Generalized]: Generalized Finite Element Methods: Their Performance and Their Relation to Mixed Methods. SIAM Journal on Numerical Analysis 1983.
[^Zhang2022Condensed]: A Condensed Generalized Finite Element Method (CGFEM) for Interface Problems. Computer Methods in Applied Mechanics and Engineering 2022.
[^Dolbow2001Extended]: An Extended Finite Element Method for Modeling Crack Growth With Frictional Contact. Computer Methods in Applied Mechanics and Engineering 2001.
[^Li1998Immersed]: The Immersed Interface Method Using a Finite Element Formulation. Applied Numerical Mathematics 1998.
[^Adjerid2024High]: A High Order Geometry Conforming Immersed Finite Element for Elliptic Interface Problems. Computer Methods in Applied Mechanics and Engineering 2024.
[^Chen2021Adaptive]: An Adaptive High-Order Unfitted Finite Element Method for Elliptic Interface Problems. Numerische Mathematik 2021.
[^Raissi2019Physics-Informed]: Physics-Informed Neural Networks: A Deep Learning Framework for Solving Forward and Inverse Problems Involving Nonlinear Partial Differential Equations. Journal of Computational Physics 2019.
[^Lu2021Learning]: Learning Nonlinear Operators via DeepONet Based on the Universal Approximation Theorem of Operators. Nature Machine Intelligence 2021.
[^Tseng2023Cusp-Capturing]: A Cusp-Capturing PINN for Elliptic Interface Problems. Journal of Computational Physics 2023.
[^Hu2022Discontinuity]: A Discontinuity Capturing Shallow Neural Network for Elliptic Interface Problems. Journal of Computational Physics 2022.
[^Wu2022Inn]: INN: Interfaced Neural Networks as an Accessible Meshless Approach for Solving Interface PDE Problems. Journal of Computational Physics 2022.
[^Ying2024Accurate]: An Accurate and Efficient Continuity-Preserved Method Based on Randomized Neural Networks for Elliptic Interface Problems. SIAM Journal on Scientific Computing 2024.
[^Wu2024Solving]: Solving Parametric Elliptic Interface Problems via Interfaced Operator Network. Journal of Computational Physics 2024.
[^Bi2025XI-DeepONet]: XI-DeepONet: An Operator Learning Method for Elliptic Interface Problems. Journal of Computational Physics 2025.
[^Hu2024Solving]: Solving Poisson Problems in Polygonal Domains With Singularity Enriched Physics Informed Neural Networks. SIAM Journal on Scientific Computing 2024.
[^Osher2004Level]: Level Set Methods and Dynamic Implicit Surfaces. Appl. Mech. Rev. 2004.
[^Li2025Continuity-Preserved]: Continuity-Preserved Deep Learning Method for Solving Elliptic Interface Problems. Computational and Applied Mathematics 2025.
[^Verner1978Explicit]: Explicit Runge--Kutta Methods With Estimates of the Local Truncation Error. SIAM Journal on Numerical Analysis 1978.
[^Tartar2007Introduction]: An Introduction to Sobolev Spaces and Interpolation Spaces. 
[^Lions1972Non-Homogeneous]: Non-Homogeneous Boundary Value Problems and Applications. 
[^Liu2022Loss]: Loss Landscapes and Optimization in Over-Parameterized Non-Linear Systems and Neural Networks. Applied and Computational Harmonic Analysis 2022.
[^Goar2024Foundations]: Foundations of Machine Learning. Intelligent Optimization Techniques for Business Analytics 2024.
[^Li2010Optimal]: Optimal a Priori Estimates for Higher Order Finite Elements for Elliptic Interface Problems. Applied Numerical Mathematics 2010.
[^Jacot2018Neural]: Neural Tangent Kernel: Convergence and Generalization in Neural Networks. Advances in Neural Information Processing Systems 2018.
[^Wang2022When]: When and Why PINNs Fail to Train: A Neural Tangent Kernel Perspective. Journal of Computational Physics 2022.
[^Transtrum2012Improvements]: Improvements to the Levenberg-Marquardt Algorithm for Nonlinear Least-Squares Minimization. arXiv:1201.5885.
[^Ma2023High-Order]: High-Order Unfitted Characteristic Finite Element Methods for Moving Interface Problem of Oseen Equations. Journal of Computational and Applied Mathematics 2023.